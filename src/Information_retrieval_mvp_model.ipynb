{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a lightweight document to question similarity evaluation; data too large to process with RNN|LSTM|Attention given current time and computational power available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df_master = pd.read_csv('master_data.csv')\n",
    "del df_master['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>book_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>section_id</th>\n",
       "      <th>Gutenberg</th>\n",
       "      <th>﻿</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>tokens</th>\n",
       "      <th>what_ind</th>\n",
       "      <th>...</th>\n",
       "      <th>question_len</th>\n",
       "      <th>lexile_score</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>context</th>\n",
       "      <th>tags</th>\n",
       "      <th>master_text</th>\n",
       "      <th>tokens_ans</th>\n",
       "      <th>question_c</th>\n",
       "      <th>answer_c</th>\n",
       "      <th>context_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>180</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>179.0</td>\n",
       "      <td>180. How many days after returning home\\r\\r\\r\\...</td>\n",
       "      <td>Two</td>\n",
       "      <td>['180', '.', 'How', 'many', 'days', 'after', '...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>1070</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>\\r\\r\\nIN WHICH IT IS SHOWN THAT PHILEAS FOGG G...</td>\n",
       "      <td>93\\30.txt</td>\n",
       "      <td>180. how many days after returning home\\r\\r\\r\\...</td>\n",
       "      <td>['two']</td>\n",
       "      <td>180. how many days after returning home\\r\\r\\r\\...</td>\n",
       "      <td>two</td>\n",
       "      <td>\\r\\r\\nin which it is shown that phileas fogg g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>179</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>178.0</td>\n",
       "      <td>179. What does Fogg get from his journey\\r\\r\\r...</td>\n",
       "      <td>All of these</td>\n",
       "      <td>['179', '.', 'What', 'does', 'Fogg', 'get', 'f...</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>1070</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>\\r\\r\\nIN WHICH IT IS SHOWN THAT PHILEAS FOGG G...</td>\n",
       "      <td>93\\30.txt</td>\n",
       "      <td>179. what does fogg get from his journey\\r\\r\\r...</td>\n",
       "      <td>['all', 'of', 'these']</td>\n",
       "      <td>179. what does fogg get from his journey\\r\\r\\r...</td>\n",
       "      <td>all of these</td>\n",
       "      <td>\\r\\r\\nin which it is shown that phileas fogg g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>178</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>177.0</td>\n",
       "      <td>178. Who says That we might have\\r\\r\\r\\r\\r\\nma...</td>\n",
       "      <td>Passepartout</td>\n",
       "      <td>['178.', 'Who', 'says', 'That', 'we', 'might',...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1070</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>\\r\\r\\nIN WHICH IT IS SHOWN THAT PHILEAS FOGG G...</td>\n",
       "      <td>93\\30.txt</td>\n",
       "      <td>178. who says that we might have\\r\\r\\r\\r\\r\\nma...</td>\n",
       "      <td>['passepartout']</td>\n",
       "      <td>178. who says that we might have\\r\\r\\r\\r\\r\\nma...</td>\n",
       "      <td>passepartout</td>\n",
       "      <td>\\r\\r\\nin which it is shown that phileas fogg g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>177</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>176.0</td>\n",
       "      <td>177. What does Passepartout do with the\\r\\r\\r\\...</td>\n",
       "      <td>He splits it between Passepartout and Fix</td>\n",
       "      <td>['177', '.', 'What', 'does', 'Passepartout', '...</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>1070</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>\\r\\r\\nIN WHICH IT IS SHOWN THAT PHILEAS FOGG G...</td>\n",
       "      <td>93\\30.txt</td>\n",
       "      <td>177. what does passepartout do with the\\r\\r\\r\\...</td>\n",
       "      <td>['he', 'splits', 'it', 'between', 'passepartou...</td>\n",
       "      <td>177. what does passepartout do with the\\r\\r\\r\\...</td>\n",
       "      <td>he splits it between passepartout and fix</td>\n",
       "      <td>\\r\\r\\nin which it is shown that phileas fogg g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>93</td>\n",
       "      <td>176</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>175.0</td>\n",
       "      <td>176. What does Passepartout discover on\\r\\r\\r\\...</td>\n",
       "      <td>They arrived on Saturday not Sunday</td>\n",
       "      <td>['176', '.', 'What', 'does', 'Passepartout', '...</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1070</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>\\r\\r\\nIN WHICH IT IS SHOWN THAT PHILEAS FOGG G...</td>\n",
       "      <td>93\\30.txt</td>\n",
       "      <td>176. what does passepartout discover on\\r\\r\\r\\...</td>\n",
       "      <td>['they', 'arrived', 'on', 'saturday', 'not', '...</td>\n",
       "      <td>176. what does passepartout discover on\\r\\r\\r\\...</td>\n",
       "      <td>they arrived on saturday not sunday</td>\n",
       "      <td>\\r\\r\\nin which it is shown that phileas fogg g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0_x  book_id  question_id  section_id  Gutenberg      ﻿  \\\n",
       "0             0       93          180          30          1  179.0   \n",
       "1             1       93          179          30          1  178.0   \n",
       "2             2       93          178          30          1  177.0   \n",
       "3             3       93          177          30          1  176.0   \n",
       "4             4       93          176          30          1  175.0   \n",
       "\n",
       "                                            question  \\\n",
       "0  180. How many days after returning home\\r\\r\\r\\...   \n",
       "1  179. What does Fogg get from his journey\\r\\r\\r...   \n",
       "2  178. Who says That we might have\\r\\r\\r\\r\\r\\nma...   \n",
       "3  177. What does Passepartout do with the\\r\\r\\r\\...   \n",
       "4  176. What does Passepartout discover on\\r\\r\\r\\...   \n",
       "\n",
       "                                      answer  \\\n",
       "0                                        Two   \n",
       "1                               All of these   \n",
       "2                               Passepartout   \n",
       "3  He splits it between Passepartout and Fix   \n",
       "4        They arrived on Saturday not Sunday   \n",
       "\n",
       "                                              tokens  what_ind  \\\n",
       "0  ['180', '.', 'How', 'many', 'days', 'after', '...     False   \n",
       "1  ['179', '.', 'What', 'does', 'Fogg', 'get', 'f...      True   \n",
       "2  ['178.', 'Who', 'says', 'That', 'we', 'might',...     False   \n",
       "3  ['177', '.', 'What', 'does', 'Passepartout', '...      True   \n",
       "4  ['176', '.', 'What', 'does', 'Passepartout', '...      True   \n",
       "\n",
       "                         ...                         question_len  \\\n",
       "0                        ...                                   12   \n",
       "1                        ...                                   11   \n",
       "2                        ...                                   17   \n",
       "3                        ...                                   12   \n",
       "4                        ...                                    8   \n",
       "\n",
       "   lexile_score  Unnamed: 0_y  \\\n",
       "0          1070        1619.0   \n",
       "1          1070        1619.0   \n",
       "2          1070        1619.0   \n",
       "3          1070        1619.0   \n",
       "4          1070        1619.0   \n",
       "\n",
       "                                             context       tags  \\\n",
       "0  \\r\\r\\nIN WHICH IT IS SHOWN THAT PHILEAS FOGG G...  93\\30.txt   \n",
       "1  \\r\\r\\nIN WHICH IT IS SHOWN THAT PHILEAS FOGG G...  93\\30.txt   \n",
       "2  \\r\\r\\nIN WHICH IT IS SHOWN THAT PHILEAS FOGG G...  93\\30.txt   \n",
       "3  \\r\\r\\nIN WHICH IT IS SHOWN THAT PHILEAS FOGG G...  93\\30.txt   \n",
       "4  \\r\\r\\nIN WHICH IT IS SHOWN THAT PHILEAS FOGG G...  93\\30.txt   \n",
       "\n",
       "                                         master_text  \\\n",
       "0  180. how many days after returning home\\r\\r\\r\\...   \n",
       "1  179. what does fogg get from his journey\\r\\r\\r...   \n",
       "2  178. who says that we might have\\r\\r\\r\\r\\r\\nma...   \n",
       "3  177. what does passepartout do with the\\r\\r\\r\\...   \n",
       "4  176. what does passepartout discover on\\r\\r\\r\\...   \n",
       "\n",
       "                                          tokens_ans  \\\n",
       "0                                            ['two']   \n",
       "1                             ['all', 'of', 'these']   \n",
       "2                                   ['passepartout']   \n",
       "3  ['he', 'splits', 'it', 'between', 'passepartou...   \n",
       "4  ['they', 'arrived', 'on', 'saturday', 'not', '...   \n",
       "\n",
       "                                          question_c  \\\n",
       "0  180. how many days after returning home\\r\\r\\r\\...   \n",
       "1  179. what does fogg get from his journey\\r\\r\\r...   \n",
       "2  178. who says that we might have\\r\\r\\r\\r\\r\\nma...   \n",
       "3  177. what does passepartout do with the\\r\\r\\r\\...   \n",
       "4  176. what does passepartout discover on\\r\\r\\r\\...   \n",
       "\n",
       "                                    answer_c  \\\n",
       "0                                        two   \n",
       "1                               all of these   \n",
       "2                               passepartout   \n",
       "3  he splits it between passepartout and fix   \n",
       "4        they arrived on saturday not sunday   \n",
       "\n",
       "                                           context_c  \n",
       "0  \\r\\r\\nin which it is shown that phileas fogg g...  \n",
       "1  \\r\\r\\nin which it is shown that phileas fogg g...  \n",
       "2  \\r\\r\\nin which it is shown that phileas fogg g...  \n",
       "3  \\r\\r\\nin which it is shown that phileas fogg g...  \n",
       "4  \\r\\r\\nin which it is shown that phileas fogg g...  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(df_master['question_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 93, 178, 175, 174, 173, 172, 171, 168, 167, 166, 165, 164, 163,\n",
       "       162, 161, 160, 158, 157, 156, 155, 154, 153, 152, 150, 151, 148,\n",
       "       147, 146, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136, 135,\n",
       "       134, 133, 132, 131, 130, 128,  35,  12,  79,  77,  83,  84,  85,\n",
       "        86,  87,  88,  90,  89, 127,  91,  92,  94,  95,  96,  98, 100,\n",
       "       101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114,\n",
       "       115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master['book_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0_x                                                 10614\n",
       "book_id                                                        127\n",
       "question_id                                                    180\n",
       "section_id                                                       5\n",
       "Gutenberg                                                        1\n",
       "﻿                                                             2879\n",
       "question         180. What does the narrator say Thnardier goes...\n",
       "answer                                              A slave-trader\n",
       "tokens           ['180', '.', 'What', 'does', 'the', 'narrator'...\n",
       "what_ind                                                      True\n",
       "tokens2          ['', '.', 'what', 'does', 'the', 'narrator', '...\n",
       "how_ind                                                      False\n",
       "where_ind                                                    False\n",
       "when_ind                                                     False\n",
       "why_ind                                                      False\n",
       "who_ind                                                      False\n",
       "which_ind                                                    False\n",
       "question_type                                                 What\n",
       "answer_len                                                       2\n",
       "question_len                                                    21\n",
       "lexile_score                                                  1010\n",
       "Unnamed: 0_y                                                   551\n",
       "context          VOLUME V--JEAN VALJEAN\\r\\r\\n\\r\\r\\nEnlarge\\r\\r\\...\n",
       "tags                                                     127\\5.txt\n",
       "master_text      180. what does the narrator say thnardier goes...\n",
       "tokens_ans                                   ['a', 'slave-trader']\n",
       "question_c       180. what does the narrator say thnardier goes...\n",
       "answer_c                                            a slave-trader\n",
       "context_c        volume v--jean valjean\\r\\r\\n\\r\\r\\nenlarge\\r\\r\\...\n",
       "Name: 10614, dtype: object"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master.iloc[10530,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>book_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>section_id</th>\n",
       "      <th>Gutenberg</th>\n",
       "      <th>﻿</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>tokens</th>\n",
       "      <th>what_ind</th>\n",
       "      <th>...</th>\n",
       "      <th>question_len</th>\n",
       "      <th>lexile_score</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>context</th>\n",
       "      <th>tags</th>\n",
       "      <th>master_text</th>\n",
       "      <th>tokens_ans</th>\n",
       "      <th>question_c</th>\n",
       "      <th>answer_c</th>\n",
       "      <th>context_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10488</th>\n",
       "      <td>10488</td>\n",
       "      <td>89</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>948.0</td>\n",
       "      <td>49. Why is Mr. Bumble planning to go to\\r\\r\\r\\...</td>\n",
       "      <td>To transfer two sick children to a\\r\\r\\r\\r\\r\\n...</td>\n",
       "      <td>['49', '.', 'Why', 'is', 'Mr.', 'Bumble', 'pla...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>1060</td>\n",
       "      <td>1529.0</td>\n",
       "      <td>CHAPTER XIX\\r\\r\\n\\r\\r\\nIN WHICH A NOTABLE PLAN...</td>\n",
       "      <td>89\\9.txt</td>\n",
       "      <td>49. why is mr. bumble planning to go to\\r\\r\\r\\...</td>\n",
       "      <td>['to', 'transfer', 'two', 'sick', 'children', ...</td>\n",
       "      <td>49. why is mr. bumble planning to go to\\r\\r\\r\\...</td>\n",
       "      <td>to transfer two sick children to a\\r\\r\\r\\r\\r\\n...</td>\n",
       "      <td>chapter xix\\r\\r\\n\\r\\r\\nin which a notable plan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0_x  book_id  question_id  section_id  Gutenberg      ﻿  \\\n",
       "10488         10488       89           49           9          1  948.0   \n",
       "\n",
       "                                                question  \\\n",
       "10488  49. Why is Mr. Bumble planning to go to\\r\\r\\r\\...   \n",
       "\n",
       "                                                  answer  \\\n",
       "10488  To transfer two sick children to a\\r\\r\\r\\r\\r\\n...   \n",
       "\n",
       "                                                  tokens  what_ind  \\\n",
       "10488  ['49', '.', 'Why', 'is', 'Mr.', 'Bumble', 'pla...     False   \n",
       "\n",
       "                             ...                         question_len  \\\n",
       "10488                        ...                                   20   \n",
       "\n",
       "       lexile_score  Unnamed: 0_y  \\\n",
       "10488          1060        1529.0   \n",
       "\n",
       "                                                 context      tags  \\\n",
       "10488  CHAPTER XIX\\r\\r\\n\\r\\r\\nIN WHICH A NOTABLE PLAN...  89\\9.txt   \n",
       "\n",
       "                                             master_text  \\\n",
       "10488  49. why is mr. bumble planning to go to\\r\\r\\r\\...   \n",
       "\n",
       "                                              tokens_ans  \\\n",
       "10488  ['to', 'transfer', 'two', 'sick', 'children', ...   \n",
       "\n",
       "                                              question_c  \\\n",
       "10488  49. why is mr. bumble planning to go to\\r\\r\\r\\...   \n",
       "\n",
       "                                                answer_c  \\\n",
       "10488  to transfer two sick children to a\\r\\r\\r\\r\\r\\n...   \n",
       "\n",
       "                                               context_c  \n",
       "10488  chapter xix\\r\\r\\n\\r\\r\\nin which a notable plan...  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df_master.loc[df_master.book_id == 89] \n",
    "temp.loc [df_master.question_id == 49] #df.loc[(df.Product == p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing problem children:ex 'there is no chapter 25'\n",
    "df_master = df_master.drop(df_master.index[[7020, 7021, 7022, 7023,7024, 7025, 7026,7027,7028,7029,7030, 7031,7032,7033,7034,\n",
    "                                       7035,7036,7037,7038,7039,7040,7041,7042,7043]])#[8790,8791,8792,8793,8794 ,8795]]) #,8766,8767,8768,8769,8770,8771]])\n",
    "#,10524, 10525,10526,10527,10528,10529]]) # THROUGH 7020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = df_master.drop(df_master.index[[8790,8791,8792,8793,8794 ,8795]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = df_master.drop(df_master.index[[10578,10579,10580,10581,10582,10583]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0_x                                                 10620\n",
       "book_id                                                        127\n",
       "question_id                                                    174\n",
       "section_id                                                       5\n",
       "Gutenberg                                                        1\n",
       "﻿                                                             2873\n",
       "question         174. What is the title of Book VIII in Part V ...\n",
       "answer                                         The Twilight Wanes.\n",
       "tokens           ['174', '.', 'What', 'is', 'the', 'title', 'of...\n",
       "what_ind                                                      True\n",
       "tokens2          ['', '.', 'what', 'is', 'the', 'title', 'of', ...\n",
       "how_ind                                                      False\n",
       "where_ind                                                    False\n",
       "when_ind                                                     False\n",
       "why_ind                                                      False\n",
       "who_ind                                                      False\n",
       "which_ind                                                    False\n",
       "question_type                                                 What\n",
       "answer_len                                                       3\n",
       "question_len                                                    13\n",
       "lexile_score                                                  1010\n",
       "Unnamed: 0_y                                                   551\n",
       "context          VOLUME V--JEAN VALJEAN\\r\\r\\n\\r\\r\\nEnlarge\\r\\r\\...\n",
       "tags                                                     127\\5.txt\n",
       "master_text      174. what is the title of book viii in part v ...\n",
       "tokens_ans                       ['the', 'twilight', 'wanes', '.']\n",
       "question_c       174. what is the title of book viii in part v ...\n",
       "answer_c                                       the twilight wanes.\n",
       "context_c        volume v--jean valjean\\r\\r\\n\\r\\r\\nenlarge\\r\\r\\...\n",
       "Name: 10620, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master.iloc[10584,] #13347,] #13354,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### df_master.iloc[8819, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking out some starter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.25082859, 0.39482963, 0.        ],\n",
       "       [0.25082859, 1.        , 0.22057609, 0.        ],\n",
       "       [0.39482963, 0.22057609, 1.        , 0.26264139],\n",
       "       [0.        , 0.        , 0.26264139, 1.        ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check demo code\n",
    "vect = TfidfVectorizer(min_df=1)\n",
    "tfidf = vect.fit_transform([\"I'd like an apple\",\n",
    "                             \"An apple a day keeps the doctor away\",\n",
    "                             \"Never compare an apple to an orange\",\n",
    "                             \"I prefer scikit-learn to Orange\"])\n",
    "matrix_test = (tfidf * tfidf.T).A\n",
    "matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 1, 2, 0],\n",
       "       [3, 2, 0, 1],\n",
       "       [1, 3, 0, 2],\n",
       "       [0, 1, 2, 3]], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_test = np.array(matrix_test)\n",
    "matrix_test.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180. how many days after returning home\r",
      "\r",
      "\r",
      "\r",
      "\r\n",
      "are fogg and aouda married\n"
     ]
    }
   ],
   "source": [
    "test = df_master.iloc[0,26]\n",
    "print(test)\n",
    "\n",
    "test2 = df_master.iloc[1,26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.04604361],\n",
       "       [0.04604361, 1.        ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test small sample\n",
    "vect = TfidfVectorizer(min_df=1)\n",
    "tfidf = vect.fit_transform([test,\n",
    "                             test2])\n",
    "matrix_test = (tfidf * tfidf.T).A\n",
    "matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04604360647426782"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_test[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\r\\nin which it is shown that phileas fogg gained nothing by his tour\\r\\r\\naround the world unless it were happiness\\r\\r\\n\\r\\r\\n\\r\\r\\nyes phileas fogg in person.\\r\\r\\n\\r\\r\\nthe reader will remember that at five minutes past eight in the\\r\\r\\nevening--about five and twenty hours after the arrival of the\\r\\r\\ntravellers in london--passepartout had been sent by his master to\\r\\r\\nengage the services of the reverend samuel wilson in a certain marriage\\r\\r\\nceremony which was to take place the next day.\\r\\r\\n\\r\\r\\npassepartout went on his errand enchanted.  he soon reached the\\r\\r\\nclergymans house but found him not at home.  passepartout waited a\\r\\r\\ngood twenty minutes and when he left the reverend gentleman it was\\r\\r\\nthirty-five minutes past eight.  but in what a state he was  with his\\r\\r\\nhair in disorder and without his hat he ran along the street as never\\r\\r\\nman was seen to run before overturning passers-by rushing over the\\r\\r\\nsidewalk like a waterspout.\\r\\r\\n\\r\\r\\nin three minutes he was in saville row again and staggered back into\\r\\r\\nmr. foggs room.\\r\\r\\n\\r\\r\\nhe could not speak.\\r\\r\\n\\r\\r\\nwhat is the matter asked mr. fogg.\\r\\r\\n\\r\\r\\nmy master gasped passepartout--marriage--impossible--\\r\\r\\n\\r\\r\\nimpossible\\r\\r\\n\\r\\r\\nimpossible--for to-morrow.\\r\\r\\n\\r\\r\\nwhy so\\r\\r\\n\\r\\r\\nbecause to-morrow--is sunday\\r\\r\\n\\r\\r\\nmonday replied mr. fogg.\\r\\r\\n\\r\\r\\nno--to-day is saturday.\\r\\r\\n\\r\\r\\nsaturday  impossible\\r\\r\\n\\r\\r\\nyes yes yes yes cried passepartout.  you have made a mistake of\\r\\r\\none day  we arrived twenty-four hours ahead of time but there are\\r\\r\\nonly ten minutes left\\r\\r\\n\\r\\r\\npassepartout had seized his master by the collar and was dragging him\\r\\r\\nalong with irresistible force.\\r\\r\\n\\r\\r\\nphileas fogg thus kidnapped without having time to think left his\\r\\r\\nhouse jumped into a cab promised a hundred pounds to the cabman and\\r\\r\\nhaving run over two dogs and overturned five carriages reached the\\r\\r\\nreform club.\\r\\r\\n\\r\\r\\nthe clock indicated a quarter before nine when he appeared in the great\\r\\r\\nsaloon.\\r\\r\\n\\r\\r\\nphileas fogg had accomplished the journey round the world in eighty\\r\\r\\ndays\\r\\r\\n\\r\\r\\nphileas fogg had won his wager of twenty thousand pounds\\r\\r\\n\\r\\r\\nhow was it that a man so exact and fastidious could have made this\\r\\r\\nerror of a day  how came he to think that he had arrived in london on\\r\\r\\nsaturday the twenty-first day of december when it was really friday\\r\\r\\nthe twentieth the seventy-ninth day only from his departure\\r\\r\\n\\r\\r\\nthe cause of the error is very simple.\\r\\r\\n\\r\\r\\nphileas fogg had without suspecting it gained one day on his journey\\r\\r\\nand this merely because he had travelled constantly eastward he would\\r\\r\\non the contrary have lost a day had he gone in the opposite direction\\r\\r\\nthat is westward.\\r\\r\\n\\r\\r\\nin journeying eastward he had gone towards the sun and the days\\r\\r\\ntherefore diminished for him as many times four minutes as he crossed\\r\\r\\ndegrees in this direction.  there are three hundred and sixty degrees\\r\\r\\non the circumference of the earth and these three hundred and sixty\\r\\r\\ndegrees multiplied by four minutes gives precisely twenty-four\\r\\r\\nhours--that is the day unconsciously gained.  in other words while\\r\\r\\nphileas fogg going eastward saw the sun pass the meridian eighty\\r\\r\\ntimes his friends in london only saw it pass the meridian seventy-nine\\r\\r\\ntimes.  this is why they awaited him at the reform club on saturday\\r\\r\\nand not sunday as mr. fogg thought.\\r\\r\\n\\r\\r\\nand passepartouts famous family watch which had always kept london\\r\\r\\ntime would have betrayed this fact if it had marked the days as well\\r\\r\\nas the hours and the minutes\\r\\r\\n\\r\\r\\nphileas fogg then had won the twenty thousand pounds but as he had\\r\\r\\nspent nearly nineteen thousand on the way the pecuniary gain was\\r\\r\\nsmall.  his object was however to be victorious and not to win\\r\\r\\nmoney.  he divided the one thousand pounds that remained between\\r\\r\\npassepartout and the unfortunate fix against whom he cherished no\\r\\r\\ngrudge.  he deducted however from passepartouts share the cost of\\r\\r\\nthe gas which had burned in his room for nineteen hundred and twenty\\r\\r\\nhours for the sake of regularity.\\r\\r\\n\\r\\r\\nthat evening mr. fogg as tranquil and phlegmatic as ever said to\\r\\r\\naouda is our marriage still agreeable to you\\r\\r\\n\\r\\r\\nmr. fogg replied she it is for me to ask that question.  you were\\r\\r\\nruined but now you are rich again.\\r\\r\\n\\r\\r\\npardon me madam my fortune belongs to you.  if you had not suggested\\r\\r\\nour marriage my servant would not have gone to the reverend samuel\\r\\r\\nwilsons i should not have been apprised of my error and--\\r\\r\\n\\r\\r\\ndear mr. fogg said the young woman.\\r\\r\\n\\r\\r\\ndear aouda replied phileas fogg.\\r\\r\\n\\r\\r\\nit need not be said that the marriage took place forty-eight hours\\r\\r\\nafter and that passepartout glowing and dazzling gave the bride\\r\\r\\naway.  had he not saved her and was he not entitled to this honour\\r\\r\\n\\r\\r\\nthe next day as soon as it was light passepartout rapped vigorously\\r\\r\\nat his masters door.  mr. fogg opened it and asked whats the\\r\\r\\nmatter passepartout\\r\\r\\n\\r\\r\\nwhat is it sir  why ive just this instant found out--\\r\\r\\n\\r\\r\\nwhat\\r\\r\\n\\r\\r\\nthat we might have made the tour of the world in only seventy-eight\\r\\r\\ndays.\\r\\r\\n\\r\\r\\nno doubt returned mr. fogg by not crossing india.  but if i had\\r\\r\\nnot crossed india i should not have saved aouda she would not have\\r\\r\\nbeen my wife and--\\r\\r\\n\\r\\r\\nmr. fogg quietly shut the door.\\r\\r\\n\\r\\r\\nphileas fogg had won his wager and had made his journey around the\\r\\r\\nworld in eighty days.  to do this he had employed every means of\\r\\r\\nconveyance--steamers railways carriages yachts trading-vessels\\r\\r\\nsledges elephants.  the eccentric gentleman had throughout displayed\\r\\r\\nall his marvellous qualities of coolness and exactitude.  but what\\r\\r\\nthen  what had he really gained by all this trouble  what had he\\r\\r\\nbrought back from this long and weary journey\\r\\r\\n\\r\\r\\nnothing say you  perhaps so nothing but a charming woman who\\r\\r\\nstrange as it may appear made him the happiest of men\\r\\r\\n\\r\\r\\ntruly would you not for less than that make the tour around the world\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_test = df_master.iloc[1,28]\n",
    "context_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from stackoverflow https://stackoverflow.com/questions/4576077/python-split-text-on-sentences\n",
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "caps = \"([A-Z])\"\n",
    "prefixes = \"(mr|mrs|ms|dr|no)[.]\"#added no, took 'st' out (exist., west. had problems and st. is less common\n",
    "suffixes = \"(inc|ltd|jr|sr|co)\"\n",
    "starters = \"(mr|mrs|ms|mr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    if isinstance(text,float):\n",
    "        text = \"JUNK THAT SHOULD BE QC\"\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.replace(\"\\r\",\" \")\n",
    "    text = text.replace(\"      \", \" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"ph.d\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\"<stop>\")# took out punct\n",
    "    text = text.replace(\"?\",\"<stop>\")# took out punct\n",
    "    text = text.replace(\"!\",\"<stop>\")# took out punct\n",
    "    text = text.replace(\"<prd>\",\".\")# took out punct\n",
    "    #I added to debug LSTM\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip(r'\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>?]') for s in sentences]\n",
    "    #sentences = sentences.replace('[`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>?]',' ')\n",
    "    if not sentences:\n",
    "        sentences =  list([\"JUNK\", \"THAT SHOULD\", \"BE removed\", \"when I have time\",\"opps my bad\"])\n",
    "    return sentences\n",
    "#print(context_test)\n",
    "context_test2 = split_into_sentences(context_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JUNK', 'THAT SHOULD', 'BE removed', 'when I have time', 'opps my bad']"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_sentences(df_master.iloc[8815,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    in which it is shown that phileas fogg gained nothing by his tour   around the world unless it were happiness    yes phileas fogg in person',\n",
       " ' the reader will remember that at five minutes past eight in the   evening  about five and twenty hours after the arrival of the   travellers in london  passepartout had been sent by his master to   engage the services of the reverend samuel wilson in a certain marriage   ceremony which was to take place the next day',\n",
       " ' passepartout went on his errand enchanted',\n",
       " '  he soon reached the   clergymans house but found him not at home',\n",
       " '  passepartout waited a   good twenty minutes and when he left the reverend gentleman it was   thirty five minutes past eight',\n",
       " '  but in what a state he was  with his   hair in disorder and without his hat he ran along the street as never   man was seen to run before overturning passers by rushing over the   sidewalk like a waterspout',\n",
       " ' in three minutes he was in saville row again and staggered back into   mr. foggs room',\n",
       " ' he could not speak',\n",
       " ' what is the matter asked mr. fogg',\n",
       " ' my master gasped passepartout  marriage  impossible   impossible impossible  for to morrow',\n",
       " ' why so because to morrow  is sunday monday replied mr. fogg',\n",
       " ' no  to day is saturday',\n",
       " ' saturday  impossible yes yes yes yes cried passepartout',\n",
       " '  you have made a mistake of   one day  we arrived twenty four hours ahead of time but there are   only ten minutes left passepartout had seized his master by the collar and was dragging him   along with irresistible force',\n",
       " ' phileas fogg thus kidnapped without having time to think left his   house jumped into a cab promised a hundred pounds to the cabman and   having run over two dogs and overturned five carriages reached the   reform club',\n",
       " ' the clock indicated a quarter before nine when he appeared in the great   saloon',\n",
       " ' phileas fogg had accomplished the journey round the world in eighty   days phileas fogg had won his wager of twenty thousand pounds how was it that a man so exact and fastidious could have made this   error of a day  how came he to think that he had arrived in london on   saturday the twenty first day of december when it was really friday   the twentieth the seventy ninth day only from his departure the cause of the error is very simple',\n",
       " ' phileas fogg had without suspecting it gained one day on his journey   and this merely because he had travelled constantly eastward he would   on the contrary have lost a day had he gone in the opposite direction   that is westward',\n",
       " ' in journeying eastward he had gone towards the sun and the days   therefore diminished for him as many times four minutes as he crossed   degrees in this direction',\n",
       " '  there are three hundred and sixty degrees   on the circumference of the earth and these three hundred and sixty   degrees multiplied by four minutes gives precisely twenty four   hours  that is the day unconsciously gained',\n",
       " '  in other words while   phileas fogg going eastward saw the sun pass the meridian eighty   times his friends in london only saw it pass the meridian seventy nine   times',\n",
       " '  this is why they awaited him at the reform club on saturday   and not sunday as mr. fogg thought',\n",
       " ' and passepartouts famous family watch which had always kept london   time would have betrayed this fact if it had marked the days as well   as the hours and the minutes phileas fogg then had won the twenty thousand pounds but as he had   spent nearly nineteen thousand on the way the pecuniary gain was   small',\n",
       " '  his object was however to be victorious and not to win   money',\n",
       " '  he divided the one thousand pounds that remained between   passepartout and the unfortunate fix against whom he cherished no   grudge',\n",
       " '  he deducted however from passepartouts share the cost of   the gas which had burned in his room for nineteen hundred and twenty   hours for the sake of regularity',\n",
       " ' that evening mr. fogg as tranquil and phlegmatic as ever said to   aouda is our marriage still agreeable to you mr. fogg replied she it is for me to ask that question',\n",
       " '  you were   ruined but now you are rich again',\n",
       " ' pardon me madam my fortune belongs to you',\n",
       " '  if you had not suggested   our marriage my servant would not have gone to the reverend samuel   wilsons i should not have been apprised of my error and   dear mr. fogg said the young woman',\n",
       " ' dear aouda replied phileas fogg',\n",
       " ' it need not be said that the marriage took place forty eight hours   after and that passepartout glowing and dazzling gave the bride   away',\n",
       " '  had he not saved her and was he not entitled to this honour the next day as soon as it was light passepartout rapped vigorously   at his masters door',\n",
       " '  mr. fogg opened it and asked whats the   matter passepartout what is it sir  why ive just this instant found out   what that we might have made the tour of the world in only seventy eight   days',\n",
       " ' no doubt returned mr. fogg by not crossing india',\n",
       " '  but if i had   not crossed india i should not have saved aouda she would not have   been my wife and   mr. fogg quietly shut the door',\n",
       " ' phileas fogg had won his wager and had made his journey around the   world in eighty days',\n",
       " '  to do this he had employed every means of   conveyance  steamers railways carriages yachts trading vessels   sledges elephants',\n",
       " '  the eccentric gentleman had throughout displayed   all his marvellous qualities of coolness and exactitude']"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_sentences(df_master.iloc[2,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  but in what a state he was  with his   hair in disorder and without his hat he ran along the street as never   man was seen to run before overturning passers by rushing over the   sidewalk like a waterspout'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_test2[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\r sir francis cromarty had observed the oddity of his travelling\\r\\r companion  although the only opportunity he had for studying him had\\r\\r been while he was dealing the cards and between two rubbers  and\\r\\r questioned himself whether a human heart really beat beneath this cold\\r\\r exterior and whether phileas fogg had any sense of the beauties of\\r\\r nature'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_test2[7].replace(\"-\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Compares the question to all other sentences in a set of chapters at once\n",
    "Mediocre performance on the test example as evidenced by the correct answer being 6th on the distance list, but I thought it might do better because the tfidf would weed out more words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.16587784, 0.03040895, ..., 0.        , 0.02963158,\n",
       "        0.03551618],\n",
       "       [0.16587784, 1.        , 0.04510202, ..., 0.06270105, 0.11354278,\n",
       "        0.04518333],\n",
       "       [0.03040895, 0.04510202, 1.        , ..., 0.        , 0.03917223,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.06270105, 0.        , ..., 1.        , 0.05238752,\n",
       "        0.        ],\n",
       "       [0.02963158, 0.11354278, 0.03917223, ..., 0.05238752, 1.        ,\n",
       "        0.01537483],\n",
       "       [0.03551618, 0.04518333, 0.        , ..., 0.        , 0.01537483,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_sent = []\n",
    "second_best = []\n",
    "third_best = []\n",
    "fourth_best = []\n",
    "fifth_best = []\n",
    "similiarity_lst = []\n",
    "#split up chapters into sentences\n",
    "context = split_into_sentences(context_test) \n",
    "context.append(test)\n",
    "#compare each sentence with the question and calc similarity\n",
    "#for sentence in range(0, len(context)):\n",
    "    #compare question and sentence\n",
    "vect = TfidfVectorizer(min_df=1)\n",
    "tfidf = vect.fit_transform(context)\n",
    "cos_matrix = (tfidf * tfidf.T).A\n",
    "cos_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in which it is shown that phileas fogg gained nothing by his tour\\r\\r around the world unless it were happiness\\r\\r \\r\\r \\r\\r yes phileas fogg in person.',\n",
       " 'the reader will remember that at five minutes past eight in the\\r\\r evening--about five and twenty hours after the arrival of the\\r\\r travellers in london--passepartout had been sent by his master to\\r\\r engage the services of the reverend samuel wilson in a certain marriage\\r\\r ceremony which was to take place the next day.',\n",
       " 'passepartout went on his errand enchanted.',\n",
       " 'he soon reached the\\r\\r clergymans house but found him not at home.',\n",
       " 'passepartout waited a\\r\\r good twenty minutes and when he left the reverend gentleman it was\\r\\r thirty-five minutes past eight.',\n",
       " 'but in what a state he was  with his\\r\\r hair in disorder and without his hat he ran along the street as never\\r\\r man was seen to run before overturning passers-by rushing over the\\r\\r sidewalk like a waterspout.',\n",
       " 'in three minutes he was in saville row again and staggered back into\\r\\r mr. foggs room.',\n",
       " 'he could not speak.',\n",
       " 'what is the matter asked mr. fogg.',\n",
       " 'my master gasped passepartout--marriage--impossible--\\r\\r \\r\\r impossible\\r\\r \\r\\r impossible--for to-morrow.',\n",
       " 'why so\\r\\r \\r\\r because to-morrow--is sunday\\r\\r \\r\\r monday replied mr. fogg.',\n",
       " 'no--to-day is saturday.',\n",
       " 'saturday  impossible\\r\\r \\r\\r yes yes yes yes cried passepartout.',\n",
       " 'you have made a mistake of\\r\\r one day  we arrived twenty-four hours ahead of time but there are\\r\\r only ten minutes left\\r\\r \\r\\r passepartout had seized his master by the collar and was dragging him\\r\\r along with irresistible force.',\n",
       " 'phileas fogg thus kidnapped without having time to think left his\\r\\r house jumped into a cab promised a hundred pounds to the cabman and\\r\\r having run over two dogs and overturned five carriages reached the\\r\\r reform club.',\n",
       " 'the clock indicated a quarter before nine when he appeared in the great\\r\\r saloon.',\n",
       " 'phileas fogg had accomplished the journey round the world in eighty\\r\\r days\\r\\r \\r\\r phileas fogg had won his wager of twenty thousand pounds\\r\\r \\r\\r how was it that a man so exact and fastidious could have made this\\r\\r error of a day  how came he to think that he had arrived in london on\\r\\r saturday the twenty-first day of december when it was really friday\\r\\r the twentieth the seventy-ninth day only from his departure\\r\\r \\r\\r the cause of the error is very simple.',\n",
       " 'phileas fogg had without suspecting it gained one day on his journey\\r\\r and this merely because he had travelled constantly eastward he would\\r\\r on the contrary have lost a day had he gone in the opposite direction\\r\\r that is westward.',\n",
       " 'in journeying eastward he had gone towards the sun and the days\\r\\r therefore diminished for him as many times four minutes as he crossed\\r\\r degrees in this direction.',\n",
       " 'there are three hundred and sixty degrees\\r\\r on the circumference of the earth and these three hundred and sixty\\r\\r degrees multiplied by four minutes gives precisely twenty-four\\r\\r hours--that is the day unconsciously gained.',\n",
       " 'in other words while\\r\\r phileas fogg going eastward saw the sun pass the meridian eighty\\r\\r times his friends in london only saw it pass the meridian seventy-nine\\r\\r times.',\n",
       " 'this is why they awaited him at the reform club on saturday\\r\\r and not sunday as mr. fogg thought.',\n",
       " 'and passepartouts famous family watch which had always kept london\\r\\r time would have betrayed this fact if it had marked the days as well\\r\\r as the hours and the minutes\\r\\r \\r\\r phileas fogg then had won the twenty thousand pounds but as he had\\r\\r spent nearly nineteen thousand on the way the pecuniary gain was\\r\\r small.',\n",
       " 'his object was however to be victorious and not to win\\r\\r money.',\n",
       " 'he divided the one thousand pounds that remained between\\r\\r passepartout and the unfortunate fix against whom he cherished no\\r\\r grudge.',\n",
       " 'he deducted however from passepartouts share the cost of\\r\\r the gas which had burned in his room for nineteen hundred and twenty\\r\\r hours for the sake of regularity.',\n",
       " 'that evening mr. fogg as tranquil and phlegmatic as ever said to\\r\\r aouda is our marriage still agreeable to you\\r\\r \\r\\r mr. fogg replied she it is for me to ask that question.',\n",
       " 'you were\\r\\r ruined but now you are rich again.',\n",
       " 'pardon me madam my fortune belongs to you.',\n",
       " 'if you had not suggested\\r\\r our marriage my servant would not have gone to the reverend samuel\\r\\r wilsons i should not have been apprised of my error and--\\r\\r \\r\\r dear mr. fogg said the young woman.',\n",
       " 'dear aouda replied phileas fogg.',\n",
       " 'it need not be said that the marriage took place forty-eight hours\\r\\r after and that passepartout glowing and dazzling gave the bride\\r\\r away.',\n",
       " 'had he not saved her and was he not entitled to this honour\\r\\r \\r\\r the next day as soon as it was light passepartout rapped vigorously\\r\\r at his masters door.',\n",
       " 'mr. fogg opened it and asked whats the\\r\\r matter passepartout\\r\\r \\r\\r what is it sir  why ive just this instant found out--\\r\\r \\r\\r what\\r\\r \\r\\r that we might have made the tour of the world in only seventy-eight\\r\\r days.',\n",
       " 'no doubt returned mr. fogg by not crossing india.',\n",
       " 'but if i had\\r\\r not crossed india i should not have saved aouda she would not have\\r\\r been my wife and--\\r\\r \\r\\r mr. fogg quietly shut the door.',\n",
       " 'phileas fogg had won his wager and had made his journey around the\\r\\r world in eighty days.',\n",
       " 'to do this he had employed every means of\\r\\r conveyance--steamers railways carriages yachts trading-vessels\\r\\r sledges elephants.',\n",
       " 'the eccentric gentleman had throughout displayed\\r\\r all his marvellous qualities of coolness and exactitude.',\n",
       " '180. how many days after returning home\\r\\r\\r\\r\\r\\nare fogg and aouda married']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array(cos_matrix)[-1]#[:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27, 35, 26, 31, 36,  3, 16, 18, 30, 39], dtype=int64)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'180. how many days after returning home\\r\\r\\r\\r\\r\\nare fogg and aouda married'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dear aouda replied phileas fogg.'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in journeying eastward he had gone towards the sun and the days\\r\\r therefore diminished for him as many times four minutes as he crossed\\r\\r degrees in this direction.'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phileas fogg had accomplished the journey round the world in eighty\\r\\r days\\r\\r \\r\\r phileas fogg had won his wager of twenty thousand pounds\\r\\r \\r\\r how was it that a man so exact and fastidious could have made this\\r\\r error of a day  how came he to think that he had arrived in london on\\r\\r saturday the twenty-first day of december when it was really friday\\r\\r the twentieth the seventy-ninth day only from his departure\\r\\r \\r\\r the cause of the error is very simple.'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he soon reached the\\r\\r clergymans house but found him not at home.'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phileas fogg had won his wager and had made his journey around the\\r\\r world in eighty days.'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it need not be said that the marriage took place forty-eight hours\\r\\r after and that passepartout glowing and dazzling gave the bride\\r\\r away.'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row 31 contains the answer so this method did not perform spectacuarly because the 6th sentence might not make the cut into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: compares only 1 sentence to the question\n",
    "Actually did much better on the test example (correct answer #4 best score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "top_sent = []\n",
    "second_best = []\n",
    "third_best = []\n",
    "fourth_best = []\n",
    "fifth_best = []\n",
    "similiarity_lst = []\n",
    "#split up chapters into sentences\n",
    "context = split_into_sentences(context_test)\n",
    "#compare each sentence with the question and calc similarity\n",
    "for sentence in range(0, len(context)):\n",
    "    #compare question and sentence\n",
    "    vect = TfidfVectorizer(min_df=1)\n",
    "    tfidf = vect.fit_transform([test, context[sentence]])\n",
    "    cos_matrix = (tfidf * tfidf.T).A\n",
    "    #find best alignment score\n",
    "    similiarity_lst.append(cos_matrix[0,1])\n",
    "position_top5 = np.array(similiarity_lst).argsort()[-5:][::-1]\n",
    "#store top values (didn't go with df or dict because it caused great pain trying to work into future LSTM code before)\n",
    "for top5_sent in range(0,4):\n",
    "    print (top5_sent)\n",
    "    cont_index = position_top5[top5_sent]\n",
    "    if top5_sent == 0: \n",
    "        top_sent.append(context[cont_index])\n",
    "    if top5_sent == 1: \n",
    "        second_best.append(context[cont_index])\n",
    "    if top5_sent == 2: \n",
    "        third_best.append(context[cont_index])\n",
    "    if top5_sent == 3: \n",
    "        fourth_best.append(context[cont_index])\n",
    "    if top5_sent == 4: \n",
    "        fifth_best.append(context[cont_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear aouda replied phileas fogg.']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phileas fogg had won his wager and had made his journey around the\\r\\r world in eighty days.']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing approach on the first question & context. The answer is found in the sentence with the 4th highest similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05358797469131102,\n",
       " 0.029493743944264034,\n",
       " 0.0,\n",
       " 0.043997326267831816,\n",
       " 0.032957177740365697,\n",
       " 0.02188454095615924,\n",
       " 0.03566946421355304,\n",
       " 0.0,\n",
       " 0.05850970413367294,\n",
       " 0.0,\n",
       " 0.04604360647426782,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.048847834143217424,\n",
       " 0.07188587407390887,\n",
       " 0.0,\n",
       " 0.075126382581545,\n",
       " 0.04076661992479196,\n",
       " 0.07970251181266944,\n",
       " 0.08456971633273137,\n",
       " 0.022370855988075977,\n",
       " 0.07188864279873158,\n",
       " 0.061212946253748325,\n",
       " 0.0406093257697673,\n",
       " 0.030782327506493744,\n",
       " 0.024369200829988422,\n",
       " 0.09303603605128495,\n",
       " 0.04604360647426782,\n",
       " 0.0,\n",
       " 0.04547582924081301,\n",
       " 0.15231415519375907,\n",
       " 0.08722192510054794,\n",
       " 0.024700713493349607,\n",
       " 0.0694071716911208,\n",
       " 0.051171029771728015,\n",
       " 0.08340941143953931,\n",
       " 0.10599213135093252,\n",
       " 0.0,\n",
       " 0.0406093257697673]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similiarity_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08456971633273137, 0.08722192510054794, 0.09303603605128495, 0.10599213135093252, 0.15231415519375907]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(sorted(similiarity_lst)[-5:])\n",
    "print(similiarity_lst.index(max(similiarity_lst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 36, 26, 31, 19], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(similiarity_lst).argsort()[-5:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'180. how many days after returning home\\r\\r\\r\\r\\r\\nare fogg and aouda married'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dear aouda replied phileas fogg.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phileas fogg had won his wager and had made his journey around the\\r\\r world in eighty days.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that evening mr. fogg as tranquil and phlegmatic as ever said to\\r\\r aouda is our marriage still agreeable to you\\r\\r \\r\\r mr. fogg replied she it is for me to ask that question.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it need not be said that the marriage took place forty-eight hours\\r\\r after and that passepartout glowing and dazzling gave the bride\\r\\r away.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3: No tfidf & sentence to question\n",
    "I began to wonder if in my 1:1 approach the overlapping words were important clues and being removed with tfidf, leaving the similiarity assessment to be based on extraneous words left behind. Just to prove to myself that countvect was not the way to go I ran method 3. The correct answer is no where in sight of the top sentences and the sentences that were selected were not remotely related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "top_sent = []\n",
    "second_best = []\n",
    "third_best = []\n",
    "fourth_best = []\n",
    "fifth_best = []\n",
    "similiarity_lst = []\n",
    "#split up chapters into sentences\n",
    "context = split_into_sentences(context_test)\n",
    "#compare each sentence with the question and calc similarity\n",
    "for sentence in range(0, len(context)):\n",
    "    #compare question and sentence\n",
    "    vect = CountVectorizer()\n",
    "    tfidf = vect.fit_transform([test, context[sentence]])\n",
    "    cos_matrix = (tfidf * tfidf.T).A\n",
    "    #find best alignment score\n",
    "    similiarity_lst.append(cos_matrix[0,1])\n",
    "position_top5 = np.array(similiarity_lst).argsort()[-5:][::-1]\n",
    "#store top values (didn't go with df or dict because it caused great pain trying to work into future LSTM code before)\n",
    "for top5_sent in range(0,5):\n",
    "    print (top5_sent)\n",
    "    cont_index = position_top5[top5_sent]\n",
    "    if top5_sent == 0: \n",
    "        top_sent.append(context[cont_index])\n",
    "    if top5_sent == 1: \n",
    "        second_best.append(context[cont_index])\n",
    "    if top5_sent == 2: \n",
    "        third_best.append(context[cont_index])\n",
    "    if top5_sent == 3: \n",
    "        fourth_best.append(context[cont_index])\n",
    "    if top5_sent == 4: \n",
    "        fifth_best.append(context[cont_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phileas fogg had accomplished the journey round the world in eighty\\r\\r days\\r\\r \\r\\r phileas fogg had won his wager of twenty thousand pounds\\r\\r \\r\\r how was it that a man so exact and fastidious could have made this\\r\\r error of a day  how came he to think that he had arrived in london on\\r\\r saturday the twenty-first day of december when it was really friday\\r\\r the twentieth the seventy-ninth day only from his departure\\r\\r \\r\\r the cause of the error is very simple.']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and passepartouts famous family watch which had always kept london\\r\\r time would have betrayed this fact if it had marked the days as well\\r\\r as the hours and the minutes\\r\\r \\r\\r phileas fogg then had won the twenty thousand pounds but as he had\\r\\r spent nearly nineteen thousand on the way the pecuniary gain was\\r\\r small.']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that evening mr. fogg as tranquil and phlegmatic as ever said to\\r\\r aouda is our marriage still agreeable to you\\r\\r \\r\\r mr. fogg replied she it is for me to ask that question.']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourth_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phileas fogg had won his wager and had made his journey around the\\r\\r world in eighty days.']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fifth_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similiarity_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Applying technique to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_sent = []\n",
    "similiarity_lst = []\n",
    "dict_lstm = dict()\n",
    "for row in range(0,len(df_master)):\n",
    "    if not isinstance((df_master.iloc[row, 28]), str):\n",
    "        dict_lstm[row] = ['JUNK']\n",
    "        top_sent.append('unk')\n",
    "        continue\n",
    "    context_test = df_master.iloc[row, 28]\n",
    "    if not isinstance((df_master.iloc[row, 26]), str):\n",
    "        dict_lstm[row] = ['JUNK']\n",
    "        top_sent.append('unk')\n",
    "        question = 'Not a real question'\n",
    "        #continue\n",
    "    else: question = df_master.iloc[row,26]\n",
    "    #split up chapters into sentences\n",
    "    context = split_into_sentences(context_test) \n",
    "    context.append(question)\n",
    "    #compare each sentence with the question and calc similarity\n",
    "    vect = TfidfVectorizer(min_df=1)\n",
    "    tfidf = vect.fit_transform(context)\n",
    "    cos_matrix = (tfidf * tfidf.T).A\n",
    "    cos_matrix\n",
    "    #find top matches\n",
    "    temp = np.array(cos_matrix)[-1]\n",
    "    position_top5 = temp.argsort()[-10:]\n",
    "    #store top values (didn't go with df or dict because it caused great pain trying to work into future LSTM code before)\n",
    "    for top5_sent in range(0,len(position_top5)):\n",
    "        cont_index = position_top5[top5_sent]\n",
    "        if top5_sent == 0: \n",
    "            top_sent.append(context[cont_index])\n",
    "    if not isinstance((df_master.iloc[row, 27]), str):\n",
    "        dict_lstm[row] = ['JUNK']\n",
    "        continue\n",
    "    answer = df_master.iloc[row,27]\n",
    "    #print(row)\n",
    "    #print('top sent', top_sent)\n",
    "    #print('top_sent indexed', top_sent[row])\n",
    "    #data prep for LSTM\n",
    "    dict_lstm[row] =  top_sent[row].split(), question.split(), answer.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['it',\n",
       "  'was',\n",
       "  'completely',\n",
       "  'covered',\n",
       "  'with',\n",
       "  'vines',\n",
       "  'climbing',\n",
       "  'roses',\n",
       "  'and',\n",
       "  'honeysuckles'],\n",
       " ['1.', 'which', 'genre', 'best', 'describes', 'this', 'novel'],\n",
       " ['autobiography'])"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dict_lstm[7019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['31.',\n",
       " 'who',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'following',\n",
       " 'statement',\n",
       " 'in',\n",
       " 'the',\n",
       " 'way',\n",
       " 'this',\n",
       " 'strange',\n",
       " 'gentleman',\n",
       " 'was',\n",
       " 'going',\n",
       " 'on',\n",
       " 'he',\n",
       " 'would',\n",
       " 'leave',\n",
       " 'the',\n",
       " 'world',\n",
       " 'without',\n",
       " 'having',\n",
       " 'done',\n",
       " 'any',\n",
       " 'good',\n",
       " 'to',\n",
       " 'himself',\n",
       " 'or',\n",
       " 'anybody',\n",
       " 'else.']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "180-128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0_x                                                  9900\n",
       "book_id                                                         87\n",
       "question_id                                                      1\n",
       "section_id                                                       1\n",
       "Gutenberg                                                        1\n",
       "﻿                                                             4221\n",
       "question         1. What is the state of Jacob Marley at\\r\\r\\r\\...\n",
       "answer                                                  He is dead\n",
       "tokens           ['1', '.', 'What', 'is', 'the', 'state', 'of',...\n",
       "what_ind                                                      True\n",
       "tokens2          ['', '.', 'what', 'is', 'the', 'state', 'of', ...\n",
       "how_ind                                                      False\n",
       "where_ind                                                    False\n",
       "when_ind                                                     False\n",
       "why_ind                                                      False\n",
       "who_ind                                                      False\n",
       "which_ind                                                    False\n",
       "question_type                                                 What\n",
       "answer_len                                                       3\n",
       "question_len                                                    14\n",
       "lexile_score                                                   900\n",
       "Unnamed: 0_y                                                  1448\n",
       "context          MARLEYS GHOST\\r\\r\\n\\r\\r\\n\\r\\r\\nMarley was dead...\n",
       "tags                                                      87\\1.txt\n",
       "master_text      1. what is the state of jacob marley at\\r\\r\\r\\...\n",
       "tokens_ans                                    ['he', 'is', 'dead']\n",
       "question_c       1. what is the state of jacob marley at\\r\\r\\r\\...\n",
       "answer_c                                                he is dead\n",
       "context_c        marleys ghost\\r\\r\\n\\r\\r\\n\\r\\r\\nmarley was dead...\n",
       "Name: 9900, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master.iloc[9870,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dictionary into train and test\n",
    "train_range = list(range(0,9870)) #10488  #10480,10492))\n",
    "test_range = list(range(9870,12918 )) #10497))#9720,12989))\n",
    "\n",
    "filterByKey = lambda keys: {x: dict_lstm[x] for x in keys}\n",
    "dict_train = filterByKey(train_range)\n",
    "dict_test = filterByKey(test_range)\n",
    "\n",
    "#print(dict_train,dict_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10480,\n",
       " 10481,\n",
       " 10482,\n",
       " 10483,\n",
       " 10484,\n",
       " 10485,\n",
       " 10486,\n",
       " 10487,\n",
       " 10488,\n",
       " 10489,\n",
       " 10490,\n",
       " 10491]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for key, value in dict_lstm.items():\n",
    "    temp = [key,value]\n",
    "    vocab.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['id', 'like', 'to', 'see', 'you', 'do', 'it', 'in', 'eighty', 'days'],\n",
       "  ['10.',\n",
       "   'how',\n",
       "   'much',\n",
       "   'is',\n",
       "   'wagered',\n",
       "   'against',\n",
       "   'fogg',\n",
       "   'being',\n",
       "   'able',\n",
       "   'to',\n",
       "   'travel',\n",
       "   'the',\n",
       "   'world',\n",
       "   'in',\n",
       "   '80',\n",
       "   'days'],\n",
       "  ['20000', 'pounds']),\n",
       " (['two',\n",
       "   'first',\n",
       "   'class',\n",
       "   'tickets',\n",
       "   'for',\n",
       "   'paris',\n",
       "   'having',\n",
       "   'been',\n",
       "   'speedily',\n",
       "   'purchased',\n",
       "   'mr.',\n",
       "   'fogg',\n",
       "   'was',\n",
       "   'crossing',\n",
       "   'the',\n",
       "   'station',\n",
       "   'to',\n",
       "   'the',\n",
       "   'train',\n",
       "   'when',\n",
       "   'he',\n",
       "   'perceived',\n",
       "   'his',\n",
       "   'five',\n",
       "   'friends',\n",
       "   'of',\n",
       "   'the',\n",
       "   'reform'],\n",
       "  ['9.',\n",
       "   'what',\n",
       "   'game',\n",
       "   'does',\n",
       "   'fogg',\n",
       "   'enjoy',\n",
       "   'playing',\n",
       "   'at',\n",
       "   'the',\n",
       "   'reform',\n",
       "   'club'],\n",
       "  ['whist']),\n",
       " (['well',\n",
       "   'ralph',\n",
       "   'said',\n",
       "   'thomas',\n",
       "   'flanagan',\n",
       "   'what',\n",
       "   'about',\n",
       "   'that',\n",
       "   'robbery',\n",
       "   'oh',\n",
       "   'replied',\n",
       "   'stuart',\n",
       "   'the',\n",
       "   'bank',\n",
       "   'will',\n",
       "   'lose',\n",
       "   'the',\n",
       "   'money'],\n",
       "  ['8.', 'how', 'much', 'is', 'stolen', 'from', 'the', 'bank'],\n",
       "  ['50000', 'pounds']),\n",
       " (['phileas',\n",
       "   'fogg',\n",
       "   'and',\n",
       "   'his',\n",
       "   'servant',\n",
       "   'seated',\n",
       "   'themselves',\n",
       "   'in',\n",
       "   'a',\n",
       "   'first',\n",
       "   'class',\n",
       "   'carriage',\n",
       "   'at',\n",
       "   'twenty',\n",
       "   'minutes',\n",
       "   'before',\n",
       "   'nine',\n",
       "   'five',\n",
       "   'minutes',\n",
       "   'later',\n",
       "   'the',\n",
       "   'whistle',\n",
       "   'screamed',\n",
       "   'and',\n",
       "   'the',\n",
       "   'train',\n",
       "   'slowly',\n",
       "   'glided',\n",
       "   'out',\n",
       "   'of',\n",
       "   'the',\n",
       "   'station'],\n",
       "  ['7.',\n",
       "   'what',\n",
       "   'bank',\n",
       "   'does',\n",
       "   'fogg',\n",
       "   'find',\n",
       "   'out',\n",
       "   'has',\n",
       "   'been',\n",
       "   'robbed'],\n",
       "  ['the', 'bank', 'of', 'england']),\n",
       " (['the',\n",
       "   'habits',\n",
       "   'of',\n",
       "   'its',\n",
       "   'occupant',\n",
       "   'were',\n",
       "   'such',\n",
       "   'as',\n",
       "   'to',\n",
       "   'demand',\n",
       "   'but',\n",
       "   'little',\n",
       "   'from',\n",
       "   'the',\n",
       "   'sole',\n",
       "   'domestic',\n",
       "   'but',\n",
       "   'phileas',\n",
       "   'fogg',\n",
       "   'required',\n",
       "   'him',\n",
       "   'to',\n",
       "   'be',\n",
       "   'almost',\n",
       "   'superhumanly',\n",
       "   'prompt',\n",
       "   'and',\n",
       "   'regular'],\n",
       "  ['6.', 'where', 'does', 'phileas', 'fogg', 'live'],\n",
       "  ['no.', '7', 'saville', 'row']),\n",
       " (['his',\n",
       "   'eyes',\n",
       "   'were',\n",
       "   'blue',\n",
       "   'his',\n",
       "   'complexion',\n",
       "   'rubicund',\n",
       "   'his',\n",
       "   'figure',\n",
       "   'almost',\n",
       "   'portly',\n",
       "   'and',\n",
       "   'well',\n",
       "   'built',\n",
       "   'his',\n",
       "   'body',\n",
       "   'muscular',\n",
       "   'and',\n",
       "   'his',\n",
       "   'physical',\n",
       "   'powers',\n",
       "   'fully',\n",
       "   'developed',\n",
       "   'by',\n",
       "   'the',\n",
       "   'exercises',\n",
       "   'of',\n",
       "   'his',\n",
       "   'younger',\n",
       "   'days'],\n",
       "  ['5.',\n",
       "   'to',\n",
       "   'what',\n",
       "   'does',\n",
       "   'passepartout',\n",
       "   'compare',\n",
       "   'his',\n",
       "   'new',\n",
       "   'employer'],\n",
       "  ['madame', 'tussands', 'wax', 'museum', 'figures']),\n",
       " (['as',\n",
       "   'for',\n",
       "   'passepartout',\n",
       "   'he',\n",
       "   'was',\n",
       "   'a',\n",
       "   'true',\n",
       "   'parisian',\n",
       "   'of',\n",
       "   'paris'],\n",
       "  ['4.', 'what', 'nationality', 'is', 'jean', 'passepartout'],\n",
       "  ['french']),\n",
       " (['in',\n",
       "   'which',\n",
       "   'phileas',\n",
       "   'fogg',\n",
       "   'and',\n",
       "   'passepartout',\n",
       "   'accept',\n",
       "   'each',\n",
       "   'other',\n",
       "   'the',\n",
       "   'one',\n",
       "   'as',\n",
       "   'master',\n",
       "   'the',\n",
       "   'other',\n",
       "   'as',\n",
       "   'man',\n",
       "   'mr.',\n",
       "   'phileas',\n",
       "   'fogg',\n",
       "   'lived',\n",
       "   'in',\n",
       "   '1872',\n",
       "   'at',\n",
       "   'no.',\n",
       "   '7',\n",
       "   'saville',\n",
       "   'row',\n",
       "   'burlington',\n",
       "   'gardens',\n",
       "   'the',\n",
       "   'house',\n",
       "   'in',\n",
       "   'which',\n",
       "   'sheridan',\n",
       "   'died',\n",
       "   'in',\n",
       "   '1814'],\n",
       "  ['3.', 'for', 'what', 'job', 'does', 'fogg', 'hire', 'passepartout'],\n",
       "  ['to', 'be', 'his', 'servant']),\n",
       " (['in',\n",
       "   'which',\n",
       "   'phileas',\n",
       "   'fogg',\n",
       "   'and',\n",
       "   'passepartout',\n",
       "   'accept',\n",
       "   'each',\n",
       "   'other',\n",
       "   'the',\n",
       "   'one',\n",
       "   'as',\n",
       "   'master',\n",
       "   'the',\n",
       "   'other',\n",
       "   'as',\n",
       "   'man',\n",
       "   'mr.',\n",
       "   'phileas',\n",
       "   'fogg',\n",
       "   'lived',\n",
       "   'in',\n",
       "   '1872',\n",
       "   'at',\n",
       "   'no.',\n",
       "   '7',\n",
       "   'saville',\n",
       "   'row',\n",
       "   'burlington',\n",
       "   'gardens',\n",
       "   'the',\n",
       "   'house',\n",
       "   'in',\n",
       "   'which',\n",
       "   'sheridan',\n",
       "   'died',\n",
       "   'in',\n",
       "   '1814'],\n",
       "  ['2.',\n",
       "   'what',\n",
       "   'is',\n",
       "   'the',\n",
       "   'only',\n",
       "   'establishment',\n",
       "   'of',\n",
       "   'which',\n",
       "   'fogg',\n",
       "   'is',\n",
       "   'a',\n",
       "   'member'],\n",
       "  ['the', 'reform', 'club', 'of', 'london']),\n",
       " (['during',\n",
       "   'his',\n",
       "   'brief',\n",
       "   'interview',\n",
       "   'with',\n",
       "   'mr.',\n",
       "   'fogg',\n",
       "   'passepartout',\n",
       "   'had',\n",
       "   'been',\n",
       "   'carefully',\n",
       "   'observing',\n",
       "   'him'],\n",
       "  ['1.',\n",
       "   'when',\n",
       "   'does',\n",
       "   'phileas',\n",
       "   'fogg',\n",
       "   'first',\n",
       "   'meet',\n",
       "   'jean',\n",
       "   'passepartout'],\n",
       "  ['october', '1872'])]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[170:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [item for sublist in vocab for item in sublist]\n",
    "vocab = [item for sublist in vocab for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lstm.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab.append('cannot')\n",
    "#vocab.append('streams.')\n",
    "#vocab.append('vari')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " run time (H:M:S) 00:00:47\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "run_time = time.strftime(\"%H:%M:%S\",time.gmtime(end_time - start_time))\n",
    "print (' run time (H:M:S) ' + run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncate run time (H:M:S) 00:00:00\n",
      "-\n",
      "Vocab size: 778814 unique words\n",
      "Story max length: 150 words\n",
      "Query max length: 40 words\n",
      "Number of training stories: 9870\n",
      "Number of test stories: 3048\n",
      "-\n",
      "Here's what a \"story\" tuple looks like (input, query, answer):\n",
      "(['you', 'were', 'ruined', 'but', 'now', 'you', 'are', 'rich', 'again'], ['180.', 'how', 'many', 'days', 'after', 'returning', 'home', 'are', 'fogg', 'and', 'aouda', 'married'], ['two'])\n",
      "-\n",
      "Vectorizing the word sequences...\n",
      "Jen answers_test [778726 778656 769663 ... 778748 745129 776480]\n",
      "-\n",
      "inputs: integer tensor of shape (samples, max_length)\n",
      "inputs_train shape: (9870, 150)\n",
      "inputs_test shape: (3048, 150)\n",
      "-\n",
      "queries: integer tensor of shape (samples, max_length)\n",
      "queries_train shape: (9870, 40)\n",
      "queries_test shape: (3048, 40)\n",
      "-\n",
      "answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n",
      "answers_train shape: (9870,)\n",
      "answers_test shape: (3048,)\n",
      "-\n",
      "Compiling...\n",
      "Vectorize run time (H:M:S) 00:00:00\n"
     ]
    }
   ],
   "source": [
    "'''Trains a memory network on the Classics Comprehension dataset \n",
    "Original code taken from: https://github.com/keras-team/keras/blob/master/examples/babi_memnn.py\n",
    "and modfied to fit a new dataset\n",
    "\n",
    "References:\n",
    "\n",
    "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
    "  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n",
    "  http://arxiv.org/abs/1502.05698\n",
    "\n",
    "- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n",
    "  \"End-To-End Memory Networks\",\n",
    "  http://arxiv.org/abs/1503.08895\n",
    "Reaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs.\n",
    "Time per epoch: 3s on CPU (core i7).\n",
    "'''\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "\n",
    "    '''Given the cleaned and tokenized dataset\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "\n",
    "    data = f #parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data):\n",
    "\n",
    "    inputs, queries, answers = [], [], []\n",
    "    i = 0\n",
    "    for story, query, answer in data:\n",
    "        #print(data)\n",
    "        #print('story view', story)\n",
    "        #print('query view', query)\n",
    "        #print (i)\n",
    "        ##print('answer view', answer)\n",
    "        i += 1\n",
    "        inputs.append([word_idx[w] for w in story])\n",
    "        queries.append([word_idx[w] for w in query])\n",
    "        #if i == 1:\n",
    "        answers.append(max([word_idx[w] for w in answer])) #to just do first word for now\n",
    "        #print(answer)\n",
    "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
    "            pad_sequences(queries, maxlen=query_maxlen),\n",
    "            np.array(answers))\n",
    "\n",
    "#taken from csv input above & then passed through function to truncate the context to the appropriate length\n",
    "train_stories = dict_train\n",
    "test_stories = dict_test\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = 150 #max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = 40 #max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "ans_maxlen = 1 #new addition, just to test code for now, #MVP\n",
    "\n",
    "end_time = time.time()\n",
    "run_time = time.strftime(\"%H:%M:%S\",time.gmtime(end_time - start_time))\n",
    "print ('Truncate run time (H:M:S) ' + run_time)\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[0])\n",
    "print('-')\n",
    "print('Vectorizing the word sequences...')\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#print(\"jen's vocab final test\", type(vocab))\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories.values())\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories.values())\n",
    "\n",
    "print(\"Jen answers_test\", answers_test)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n",
    "print('-')\n",
    "print('Compiling...')\n",
    "\n",
    "end_time = time.time()\n",
    "run_time = time.strftime(\"%H:%M:%S\",time.gmtime(end_time - start_time))\n",
    "print ('Vectorize run time (H:M:S) ' + run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually running Model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9870 samples, validate on 3048 samples\n",
      "Epoch 1/10\n",
      "9870/9870 [==============================] - 1043s 106ms/step - loss: 8.7056 - acc: 0.1882 - val_loss: 7.3833 - val_acc: 0.1608\n",
      "Epoch 2/10\n",
      "9870/9870 [==============================] - 1019s 103ms/step - loss: 6.3552 - acc: 0.1902 - val_loss: 7.5929 - val_acc: 0.1608\n",
      "Epoch 3/10\n",
      "9870/9870 [==============================] - 1021s 103ms/step - loss: 6.3291 - acc: 0.1902 - val_loss: 7.2121 - val_acc: 0.1608\n",
      "Epoch 4/10\n",
      "9870/9870 [==============================] - 1020s 103ms/step - loss: 6.1684 - acc: 0.1902 - val_loss: 7.2845 - val_acc: 0.1608\n",
      "Epoch 5/10\n",
      "9870/9870 [==============================] - 1019s 103ms/step - loss: 6.0651 - acc: 0.1902 - val_loss: 7.1983 - val_acc: 0.1608\n",
      "Epoch 6/10\n",
      "9870/9870 [==============================] - 1019s 103ms/step - loss: 5.9770 - acc: 0.1902 - val_loss: 7.1618 - val_acc: 0.1608\n",
      "Epoch 7/10\n",
      "9870/9870 [==============================] - 1020s 103ms/step - loss: 5.8870 - acc: 0.1903 - val_loss: 7.2400 - val_acc: 0.1601\n",
      "Epoch 8/10\n",
      "9870/9870 [==============================] - 1021s 103ms/step - loss: 5.8296 - acc: 0.1895 - val_loss: 7.2092 - val_acc: 0.1572\n",
      "Epoch 9/10\n",
      "9870/9870 [==============================] - 1019s 103ms/step - loss: 5.7687 - acc: 0.1910 - val_loss: 7.2818 - val_acc: 0.1381\n",
      "Epoch 10/10\n",
      "9870/9870 [==============================] - 1021s 103ms/step - loss: 5.6966 - acc: 0.1894 - val_loss: 7.3424 - val_acc: 0.1316\n",
      "Model run time (H:M:S) 02:50:24\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# placeholders\n",
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size, output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "\n",
    "# output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size, output_dim=query_maxlen))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "\n",
    "# output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "\n",
    "\n",
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=query_maxlen))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "\n",
    "# output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "\n",
    "\n",
    "# encode input sequence and questions (which are indices)\n",
    "\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "\n",
    "# compute a 'match' between the first input vector sequence and the question vector sequence\n",
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "\n",
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "\n",
    "# the original paper uses a matrix multiplication for this reduction step.\n",
    "\n",
    "# we choose to use a RNN instead.\n",
    "answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "# one regularization layer -- more would probably be needed.\n",
    "answer = Dropout(0.3)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "model.fit([inputs_train, queries_train], answers_train, batch_size=32, epochs=10,\n",
    "          validation_data=([inputs_test, queries_test], answers_test))\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "end_time = time.time()\n",
    "run_time = time.strftime(\"%H:%M:%S\",time.gmtime(end_time - start_time))\n",
    "print ('Model run time (H:M:S) ' + run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5.1\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-191-d03e0b2bcd56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1833\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1834\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[1;32m-> 1835\u001b[1;33m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1837\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1316\u001b[0m             \u001b[1;31m# Sample-based predictions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1318\u001b[1;33m             \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1319\u001b[0m             \u001b[0mindex_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_end\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_batches\u001b[1;34m(size, batch_size)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[0mnum_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m  \u001b[1;31m# round up\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     return [(i * batch_size, min(size, (i + 1) * batch_size))\n\u001b[1;32m--> 355\u001b[1;33m             for i in range(num_batches)]\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "predictions = model.predict([inputs_test, queries_test], answers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[inputs_test, queries_test], answers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[4,31] = 781481 is not in [0, 778814)\n\t [[Node: sequential_3/embedding_3/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_3/embeddings/read, sequential_3/embedding_3/Cast)]]\n\nCaused by op 'sequential_3/embedding_3/Gather', defined at:\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-103-68eeaed10204>\", line 37, in <module>\n    question_encoded = question_encoder(question)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 549, in call\n    return self.model.call(inputs, mask)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 2085, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 2236, in run_internal_graph\n    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 138, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1211, in gather\n    return tf.gather(reference, indices)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2698, in gather\n    params, indices, validate_indices=validate_indices, name=name)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3145, in gather\n    validate_indices=validate_indices, name=name)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[4,31] = 781481 is not in [0, 778814)\n\t [[Node: sequential_3/embedding_3/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_3/embeddings/read, sequential_3/embedding_3/Cast)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[4,31] = 781481 is not in [0, 778814)\n\t [[Node: sequential_3/embedding_3/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_3/embeddings/read, sequential_3/embedding_3/Cast)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-233-ec8fc050b245>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1833\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1834\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[1;32m-> 1835\u001b[1;33m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1837\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[4,31] = 781481 is not in [0, 778814)\n\t [[Node: sequential_3/embedding_3/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_3/embeddings/read, sequential_3/embedding_3/Cast)]]\n\nCaused by op 'sequential_3/embedding_3/Gather', defined at:\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-103-68eeaed10204>\", line 37, in <module>\n    question_encoded = question_encoder(question)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 549, in call\n    return self.model.call(inputs, mask)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 2085, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 2236, in run_internal_graph\n    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 138, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1211, in gather\n    return tf.gather(reference, indices)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2698, in gather\n    params, indices, validate_indices=validate_indices, name=name)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3145, in gather\n    validate_indices=validate_indices, name=name)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[4,31] = 781481 is not in [0, 778814)\n\t [[Node: sequential_3/embedding_3/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_3/embeddings/read, sequential_3/embedding_3/Cast)]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict([inputs_test, queries_test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2 Applied to enitre dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sent2 = []\n",
    "dict_lstm2 = dict()\n",
    "for row in range(0,len(df_master)):\n",
    "    #print(row)\n",
    "    similiarity_lst = []\n",
    "    if not isinstance((df_master.iloc[row, 28]), str):\n",
    "        dict_lstm2[row] = ['JUNK']\n",
    "        top_sent2.append('unk')\n",
    "        continue\n",
    "    context_test = df_master.iloc[row, 28]\n",
    "    if not isinstance((df_master.iloc[row, 26]), str):\n",
    "        dict_lstm2[row] = ['JUNK']\n",
    "        top_sent2.append('unk')\n",
    "        question = 'Not a real question'\n",
    "        #continue\n",
    "    else: question = df_master.iloc[row,26]\n",
    "    #split up chapters into sentences\n",
    "    context = split_into_sentences(context_test)\n",
    "    #compare each sentence with the question and calc similarity\n",
    "    for sentence in range(0, len(context)):\n",
    "        #compare question and sentence\n",
    "        vect = TfidfVectorizer(min_df=1)\n",
    "        tfidf = vect.fit_transform([question, context[sentence]])\n",
    "        cos_matrix = (tfidf * tfidf.T).A\n",
    "        #find best alignment score\n",
    "        similiarity_lst.append(cos_matrix[0,1])\n",
    "    position_top5 = np.array(similiarity_lst).argsort()[-5:][::-1]\n",
    "    #store top values (didn't go with df or dict because it caused great pain trying to work into future LSTM code before)\n",
    "    for top5_sent in range(0,2):\n",
    "        #print (top5_sent)\n",
    "        cont_index = position_top5[top5_sent]\n",
    "        if top5_sent == 0: \n",
    "            top_sent2.append(context[cont_index])\n",
    "    if not isinstance((df_master.iloc[row, 27]), str):\n",
    "        dict_lstm2[row] = ['JUNK']\n",
    "        continue\n",
    "    answer = df_master.iloc[row,27]\n",
    "    dict_lstm2[row] =  top_sent2[row].split(), question.split(), answer.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dictionary into train and test\n",
    "train_range = list(range(0,9870)) #10488  #10480,10492))\n",
    "test_range = list(range(9870,12918 )) #10497))#9720,12989))\n",
    "\n",
    "filterByKey = lambda keys: {x: dict_lstm2[x] for x in keys}\n",
    "dict_train2 = filterByKey(train_range)\n",
    "dict_test2 = filterByKey(test_range)\n",
    "\n",
    "vocab2 = []\n",
    "for key, value in dict_lstm2.items():\n",
    "    temp = [key,value]\n",
    "    vocab2.append(value)\n",
    "    \n",
    "vocab2 = [item for sublist in vocab2 for item in sublist]\n",
    "vocab2 = [item for sublist in vocab2 for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0_x                                                 12954\n",
       "book_id                                                        106\n",
       "question_id                                                      1\n",
       "section_id                                                       1\n",
       "Gutenberg                                                        1\n",
       "﻿                                                             1701\n",
       "question                         1. Where does Aunt Polly find Tom\n",
       "answer                                                 in a closet\n",
       "tokens           ['1', '.', 'Where', 'does', 'Aunt', 'Polly', '...\n",
       "what_ind                                                     False\n",
       "tokens2          ['', '.', 'where', 'does', 'aunt', 'polly', 'f...\n",
       "how_ind                                                      False\n",
       "where_ind                                                     True\n",
       "when_ind                                                     False\n",
       "why_ind                                                      False\n",
       "who_ind                                                      False\n",
       "which_ind                                                    False\n",
       "question_type                                                Where\n",
       "answer_len                                                       3\n",
       "question_len                                                     7\n",
       "lexile_score                                                   970\n",
       "Unnamed: 0_y                                                   104\n",
       "context          PREFACE\\r\\r\\n\\r\\r\\nMost of the adventures reco...\n",
       "tags                                                     106\\1.txt\n",
       "master_text      1. where does aunt polly find tom preface\\r\\r\\...\n",
       "tokens_ans                                   ['in', 'a', 'closet']\n",
       "question_c                       1. where does aunt polly find tom\n",
       "answer_c                                               in a closet\n",
       "context_c        preface\\r\\r\\n\\r\\r\\nmost of the adventures reco...\n",
       "Name: 12954, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master.iloc[12918,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model Outline: LSTM method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncate run time (H:M:S) 00:00:00\n",
      "-\n",
      "Vocab size: 781744 unique words\n",
      "Story max length: 150 words\n",
      "Query max length: 40 words\n",
      "Number of training stories: 9870\n",
      "Number of test stories: 3048\n",
      "-\n",
      "Here's what a \"story\" tuple looks like (input, query, answer):\n",
      "(['dear', 'aouda', 'replied', 'phileas', 'fogg'], ['180.', 'how', 'many', 'days', 'after', 'returning', 'home', 'are', 'fogg', 'and', 'aouda', 'married'], ['two'])\n",
      "-\n",
      "Vectorizing the word sequences...\n",
      "Jen answers_test [781638 781722 733857 ... 781675 759865 779008]\n",
      "-\n",
      "inputs: integer tensor of shape (samples, max_length)\n",
      "inputs_train shape: (9870, 150)\n",
      "inputs_test shape: (3048, 150)\n",
      "-\n",
      "queries: integer tensor of shape (samples, max_length)\n",
      "queries_train shape: (9870, 40)\n",
      "queries_test shape: (3048, 40)\n",
      "-\n",
      "answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n",
      "answers_train shape: (9870,)\n",
      "answers_test shape: (3048,)\n",
      "-\n",
      "Compiling...\n",
      "Vectorize run time (H:M:S) 00:00:02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "\n",
    "    '''Given the cleaned and tokenized dataset\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "\n",
    "    data = f #parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data):\n",
    "\n",
    "    inputs, queries, answers = [], [], []\n",
    "    i = 0\n",
    "    for story, query, answer in data:\n",
    "        #print(data)\n",
    "        #print('story view', story)\n",
    "        #print('query view', query)\n",
    "        #print (i)\n",
    "        ##print('answer view', answer)\n",
    "        i += 1\n",
    "        inputs.append([word_idx[w] for w in story])\n",
    "        queries.append([word_idx[w] for w in query])\n",
    "        #if i == 1:\n",
    "        answers.append(max([word_idx[w] for w in answer])) #to just do first word for now\n",
    "        #print(answer)\n",
    "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
    "            pad_sequences(queries, maxlen=query_maxlen),\n",
    "            np.array(answers))\n",
    "\n",
    "#taken from csv input above & then passed through function to truncate the context to the appropriate length\n",
    "train_stories = dict_train2\n",
    "test_stories = dict_test2\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab2) + 1\n",
    "story_maxlen = 150 #max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = 40 #max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "ans_maxlen = 1 #new addition, just to test code for now, #MVP\n",
    "\n",
    "end_time = time.time()\n",
    "run_time = time.strftime(\"%H:%M:%S\",time.gmtime(end_time - start_time))\n",
    "print ('Truncate run time (H:M:S) ' + run_time)\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[0])\n",
    "print('-')\n",
    "print('Vectorizing the word sequences...')\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#print(\"jen's vocab final test\", type(vocab))\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab2))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories.values())\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories.values())\n",
    "\n",
    "print(\"Jen answers_test\", answers_test)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n",
    "print('-')\n",
    "print('Compiling...')\n",
    "\n",
    "end_time = time.time()\n",
    "run_time = time.strftime(\"%H:%M:%S\",time.gmtime(end_time - start_time))\n",
    "print ('Vectorize run time (H:M:S) ' + run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0_x                                                  8814\n",
       "book_id                                                         12\n",
       "question_id                                                    175\n",
       "section_id                                                      25\n",
       "Gutenberg                                                        1\n",
       "﻿                                                             1338\n",
       "question                               175. There is no chapter 25\n",
       "answer                                 176. There is no chapter 25\n",
       "tokens           ['175', '.', 'There', 'is', 'no', 'chapter', '...\n",
       "what_ind                                                     False\n",
       "tokens2              ['', '.', 'there', 'is', 'no', 'chapter', '']\n",
       "how_ind                                                      False\n",
       "where_ind                                                    False\n",
       "when_ind                                                     False\n",
       "why_ind                                                      False\n",
       "who_ind                                                      False\n",
       "which_ind                                                    False\n",
       "question_type                                                Other\n",
       "answer_len                                                       6\n",
       "question_len                                                     6\n",
       "lexile_score                                                  1000\n",
       "Unnamed: 0_y                                                   NaN\n",
       "context                                                        NaN\n",
       "tags                                                           NaN\n",
       "master_text                                                    NaN\n",
       "tokens_ans       ['176', '.', 'there', 'is', 'no', 'chapter', '...\n",
       "question_c                             175. there is no chapter 25\n",
       "answer_c                               176. there is no chapter 25\n",
       "context_c                                                      NaN\n",
       "Name: 8814, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master.iloc[8784,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['where',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'did',\n",
       "  'you',\n",
       "  'come',\n",
       "  'from',\n",
       "  'from',\n",
       "  'the',\n",
       "  'land',\n",
       "  'of',\n",
       "  'oz',\n",
       "  'said',\n",
       "  'dorothy',\n",
       "  'gravely'],\n",
       " ['171.', 'what', 'is', 'the', 'name', 'of', 'this', 'chapter'],\n",
       " ['home', 'again'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_train2[8780]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9870 samples, validate on 3048 samples\n",
      "Epoch 1/10\n",
      "9870/9870 [==============================] - 1026s 104ms/step - loss: 8.7446 - acc: 0.1927 - val_loss: 7.4101 - val_acc: 0.1627\n",
      "Epoch 2/10\n",
      "9870/9870 [==============================] - 1025s 104ms/step - loss: 6.2379 - acc: 0.1945 - val_loss: 7.2210 - val_acc: 0.1627\n",
      "Epoch 3/10\n",
      "9870/9870 [==============================] - 1024s 104ms/step - loss: 6.1128 - acc: 0.1945 - val_loss: 7.2878 - val_acc: 0.1627\n",
      "Epoch 4/10\n",
      "9870/9870 [==============================] - 1026s 104ms/step - loss: 6.0607 - acc: 0.1945 - val_loss: 7.1686 - val_acc: 0.1627\n",
      "Epoch 5/10\n",
      "9870/9870 [==============================] - 1025s 104ms/step - loss: 6.0062 - acc: 0.1945 - val_loss: 7.1540 - val_acc: 0.1627\n",
      "Epoch 6/10\n",
      "9870/9870 [==============================] - 1037s 105ms/step - loss: 5.9536 - acc: 0.1945 - val_loss: 7.2025 - val_acc: 0.1627\n",
      "Epoch 7/10\n",
      "9870/9870 [==============================] - 1027s 104ms/step - loss: 5.8794 - acc: 0.1944 - val_loss: 7.2012 - val_acc: 0.1627\n",
      "Epoch 8/10\n",
      "9870/9870 [==============================] - 1056s 107ms/step - loss: 5.8315 - acc: 0.1942 - val_loss: 7.2214 - val_acc: 0.1608\n",
      "Epoch 9/10\n",
      "9870/9870 [==============================] - 1067s 108ms/step - loss: 5.7737 - acc: 0.1937 - val_loss: 7.2599 - val_acc: 0.1542\n",
      "Epoch 10/10\n",
      "9870/9870 [==============================] - 1004s 102ms/step - loss: 5.7188 - acc: 0.1918 - val_loss: 7.2970 - val_acc: 0.1394\n",
      "Model run time (H:M:S) 02:51:57\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# placeholders\n",
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size, output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "\n",
    "# output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size, output_dim=query_maxlen))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "\n",
    "# output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "\n",
    "\n",
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=query_maxlen))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "\n",
    "# output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "\n",
    "\n",
    "# encode input sequence and questions (which are indices)\n",
    "\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "\n",
    "# compute a 'match' between the first input vector sequence and the question vector sequence\n",
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "\n",
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "\n",
    "# the original paper uses a matrix multiplication for this reduction step.\n",
    "\n",
    "# we choose to use a RNN instead.\n",
    "answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "# one regularization layer -- more would probably be needed.\n",
    "answer = Dropout(0.3)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# build the final model\n",
    "model2 = Model([input_sequence, question], answer)\n",
    "model2.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "model2.fit([inputs_train, queries_train], answers_train, batch_size=32, epochs=10,\n",
    "          validation_data=([inputs_test, queries_test], answers_test))\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "end_time = time.time()\n",
    "run_time = time.strftime(\"%H:%M:%S\",time.gmtime(end_time - start_time))\n",
    "print ('Model run time (H:M:S) ' + run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model2_json = model2.to_json()\n",
    "with open(\"model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model2_json)\n",
    "# serialize weights to HDF5\n",
    "model2.save_weights(\"model.h2\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Comparison Model Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformatting the data for BoW model\n",
    "method1_test_data = [] \n",
    "method1_test_label = [] \n",
    "for passage, question, answer in dict_test.values():\n",
    "    method1_test_data.append(' '.join(passage + question))\n",
    "    method1_test_label.append(' '.join(answer))\n",
    "df_m1_test_data = pd.DataFrame(method1_test_data)    \n",
    "df_m1_test_label = pd.DataFrame(method1_test_label)\n",
    "\n",
    "method1_train_data = [] \n",
    "method1_train_label = [] \n",
    "for passage, question, answer in dict_train.values():\n",
    "    method1_train_data.append(' '.join(passage + question))\n",
    "    method1_train_label.append(' '.join(answer))\n",
    "df_m1_train_data = pd.DataFrame(method1_train_data)    \n",
    "df_m1_train_label = pd.DataFrame(method1_train_label)\n",
    "\n",
    "df_m1_train_data.fillna(\"\", inplace=True)  \n",
    "df_m1_test_data.fillna(\"\", inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marleys ghost marley was dead to begin with 2. what is scrooges relationship to jacob marley'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(method1_test_data)\n",
    "method1_test_data[1]\n",
    "#method1_test_label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    you were ruined but now you are rich again 180...\n",
      "1    the reader will remember that at five minutes ...\n",
      "2    he deducted however from passepartouts share t...\n",
      "3    passepartout went on his errand enchanted 177....\n",
      "4    you have made a mistake of one day we arrived ...\n",
      "Name: 0, dtype: object\n",
      "0    is that the chance and hope you mentioned jaco...\n",
      "1    marleys ghost marley was dead to begin with 2....\n",
      "2    why did i walk through crowds of fellow beings...\n",
      "3    mind i dont mean to say that i know of my own ...\n",
      "4    you wish to be anonymous i wish to be left alo...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_m1_train_data = df_m1_train_data[0].astype(str)\n",
    "print(df_m1_train_data.head()) #.astype(str)\n",
    "df_m1_test_data = df_m1_test_data[0] #.astype(str)\n",
    "print(df_m1_test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data (3048,) test labels (3048, 1) train data (9870,) train labels (9870, 1)\n"
     ]
    }
   ],
   "source": [
    "print('test data', df_m1_test_data.shape, 'test labels', df_m1_test_label.shape, 'train data', df_m1_train_data.shape, 'train labels', df_m1_train_label.shape)\n",
    "#type(df_m2_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new test label shape (3048,) new train label shape (9870, 1)\n"
     ]
    }
   ],
   "source": [
    "df_m1_test_label = df_m1_test_label[0].ravel()\n",
    "#df_m1_train_data = df_m1_train_data.ravel()\n",
    "print('new test label shape', df_m1_test_label.shape, 'new train label shape', df_m1_train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the TFIDF leveraged, the vocabulary size shrinks to : 20730\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import * \n",
    "##################################################\n",
    "vectorizer = TfidfVectorizer(lowercase=False) #already converted to lower case earlier, no need to run 2x\n",
    "train_vec = vectorizer.fit_transform(df_m1_train_data)\n",
    "test_vec = vectorizer.transform(df_m1_test_data)\n",
    "\n",
    "print (\"With the TFIDF leveraged, the vocabulary size shrinks to : \" + str(train_vec.shape[1]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model run time (H:M:S) 00:12:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Model with no preprocessing \n",
    "tfidf_LR_model_m1 = LogisticRegression(penalty=\"l2\", C = 100)\n",
    "tfidf_LR_model_m1.fit(train_vec, df_m1_train_label)\n",
    "\n",
    "#makes predictions on the dev dataset with the model I just built\n",
    "tfidf_dev_pred_labels_m1 = tfidf_LR_model_m1.predict(test_vec)\n",
    "tfidf_dev_pred_probs_m1 = tfidf_LR_model_m1.predict_proba(test_vec)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(df_m1_test_label, tfidf_dev_pred_labels_m1, average='weighted')\n",
    "\n",
    "end_time = time.time()\n",
    "run_time = time.strftime(\"%H:%M:%S\",time.gmtime(end_time - start_time))\n",
    "print ('Model run time (H:M:S) ' + run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0017450405390158335"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(df_m1_test_label, tfidf_dev_pred_labels_m1, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Comparison Model Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dictionary into train and test\n",
    "train_range = list(range(0,9870)) #10488  #10480,10492))\n",
    "test_range = list(range(9870,12918 )) #10497))#9720,12989))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformatting the data for BoW model\n",
    "method2_test_data = [] \n",
    "method2_test_label = [] \n",
    "for passage, question, answer in dict_test2.values():\n",
    "    method2_test_data.append(' '.join(passage + question))\n",
    "    method2_test_label.append(' '.join(answer))\n",
    "df_m2_test_data = pd.DataFrame(method2_test_data)    \n",
    "df_m2_test_label = pd.DataFrame(method2_test_label)\n",
    "\n",
    "method2_train_data = [] \n",
    "method2_train_label = [] \n",
    "for passage, question, answer in dict_train2.values():\n",
    "    method2_train_data.append(' '.join(passage + question))\n",
    "    method2_train_label.append(' '.join(answer))\n",
    "df_m2_train_data = pd.DataFrame(method2_train_data)    \n",
    "df_m2_train_label = pd.DataFrame(method2_train_label)\n",
    "\n",
    "df_m2_train_data.fillna(\"\", inplace=True)  \n",
    "df_m2_test_data.fillna(\"\", inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'old jacob marley tell me more speak comfort to me jacob i have none to give the ghost replied 2. what is scrooges relationship to jacob marley'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(method2_test_data)\n",
    "method2_test_data[1]\n",
    "#method2_test_label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    dear aouda replied phileas fogg 180. how many ...\n",
       "1    phileas fogg had won his wager and had made hi...\n",
       "2    mr. fogg opened it and asked whats the matter ...\n",
       "3    the reader will remember that at five minutes ...\n",
       "4    passepartout went on his errand enchanted 176....\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m2_train_data = df_m2_train_data[0].astype(str)\n",
    "df_m2_train_data.head() #.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       and being from the emotion he had undergone or...\n",
       "1       old jacob marley tell me more speak comfort to...\n",
       "2       scrooge stopped 3. which word best describes s...\n",
       "3       expect the first to morrow when the bell tolls...\n",
       "4       scrooge was not a man to be frightened by echo...\n",
       "5       scrooge followed to the window desperate in hi...\n",
       "6                    it is 7. what is scrooges first name\n",
       "7       he went the whole length of the expression and...\n",
       "8       but he couldnt replenish it for scrooge kept t...\n",
       "9       but i am sure i have always thought of christm...\n",
       "10      on the wings of the wind replied the ghost 11....\n",
       "11      to edge his way along the crowded paths of lif...\n",
       "12      but how much greater was his horror when the p...\n",
       "13      scrooge followed to the window desperate in hi...\n",
       "14      and being from the emotion he had undergone or...\n",
       "15      it is also a fact that scrooge had seen it nig...\n",
       "16      it is also a fact that scrooge had seen it nig...\n",
       "17      mind i dont mean to say that i know of my own ...\n",
       "18      i must 19. what must marley carry around with ...\n",
       "19      what do you want with me much marleys voice no...\n",
       "20      sometimes people new to the business called sc...\n",
       "21      nobody ever stopped him in the street to say w...\n",
       "22      to edge his way along the crowded paths of lif...\n",
       "23      i will said scrooge 24. what does marley tell ...\n",
       "24      its not convenient said scrooge and its not fa...\n",
       "25                it is 26. what is scrooges nephews name\n",
       "26      and the union workhouses demanded scrooge 27. ...\n",
       "27      the fog and frost so hung about the black old ...\n",
       "28      marleys ghost marley was dead to begin with 29...\n",
       "29      on the wings of the wind replied the ghost 30....\n",
       "                              ...                        \n",
       "3018    i want you to postpone all other engagements f...\n",
       "3019    lanyon you remember your vows what follows is ...\n",
       "3020    and now said he to settle what remains 153. wh...\n",
       "3021    the creature who crept into my house that nigh...\n",
       "3022    are you come from dr. jekyll i asked 155. what...\n",
       "3023    the door of my cabinet is then to be forced an...\n",
       "3024    the door of my cabinet is then to be forced an...\n",
       "3025    lanyon you remember your vows what follows is ...\n",
       "3026    lanyon you remember your vows what follows is ...\n",
       "3027    the door of my cabinet is then to be forced an...\n",
       "3028    the door of my cabinet is then to be forced an...\n",
       "3029    and now said he to settle what remains 162. ac...\n",
       "3030    yet it was by these that i was punished 163. w...\n",
       "3031    to cast in my lot with jekyll was to die to th...\n",
       "3032    his terror of the gallows drove him continuall...\n",
       "3033    he had now seen the full deformity of that cre...\n",
       "3034    under the strain of this continually impending...\n",
       "3035    to cast in my lot with jekyll was to die to th...\n",
       "3036    yet it was by these that i was punished 169. w...\n",
       "3037    hyde was thenceforth impossible whether i woul...\n",
       "3038    hyde was thenceforth impossible whether i woul...\n",
       "3039    now however and in the light of that mornings ...\n",
       "3040    yet it was by these that i was punished 173. w...\n",
       "3041    i slept after the prostration of the day with ...\n",
       "3042    hyde was thenceforth impossible whether i woul...\n",
       "3043    i next drew up that will to which you so much ...\n",
       "3044    hyde was thenceforth impossible whether i woul...\n",
       "3045    the powers of hyde seemed to have grown with t...\n",
       "3046    with jekyll it was a thing of vital instinct 1...\n",
       "3047    his terror of the gallows drove him continuall...\n",
       "Name: 0, Length: 3048, dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m2_test_data = df_m2_test_data[0] #.astype(str)\n",
    "df_m2_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data (3048,) test labels (3048, 1) train data (9870,) train labels (9870, 1)\n"
     ]
    }
   ],
   "source": [
    "print('test data', df_m2_test_data.shape, 'test labels', df_m2_test_label.shape, 'train data', df_m2_train_data.shape, 'train labels', df_m2_train_label.shape)\n",
    "#type(df_m2_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import * \n",
    "##################################################\n",
    "#create n grams\n",
    "#default CountVectorizer options \n",
    "#singleGram \n",
    "vectorizer = TfidfVectorizer(lowercase=False) #already converted to lower case earlier, no need to run 2x\n",
    "train_vec = vectorizer.fit_transform(df_m2_train_data)\n",
    "#train_vec.shape\n",
    "#train_array = train_vec.toarray()\n",
    "#print train_array [1:20,]\n",
    "test_vec = vectorizer.transform(df_m2_test_data)\n",
    "#print test_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the TFIDF leveraged, the vocabulary size shrinks to : 19681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"With the TFIDF leveraged, the vocabulary size shrinks to : \" + str(train_vec.shape[1]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#Model with no preprocessing \n",
    "tfidf_LR_model = LogisticRegression(penalty=\"l2\", C = 100)\n",
    "tfidf_LR_model.fit(train_vec, df_m2_train_label)\n",
    "\n",
    "#makes predictions on the dev dataset with the model I just built\n",
    "tfidf_dev_pred_labels = tfidf_LR_model.predict(test_vec)\n",
    "tfidf_dev_pred_probs = tfidf_LR_model.predict_proba(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "35 min run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.002090443490307989"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.f1_score(df_m2_test_label, tfidf_dev_pred_labels,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformatting the data for BoW model\n",
    "method3_test_data = [] \n",
    "method3_test_label = [] \n",
    "for passage, question, answer in dict_test.values():\n",
    "    method3_test_data.append(' '.join(passage + question))\n",
    "    method3_test_label.append(' '.join(answer))\n",
    "df_m3_test_data = pd.DataFrame(method3_test_data)    \n",
    "df_m3_test_label = pd.DataFrame(method3_test_label)\n",
    "\n",
    "method3_train_data = [] \n",
    "method3_train_label = [] \n",
    "for passage, question, answer in dict_train.values():\n",
    "    method3_train_data.append(' '.join(passage + question))\n",
    "    method3_train_label.append(' '.join(answer))\n",
    "df_m3_train_data = pd.DataFrame(method3_train_data)    \n",
    "df_m3_train_label = pd.DataFrame(method3_train_label)\n",
    "\n",
    "df_m3_train_data.fillna(\"\", inplace=True)  \n",
    "df_m3_test_data.fillna(\"\", inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([778723, 778744,   7878, ..., 467242, 778100, 778785])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([778726, 778656, 769663, ..., 778748, 745129, 776480])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3_test_labels = []\n",
    "for item in answers_test:\n",
    "    m3_test_labels.append(vocab[item-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3048"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m3_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778723\n",
      "778744\n",
      "7878\n",
      "778766\n",
      "778776\n",
      "778761\n",
      "778739\n",
      "763102\n",
      "378\n",
      "627180\n",
      "777968\n",
      "778744\n",
      "7874\n",
      "778744\n",
      "776946\n",
      "706367\n",
      "778740\n",
      "778785\n",
      "778761\n",
      "778652\n",
      "778663\n",
      "778785\n",
      "7878\n",
      "778785\n",
      "778744\n",
      "1104\n",
      "773417\n",
      "1181\n",
      "778785\n",
      "759098\n",
      "778785\n",
      "778806\n",
      "442618\n",
      "778723\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "729307\n",
      "745116\n",
      "778748\n",
      "726270\n",
      "775495\n",
      "7878\n",
      "775495\n",
      "778766\n",
      "7878\n",
      "763525\n",
      "7878\n",
      "778785\n",
      "778652\n",
      "778785\n",
      "778748\n",
      "778766\n",
      "778761\n",
      "778761\n",
      "7878\n",
      "778748\n",
      "763525\n",
      "7874\n",
      "778757\n",
      "778785\n",
      "747576\n",
      "2749\n",
      "778785\n",
      "7878\n",
      "763074\n",
      "778744\n",
      "778774\n",
      "759523\n",
      "745612\n",
      "778744\n",
      "7874\n",
      "759523\n",
      "760737\n",
      "778774\n",
      "778785\n",
      "746570\n",
      "3497\n",
      "778580\n",
      "778811\n",
      "759523\n",
      "778798\n",
      "778785\n",
      "3860\n",
      "3887\n",
      "778785\n",
      "778637\n",
      "778766\n",
      "753073\n",
      "778744\n",
      "778744\n",
      "661539\n",
      "778744\n",
      "778785\n",
      "661539\n",
      "778785\n",
      "778774\n",
      "778751\n",
      "778766\n",
      "776691\n",
      "776751\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "763218\n",
      "5162\n",
      "778702\n",
      "778785\n",
      "647391\n",
      "778805\n",
      "778744\n",
      "759523\n",
      "778785\n",
      "388634\n",
      "5133\n",
      "778785\n",
      "778785\n",
      "7878\n",
      "778785\n",
      "7874\n",
      "778807\n",
      "778807\n",
      "777671\n",
      "778811\n",
      "5534\n",
      "778761\n",
      "778776\n",
      "778740\n",
      "776793\n",
      "7874\n",
      "778761\n",
      "778785\n",
      "778766\n",
      "778723\n",
      "778785\n",
      "769383\n",
      "778748\n",
      "761525\n",
      "778785\n",
      "712988\n",
      "7878\n",
      "778785\n",
      "778744\n",
      "7874\n",
      "770669\n",
      "6439\n",
      "761363\n",
      "778766\n",
      "761525\n",
      "778744\n",
      "778744\n",
      "531454\n",
      "759098\n",
      "759523\n",
      "778744\n",
      "778637\n",
      "778761\n",
      "759523\n",
      "775909\n",
      "778785\n",
      "778785\n",
      "778744\n",
      "217518\n",
      "778785\n",
      "746082\n",
      "763299\n",
      "7510\n",
      "778785\n",
      "778716\n",
      "763299\n",
      "659793\n",
      "763299\n",
      "778785\n",
      "752072\n",
      "751858\n",
      "776118\n",
      "778761\n",
      "778785\n",
      "722364\n",
      "778342\n",
      "778748\n",
      "708141\n",
      "778660\n",
      "778191\n",
      "778703\n",
      "778785\n",
      "778652\n",
      "8300\n",
      "778703\n",
      "778766\n",
      "778273\n",
      "609737\n",
      "609737\n",
      "778807\n",
      "778748\n",
      "776317\n",
      "778680\n",
      "709308\n",
      "763509\n",
      "763218\n",
      "739762\n",
      "763218\n",
      "778766\n",
      "778753\n",
      "778748\n",
      "778660\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778726\n",
      "708141\n",
      "775112\n",
      "739762\n",
      "778716\n",
      "778766\n",
      "717816\n",
      "679179\n",
      "778191\n",
      "778785\n",
      "778709\n",
      "778761\n",
      "778761\n",
      "15941\n",
      "778785\n",
      "15941\n",
      "759664\n",
      "778748\n",
      "776946\n",
      "651997\n",
      "759664\n",
      "778785\n",
      "778273\n",
      "778785\n",
      "759664\n",
      "778716\n",
      "753073\n",
      "326025\n",
      "765182\n",
      "778785\n",
      "778766\n",
      "778766\n",
      "759664\n",
      "778726\n",
      "714290\n",
      "739762\n",
      "15941\n",
      "759664\n",
      "759664\n",
      "778785\n",
      "778191\n",
      "778726\n",
      "778748\n",
      "739762\n",
      "778766\n",
      "714290\n",
      "778748\n",
      "763218\n",
      "778740\n",
      "778766\n",
      "759664\n",
      "778785\n",
      "762479\n",
      "778191\n",
      "762479\n",
      "761525\n",
      "761525\n",
      "778748\n",
      "739762\n",
      "739762\n",
      "12640\n",
      "778744\n",
      "679179\n",
      "763218\n",
      "778709\n",
      "775234\n",
      "778758\n",
      "778766\n",
      "739762\n",
      "778766\n",
      "759664\n",
      "778191\n",
      "759664\n",
      "679179\n",
      "679179\n",
      "15941\n",
      "761047\n",
      "667219\n",
      "778748\n",
      "708141\n",
      "708141\n",
      "778716\n",
      "778740\n",
      "708141\n",
      "778785\n",
      "778412\n",
      "778785\n",
      "778726\n",
      "778777\n",
      "708141\n",
      "14085\n",
      "778243\n",
      "778766\n",
      "763509\n",
      "776946\n",
      "778785\n",
      "778785\n",
      "778703\n",
      "778758\n",
      "778709\n",
      "778785\n",
      "778262\n",
      "778748\n",
      "778751\n",
      "763218\n",
      "762224\n",
      "778774\n",
      "778726\n",
      "778766\n",
      "778766\n",
      "667219\n",
      "778748\n",
      "778761\n",
      "778766\n",
      "778726\n",
      "778703\n",
      "778785\n",
      "223692\n",
      "778748\n",
      "15941\n",
      "778191\n",
      "707508\n",
      "778766\n",
      "778703\n",
      "778785\n",
      "16294\n",
      "778712\n",
      "778766\n",
      "778748\n",
      "762747\n",
      "679179\n",
      "759664\n",
      "679179\n",
      "778766\n",
      "778801\n",
      "223692\n",
      "778766\n",
      "778761\n",
      "778191\n",
      "778752\n",
      "471300\n",
      "739762\n",
      "778744\n",
      "778785\n",
      "778726\n",
      "762747\n",
      "778811\n",
      "778637\n",
      "778785\n",
      "728364\n",
      "16612\n",
      "778703\n",
      "763231\n",
      "22471\n",
      "736655\n",
      "778744\n",
      "242610\n",
      "774452\n",
      "775223\n",
      "16966\n",
      "778773\n",
      "778748\n",
      "769704\n",
      "714300\n",
      "778026\n",
      "778734\n",
      "778785\n",
      "778734\n",
      "563071\n",
      "778785\n",
      "19513\n",
      "778785\n",
      "459063\n",
      "778771\n",
      "778748\n",
      "778785\n",
      "17745\n",
      "778785\n",
      "730286\n",
      "644328\n",
      "778734\n",
      "746525\n",
      "501645\n",
      "757750\n",
      "22160\n",
      "747576\n",
      "778785\n",
      "763375\n",
      "213929\n",
      "778273\n",
      "741091\n",
      "778734\n",
      "777847\n",
      "778748\n",
      "778621\n",
      "711456\n",
      "778294\n",
      "765708\n",
      "18511\n",
      "717239\n",
      "776118\n",
      "748974\n",
      "778785\n",
      "758777\n",
      "778637\n",
      "763173\n",
      "778785\n",
      "716217\n",
      "778753\n",
      "778748\n",
      "778811\n",
      "778748\n",
      "778748\n",
      "764739\n",
      "599628\n",
      "778785\n",
      "738009\n",
      "778748\n",
      "751035\n",
      "778808\n",
      "778734\n",
      "778748\n",
      "778774\n",
      "778758\n",
      "776946\n",
      "763103\n",
      "778703\n",
      "778748\n",
      "778748\n",
      "765731\n",
      "778703\n",
      "22921\n",
      "776946\n",
      "778748\n",
      "19516\n",
      "744313\n",
      "777925\n",
      "704223\n",
      "704555\n",
      "778592\n",
      "717837\n",
      "748974\n",
      "778810\n",
      "778734\n",
      "235149\n",
      "778592\n",
      "778761\n",
      "740908\n",
      "20065\n",
      "738009\n",
      "778748\n",
      "778748\n",
      "778774\n",
      "778748\n",
      "778806\n",
      "778774\n",
      "777639\n",
      "714300\n",
      "778809\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "743876\n",
      "778740\n",
      "739223\n",
      "778273\n",
      "776946\n",
      "714300\n",
      "768775\n",
      "777555\n",
      "771537\n",
      "22921\n",
      "778504\n",
      "778730\n",
      "346269\n",
      "456583\n",
      "778329\n",
      "778761\n",
      "778710\n",
      "778243\n",
      "778734\n",
      "738009\n",
      "778703\n",
      "738009\n",
      "778748\n",
      "472153\n",
      "778810\n",
      "778726\n",
      "717239\n",
      "778748\n",
      "742411\n",
      "778748\n",
      "778785\n",
      "762952\n",
      "746589\n",
      "778785\n",
      "778273\n",
      "762434\n",
      "235149\n",
      "777194\n",
      "778776\n",
      "778785\n",
      "22218\n",
      "738009\n",
      "771341\n",
      "741088\n",
      "778703\n",
      "527855\n",
      "778726\n",
      "22408\n",
      "778734\n",
      "22471\n",
      "778810\n",
      "778726\n",
      "750367\n",
      "714300\n",
      "775871\n",
      "778726\n",
      "740805\n",
      "775201\n",
      "22938\n",
      "22803\n",
      "778734\n",
      "22853\n",
      "776656\n",
      "778776\n",
      "22927\n",
      "777374\n",
      "22969\n",
      "22982\n",
      "744935\n",
      "778703\n",
      "778761\n",
      "778785\n",
      "32364\n",
      "778761\n",
      "778785\n",
      "778774\n",
      "773177\n",
      "778761\n",
      "778726\n",
      "772206\n",
      "778785\n",
      "778025\n",
      "778660\n",
      "778716\n",
      "733650\n",
      "282903\n",
      "603557\n",
      "778806\n",
      "778734\n",
      "778761\n",
      "775583\n",
      "778771\n",
      "775583\n",
      "778761\n",
      "590543\n",
      "778726\n",
      "778663\n",
      "778808\n",
      "778734\n",
      "778766\n",
      "778785\n",
      "762353\n",
      "778810\n",
      "643480\n",
      "778726\n",
      "776339\n",
      "778806\n",
      "778774\n",
      "778663\n",
      "778726\n",
      "778726\n",
      "778761\n",
      "778785\n",
      "758463\n",
      "778734\n",
      "753862\n",
      "778785\n",
      "534921\n",
      "778663\n",
      "778726\n",
      "777374\n",
      "778761\n",
      "778204\n",
      "778750\n",
      "775583\n",
      "778810\n",
      "762291\n",
      "778774\n",
      "777767\n",
      "675297\n",
      "757337\n",
      "32364\n",
      "761796\n",
      "776946\n",
      "778761\n",
      "778806\n",
      "778785\n",
      "778766\n",
      "448474\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "763509\n",
      "778663\n",
      "778744\n",
      "778178\n",
      "778740\n",
      "601865\n",
      "778806\n",
      "778761\n",
      "778761\n",
      "778785\n",
      "778774\n",
      "603557\n",
      "729297\n",
      "743262\n",
      "778771\n",
      "778766\n",
      "778785\n",
      "778809\n",
      "778703\n",
      "778785\n",
      "778766\n",
      "778761\n",
      "778785\n",
      "778601\n",
      "778726\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778740\n",
      "28756\n",
      "778810\n",
      "778777\n",
      "778761\n",
      "778776\n",
      "778785\n",
      "778763\n",
      "747353\n",
      "735426\n",
      "778702\n",
      "778748\n",
      "651453\n",
      "31670\n",
      "778785\n",
      "778710\n",
      "778763\n",
      "778204\n",
      "778204\n",
      "778785\n",
      "778702\n",
      "778580\n",
      "776946\n",
      "778763\n",
      "778740\n",
      "750222\n",
      "471343\n",
      "756462\n",
      "778702\n",
      "778726\n",
      "778703\n",
      "778774\n",
      "778805\n",
      "778703\n",
      "763509\n",
      "778785\n",
      "778774\n",
      "778726\n",
      "778771\n",
      "778774\n",
      "778761\n",
      "778806\n",
      "778776\n",
      "778766\n",
      "778785\n",
      "778761\n",
      "778663\n",
      "603557\n",
      "603557\n",
      "778748\n",
      "778785\n",
      "774625\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778761\n",
      "729297\n",
      "778766\n",
      "755428\n",
      "763509\n",
      "778748\n",
      "31915\n",
      "778806\n",
      "778774\n",
      "776946\n",
      "778785\n",
      "778726\n",
      "778761\n",
      "778776\n",
      "778766\n",
      "747353\n",
      "720432\n",
      "778748\n",
      "778761\n",
      "778761\n",
      "769919\n",
      "778810\n",
      "762747\n",
      "778785\n",
      "778712\n",
      "778807\n",
      "38869\n",
      "778748\n",
      "778785\n",
      "39267\n",
      "775583\n",
      "778703\n",
      "778766\n",
      "38949\n",
      "778785\n",
      "778785\n",
      "778774\n",
      "771042\n",
      "778785\n",
      "778748\n",
      "769089\n",
      "778766\n",
      "39352\n",
      "778748\n",
      "778766\n",
      "39352\n",
      "778748\n",
      "778237\n",
      "715847\n",
      "778766\n",
      "778785\n",
      "778734\n",
      "778748\n",
      "38869\n",
      "723186\n",
      "778785\n",
      "761372\n",
      "778761\n",
      "778726\n",
      "778776\n",
      "39208\n",
      "778785\n",
      "778726\n",
      "38512\n",
      "37537\n",
      "38949\n",
      "763446\n",
      "778761\n",
      "778734\n",
      "778703\n",
      "715036\n",
      "778740\n",
      "778716\n",
      "778652\n",
      "778785\n",
      "778748\n",
      "778761\n",
      "778734\n",
      "778748\n",
      "778748\n",
      "763074\n",
      "778761\n",
      "778734\n",
      "35254\n",
      "704551\n",
      "344506\n",
      "778726\n",
      "778748\n",
      "39208\n",
      "778748\n",
      "778785\n",
      "649560\n",
      "778761\n",
      "778766\n",
      "778595\n",
      "763102\n",
      "778761\n",
      "751524\n",
      "778703\n",
      "753548\n",
      "778703\n",
      "763298\n",
      "776564\n",
      "344506\n",
      "649560\n",
      "778776\n",
      "778748\n",
      "777048\n",
      "778798\n",
      "778748\n",
      "775382\n",
      "312539\n",
      "778703\n",
      "39267\n",
      "37649\n",
      "778751\n",
      "777374\n",
      "778766\n",
      "778785\n",
      "778748\n",
      "778740\n",
      "778663\n",
      "778766\n",
      "778761\n",
      "778776\n",
      "762952\n",
      "37649\n",
      "778726\n",
      "763452\n",
      "754045\n",
      "39352\n",
      "775773\n",
      "778703\n",
      "38949\n",
      "778785\n",
      "778726\n",
      "778663\n",
      "38512\n",
      "778761\n",
      "719186\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778273\n",
      "778748\n",
      "776564\n",
      "778734\n",
      "37537\n",
      "778748\n",
      "778785\n",
      "39208\n",
      "778748\n",
      "778026\n",
      "778726\n",
      "37649\n",
      "778766\n",
      "778660\n",
      "778663\n",
      "501621\n",
      "778726\n",
      "778748\n",
      "778785\n",
      "778726\n",
      "778748\n",
      "39209\n",
      "778761\n",
      "778740\n",
      "778774\n",
      "541223\n",
      "778504\n",
      "778748\n",
      "778740\n",
      "778744\n",
      "762118\n",
      "38949\n",
      "38869\n",
      "746626\n",
      "38869\n",
      "777374\n",
      "778748\n",
      "778771\n",
      "655445\n",
      "778734\n",
      "763375\n",
      "778734\n",
      "778761\n",
      "778703\n",
      "38949\n",
      "778785\n",
      "39209\n",
      "39050\n",
      "778785\n",
      "778748\n",
      "775495\n",
      "778652\n",
      "778761\n",
      "733370\n",
      "778761\n",
      "751858\n",
      "770095\n",
      "775899\n",
      "739479\n",
      "778002\n",
      "778785\n",
      "778748\n",
      "778771\n",
      "697979\n",
      "778748\n",
      "776021\n",
      "778734\n",
      "778740\n",
      "778785\n",
      "778761\n",
      "778774\n",
      "778774\n",
      "778774\n",
      "715258\n",
      "778761\n",
      "778748\n",
      "739762\n",
      "778798\n",
      "778774\n",
      "778766\n",
      "778766\n",
      "778807\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "754348\n",
      "778810\n",
      "778734\n",
      "778810\n",
      "778810\n",
      "778785\n",
      "778744\n",
      "778744\n",
      "778766\n",
      "778785\n",
      "778774\n",
      "740075\n",
      "778785\n",
      "778785\n",
      "778712\n",
      "778774\n",
      "739762\n",
      "778798\n",
      "778785\n",
      "778766\n",
      "778761\n",
      "778722\n",
      "510884\n",
      "778748\n",
      "330746\n",
      "697979\n",
      "778785\n",
      "760340\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778771\n",
      "778776\n",
      "778785\n",
      "778740\n",
      "778783\n",
      "778766\n",
      "778766\n",
      "778622\n",
      "778774\n",
      "778748\n",
      "778761\n",
      "777220\n",
      "771765\n",
      "778785\n",
      "778712\n",
      "778766\n",
      "778766\n",
      "778785\n",
      "778243\n",
      "778785\n",
      "778748\n",
      "778292\n",
      "778771\n",
      "762963\n",
      "778766\n",
      "758847\n",
      "778652\n",
      "778771\n",
      "778785\n",
      "778785\n",
      "44032\n",
      "759941\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778751\n",
      "778774\n",
      "776564\n",
      "677185\n",
      "778766\n",
      "778744\n",
      "778785\n",
      "778774\n",
      "778771\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "778798\n",
      "778730\n",
      "778774\n",
      "778761\n",
      "778774\n",
      "778771\n",
      "778766\n",
      "760588\n",
      "83418\n",
      "778761\n",
      "778748\n",
      "778785\n",
      "778766\n",
      "778740\n",
      "778740\n",
      "777965\n",
      "777978\n",
      "755461\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "778761\n",
      "778663\n",
      "778785\n",
      "778771\n",
      "778712\n",
      "778785\n",
      "778722\n",
      "778748\n",
      "776270\n",
      "778810\n",
      "778785\n",
      "778766\n",
      "762245\n",
      "778774\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778811\n",
      "46842\n",
      "778785\n",
      "778744\n",
      "778748\n",
      "778663\n",
      "778771\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778774\n",
      "778771\n",
      "778766\n",
      "778766\n",
      "778766\n",
      "778785\n",
      "778744\n",
      "778785\n",
      "778771\n",
      "778771\n",
      "778771\n",
      "778734\n",
      "778656\n",
      "778766\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778766\n",
      "778761\n",
      "778753\n",
      "778811\n",
      "510884\n",
      "778773\n",
      "778771\n",
      "778748\n",
      "778761\n",
      "778726\n",
      "778748\n",
      "778774\n",
      "778774\n",
      "778774\n",
      "778806\n",
      "778774\n",
      "778785\n",
      "778785\n",
      "778776\n",
      "778785\n",
      "778761\n",
      "778748\n",
      "778660\n",
      "778402\n",
      "765182\n",
      "777374\n",
      "719582\n",
      "778766\n",
      "773130\n",
      "778785\n",
      "778761\n",
      "778734\n",
      "778806\n",
      "778761\n",
      "776564\n",
      "778740\n",
      "778748\n",
      "778783\n",
      "778740\n",
      "778761\n",
      "778703\n",
      "778748\n",
      "778785\n",
      "50217\n",
      "778785\n",
      "778723\n",
      "778774\n",
      "778806\n",
      "778703\n",
      "778748\n",
      "778761\n",
      "778798\n",
      "778748\n",
      "778774\n",
      "778723\n",
      "778785\n",
      "778785\n",
      "113931\n",
      "778726\n",
      "778766\n",
      "778785\n",
      "778748\n",
      "778766\n",
      "50710\n",
      "778734\n",
      "778723\n",
      "736850\n",
      "778761\n",
      "758232\n",
      "778748\n",
      "778726\n",
      "778785\n",
      "778748\n",
      "778805\n",
      "778273\n",
      "778807\n",
      "778726\n",
      "758847\n",
      "778703\n",
      "739997\n",
      "778805\n",
      "778785\n",
      "778774\n",
      "778785\n",
      "778734\n",
      "778785\n",
      "778761\n",
      "778716\n",
      "704104\n",
      "778370\n",
      "765182\n",
      "778761\n",
      "778734\n",
      "778766\n",
      "778785\n",
      "778748\n",
      "778726\n",
      "778751\n",
      "778776\n",
      "778785\n",
      "725014\n",
      "778748\n",
      "669722\n",
      "778785\n",
      "778734\n",
      "777886\n",
      "778766\n",
      "778785\n",
      "778702\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778251\n",
      "778761\n",
      "778806\n",
      "778810\n",
      "778785\n",
      "778703\n",
      "778748\n",
      "778761\n",
      "778761\n",
      "778740\n",
      "778734\n",
      "778748\n",
      "778744\n",
      "778744\n",
      "778744\n",
      "778702\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778703\n",
      "776946\n",
      "778748\n",
      "741088\n",
      "778637\n",
      "778748\n",
      "55257\n",
      "778703\n",
      "778785\n",
      "778748\n",
      "778761\n",
      "778776\n",
      "743048\n",
      "778785\n",
      "778757\n",
      "716255\n",
      "778785\n",
      "778785\n",
      "778204\n",
      "54786\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778703\n",
      "778761\n",
      "767871\n",
      "777374\n",
      "778766\n",
      "778785\n",
      "55263\n",
      "778324\n",
      "639155\n",
      "55359\n",
      "778809\n",
      "778773\n",
      "778748\n",
      "778740\n",
      "778751\n",
      "778748\n",
      "778785\n",
      "778811\n",
      "778210\n",
      "778785\n",
      "778734\n",
      "778637\n",
      "778758\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "56135\n",
      "56154\n",
      "775495\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "775495\n",
      "777968\n",
      "778748\n",
      "773929\n",
      "432923\n",
      "346913\n",
      "778785\n",
      "777835\n",
      "773307\n",
      "778785\n",
      "742868\n",
      "778785\n",
      "778785\n",
      "721276\n",
      "765182\n",
      "776946\n",
      "778785\n",
      "609765\n",
      "57083\n",
      "778785\n",
      "62242\n",
      "778785\n",
      "763102\n",
      "747322\n",
      "770655\n",
      "778785\n",
      "778748\n",
      "777374\n",
      "778716\n",
      "776946\n",
      "778785\n",
      "778776\n",
      "778761\n",
      "346913\n",
      "771109\n",
      "776683\n",
      "346913\n",
      "762084\n",
      "778748\n",
      "702916\n",
      "778761\n",
      "772698\n",
      "777511\n",
      "778766\n",
      "775038\n",
      "775038\n",
      "778785\n",
      "760782\n",
      "288003\n",
      "63177\n",
      "778740\n",
      "560532\n",
      "761080\n",
      "762806\n",
      "778592\n",
      "765182\n",
      "778748\n",
      "778748\n",
      "58670\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "765182\n",
      "63079\n",
      "778766\n",
      "756188\n",
      "778637\n",
      "59030\n",
      "778785\n",
      "680317\n",
      "63079\n",
      "346913\n",
      "778796\n",
      "778801\n",
      "751030\n",
      "778785\n",
      "59524\n",
      "778748\n",
      "750602\n",
      "778748\n",
      "778785\n",
      "763102\n",
      "778723\n",
      "716744\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "760673\n",
      "778734\n",
      "778638\n",
      "778702\n",
      "778734\n",
      "778748\n",
      "778748\n",
      "778652\n",
      "778703\n",
      "778660\n",
      "778748\n",
      "778748\n",
      "778748\n",
      "715120\n",
      "778748\n",
      "745064\n",
      "60653\n",
      "753532\n",
      "63177\n",
      "720782\n",
      "747353\n",
      "778637\n",
      "778703\n",
      "777374\n",
      "561004\n",
      "776513\n",
      "778637\n",
      "778748\n",
      "61084\n",
      "778744\n",
      "778723\n",
      "389228\n",
      "778723\n",
      "778785\n",
      "778785\n",
      "778751\n",
      "776564\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "778703\n",
      "778748\n",
      "778748\n",
      "763102\n",
      "778785\n",
      "778748\n",
      "778723\n",
      "755461\n",
      "778744\n",
      "685973\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "760685\n",
      "778785\n",
      "778723\n",
      "778748\n",
      "778785\n",
      "778734\n",
      "777968\n",
      "720708\n",
      "726012\n",
      "753532\n",
      "62281\n",
      "778620\n",
      "775583\n",
      "778748\n",
      "775495\n",
      "778716\n",
      "719793\n",
      "716393\n",
      "63177\n",
      "763102\n",
      "778785\n",
      "63079\n",
      "62731\n",
      "778740\n",
      "639152\n",
      "778785\n",
      "778204\n",
      "778751\n",
      "778734\n",
      "63079\n",
      "778785\n",
      "63079\n",
      "778748\n",
      "775495\n",
      "778785\n",
      "760645\n",
      "778785\n",
      "63294\n",
      "778703\n",
      "778752\n",
      "778748\n",
      "778768\n",
      "778798\n",
      "778637\n",
      "778744\n",
      "778294\n",
      "713249\n",
      "70578\n",
      "778770\n",
      "778761\n",
      "778766\n",
      "64022\n",
      "778748\n",
      "778766\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778660\n",
      "644138\n",
      "778748\n",
      "720262\n",
      "755061\n",
      "778785\n",
      "778785\n",
      "778807\n",
      "778806\n",
      "778773\n",
      "778785\n",
      "771765\n",
      "778766\n",
      "778774\n",
      "778785\n",
      "778783\n",
      "778740\n",
      "778776\n",
      "762806\n",
      "778806\n",
      "778771\n",
      "778726\n",
      "778744\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "777374\n",
      "630572\n",
      "778785\n",
      "778809\n",
      "778748\n",
      "778806\n",
      "778811\n",
      "778734\n",
      "778748\n",
      "778703\n",
      "778807\n",
      "778809\n",
      "778761\n",
      "778771\n",
      "760685\n",
      "778806\n",
      "778726\n",
      "778806\n",
      "778761\n",
      "778809\n",
      "778766\n",
      "778766\n",
      "778766\n",
      "778806\n",
      "778785\n",
      "778130\n",
      "778748\n",
      "778776\n",
      "760737\n",
      "778785\n",
      "720157\n",
      "778734\n",
      "778774\n",
      "778785\n",
      "778798\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778740\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778637\n",
      "773255\n",
      "778740\n",
      "778785\n",
      "778771\n",
      "778748\n",
      "778726\n",
      "778785\n",
      "778766\n",
      "778766\n",
      "778766\n",
      "778805\n",
      "778785\n",
      "778740\n",
      "778748\n",
      "778764\n",
      "778748\n",
      "778785\n",
      "746912\n",
      "778806\n",
      "778785\n",
      "762806\n",
      "778748\n",
      "778748\n",
      "69329\n",
      "778774\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "778771\n",
      "69573\n",
      "778748\n",
      "778740\n",
      "778726\n",
      "778740\n",
      "763509\n",
      "778294\n",
      "778015\n",
      "778785\n",
      "778726\n",
      "69973\n",
      "778807\n",
      "70173\n",
      "778726\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778751\n",
      "778785\n",
      "778783\n",
      "70578\n",
      "778766\n",
      "778785\n",
      "778766\n",
      "778805\n",
      "756188\n",
      "778726\n",
      "762806\n",
      "778726\n",
      "778748\n",
      "778811\n",
      "778766\n",
      "778748\n",
      "778806\n",
      "717239\n",
      "778418\n",
      "778726\n",
      "778744\n",
      "762411\n",
      "778785\n",
      "778740\n",
      "778785\n",
      "778726\n",
      "778740\n",
      "778744\n",
      "778766\n",
      "778766\n",
      "778785\n",
      "338385\n",
      "778785\n",
      "777620\n",
      "778748\n",
      "778748\n",
      "778734\n",
      "778748\n",
      "778748\n",
      "778748\n",
      "778766\n",
      "778785\n",
      "479451\n",
      "762479\n",
      "542114\n",
      "763497\n",
      "667219\n",
      "718218\n",
      "762806\n",
      "762806\n",
      "778726\n",
      "778748\n",
      "778761\n",
      "763509\n",
      "778663\n",
      "719737\n",
      "776063\n",
      "75019\n",
      "762806\n",
      "762479\n",
      "778761\n",
      "763505\n",
      "778766\n",
      "778761\n",
      "778642\n",
      "762479\n",
      "75019\n",
      "667219\n",
      "778592\n",
      "75019\n",
      "778785\n",
      "763848\n",
      "762479\n",
      "771341\n",
      "762479\n",
      "395018\n",
      "771259\n",
      "773549\n",
      "778325\n",
      "778785\n",
      "73223\n",
      "778785\n",
      "777374\n",
      "776946\n",
      "778807\n",
      "740805\n",
      "778785\n",
      "762806\n",
      "778810\n",
      "778785\n",
      "763509\n",
      "776195\n",
      "776024\n",
      "778798\n",
      "763509\n",
      "73707\n",
      "778785\n",
      "762479\n",
      "652863\n",
      "778618\n",
      "762411\n",
      "763173\n",
      "778785\n",
      "73931\n",
      "778785\n",
      "75019\n",
      "778785\n",
      "647228\n",
      "648562\n",
      "762479\n",
      "777571\n",
      "763509\n",
      "778777\n",
      "762806\n",
      "778766\n",
      "767966\n",
      "740805\n",
      "778748\n",
      "778785\n",
      "777925\n",
      "778785\n",
      "778785\n",
      "762806\n",
      "762806\n",
      "778730\n",
      "778748\n",
      "74670\n",
      "763509\n",
      "778774\n",
      "778785\n",
      "778761\n",
      "778801\n",
      "76833\n",
      "778726\n",
      "75019\n",
      "778748\n",
      "774144\n",
      "775095\n",
      "778761\n",
      "778785\n",
      "778210\n",
      "778785\n",
      "763497\n",
      "778748\n",
      "708634\n",
      "778748\n",
      "778748\n",
      "778748\n",
      "770479\n",
      "778748\n",
      "778748\n",
      "778748\n",
      "720157\n",
      "778748\n",
      "778766\n",
      "762806\n",
      "75799\n",
      "778243\n",
      "778766\n",
      "762224\n",
      "778806\n",
      "778748\n",
      "708634\n",
      "778766\n",
      "755989\n",
      "778748\n",
      "778785\n",
      "778682\n",
      "778761\n",
      "778785\n",
      "762479\n",
      "778761\n",
      "778776\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "777575\n",
      "778785\n",
      "763497\n",
      "778601\n",
      "667219\n",
      "778766\n",
      "778766\n",
      "778748\n",
      "777171\n",
      "732782\n",
      "778660\n",
      "778773\n",
      "778785\n",
      "647228\n",
      "778677\n",
      "639149\n",
      "774737\n",
      "778748\n",
      "762479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778785\n",
      "776805\n",
      "778785\n",
      "778763\n",
      "778748\n",
      "778748\n",
      "778740\n",
      "769082\n",
      "762806\n",
      "370322\n",
      "778785\n",
      "778342\n",
      "778748\n",
      "762479\n",
      "778805\n",
      "778748\n",
      "778774\n",
      "778774\n",
      "778726\n",
      "778785\n",
      "776838\n",
      "775095\n",
      "778785\n",
      "762806\n",
      "778412\n",
      "542114\n",
      "718218\n",
      "78081\n",
      "78129\n",
      "778753\n",
      "765834\n",
      "778785\n",
      "778761\n",
      "778807\n",
      "669993\n",
      "778748\n",
      "778748\n",
      "778766\n",
      "778748\n",
      "777970\n",
      "778785\n",
      "676677\n",
      "745898\n",
      "751180\n",
      "732168\n",
      "778796\n",
      "778785\n",
      "778748\n",
      "639131\n",
      "752214\n",
      "778785\n",
      "726714\n",
      "706582\n",
      "763240\n",
      "771830\n",
      "713635\n",
      "346880\n",
      "82452\n",
      "751623\n",
      "732710\n",
      "752072\n",
      "778785\n",
      "79735\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "79948\n",
      "778734\n",
      "728177\n",
      "778785\n",
      "778752\n",
      "776145\n",
      "778609\n",
      "778776\n",
      "778785\n",
      "753566\n",
      "777978\n",
      "747322\n",
      "80686\n",
      "684714\n",
      "751623\n",
      "778703\n",
      "762307\n",
      "778785\n",
      "778748\n",
      "778801\n",
      "778734\n",
      "730426\n",
      "778702\n",
      "346880\n",
      "753087\n",
      "81364\n",
      "778748\n",
      "81446\n",
      "778774\n",
      "772925\n",
      "763261\n",
      "697118\n",
      "755461\n",
      "778748\n",
      "778247\n",
      "81805\n",
      "778761\n",
      "81875\n",
      "778785\n",
      "778785\n",
      "82452\n",
      "721005\n",
      "775871\n",
      "82143\n",
      "743048\n",
      "778726\n",
      "778748\n",
      "778748\n",
      "778637\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "82606\n",
      "728446\n",
      "778785\n",
      "778785\n",
      "82793\n",
      "548757\n",
      "778748\n",
      "718100\n",
      "87218\n",
      "83084\n",
      "83493\n",
      "778785\n",
      "87218\n",
      "773076\n",
      "633246\n",
      "778748\n",
      "778592\n",
      "778775\n",
      "723426\n",
      "778748\n",
      "86443\n",
      "83996\n",
      "759901\n",
      "585194\n",
      "772783\n",
      "778748\n",
      "753566\n",
      "778785\n",
      "761004\n",
      "778748\n",
      "778785\n",
      "84703\n",
      "84782\n",
      "778785\n",
      "743876\n",
      "778785\n",
      "84967\n",
      "778174\n",
      "778766\n",
      "778766\n",
      "649505\n",
      "86082\n",
      "85236\n",
      "778796\n",
      "87218\n",
      "85374\n",
      "778748\n",
      "778776\n",
      "751180\n",
      "778637\n",
      "86892\n",
      "778776\n",
      "778785\n",
      "778702\n",
      "778766\n",
      "778785\n",
      "743234\n",
      "85964\n",
      "776546\n",
      "86082\n",
      "778703\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "762094\n",
      "776946\n",
      "86405\n",
      "778785\n",
      "86541\n",
      "751180\n",
      "756393\n",
      "86702\n",
      "86749\n",
      "778652\n",
      "86841\n",
      "86892\n",
      "768046\n",
      "87424\n",
      "747914\n",
      "87106\n",
      "758681\n",
      "87218\n",
      "751623\n",
      "778702\n",
      "778734\n",
      "778785\n",
      "723789\n",
      "718100\n",
      "87509\n",
      "93191\n",
      "778766\n",
      "778761\n",
      "778766\n",
      "778748\n",
      "778774\n",
      "778766\n",
      "778766\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778806\n",
      "778652\n",
      "778748\n",
      "778785\n",
      "778783\n",
      "778785\n",
      "778621\n",
      "778750\n",
      "778703\n",
      "778785\n",
      "630101\n",
      "93579\n",
      "778785\n",
      "778809\n",
      "778785\n",
      "778774\n",
      "778761\n",
      "775583\n",
      "778806\n",
      "778785\n",
      "778748\n",
      "369459\n",
      "778785\n",
      "93191\n",
      "778761\n",
      "778806\n",
      "775095\n",
      "772628\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778774\n",
      "778774\n",
      "778785\n",
      "778785\n",
      "778806\n",
      "93208\n",
      "778680\n",
      "778726\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "93053\n",
      "778805\n",
      "778785\n",
      "778752\n",
      "778785\n",
      "778783\n",
      "778734\n",
      "777374\n",
      "696320\n",
      "778748\n",
      "778785\n",
      "778734\n",
      "778806\n",
      "778807\n",
      "93208\n",
      "778766\n",
      "778806\n",
      "778805\n",
      "778785\n",
      "778773\n",
      "778734\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778766\n",
      "778761\n",
      "778785\n",
      "778776\n",
      "778761\n",
      "778761\n",
      "778774\n",
      "778785\n",
      "778774\n",
      "778806\n",
      "778710\n",
      "778766\n",
      "778785\n",
      "778703\n",
      "778726\n",
      "778758\n",
      "778785\n",
      "778774\n",
      "778810\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "778776\n",
      "778761\n",
      "778806\n",
      "778805\n",
      "778811\n",
      "778761\n",
      "407817\n",
      "778766\n",
      "778785\n",
      "778605\n",
      "778716\n",
      "778774\n",
      "778773\n",
      "778740\n",
      "778663\n",
      "778785\n",
      "778773\n",
      "778785\n",
      "778761\n",
      "778734\n",
      "778703\n",
      "778342\n",
      "778805\n",
      "778734\n",
      "778740\n",
      "778771\n",
      "773130\n",
      "778766\n",
      "773130\n",
      "778785\n",
      "777374\n",
      "778710\n",
      "778785\n",
      "778761\n",
      "773130\n",
      "778761\n",
      "778785\n",
      "778766\n",
      "778766\n",
      "778216\n",
      "778785\n",
      "778712\n",
      "778811\n",
      "778761\n",
      "778775\n",
      "778761\n",
      "778504\n",
      "778744\n",
      "778761\n",
      "778712\n",
      "778798\n",
      "630101\n",
      "778744\n",
      "566696\n",
      "778771\n",
      "778761\n",
      "778691\n",
      "778758\n",
      "778734\n",
      "778726\n",
      "778766\n",
      "218167\n",
      "778810\n",
      "93191\n",
      "778766\n",
      "776152\n",
      "778709\n",
      "778785\n",
      "778761\n",
      "778809\n",
      "778805\n",
      "566696\n",
      "765834\n",
      "778798\n",
      "778776\n",
      "778716\n",
      "93629\n",
      "715850\n",
      "778748\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778774\n",
      "97774\n",
      "102640\n",
      "778785\n",
      "103504\n",
      "608664\n",
      "749952\n",
      "759901\n",
      "778774\n",
      "753073\n",
      "758232\n",
      "175886\n",
      "778748\n",
      "94809\n",
      "777049\n",
      "778748\n",
      "339918\n",
      "774461\n",
      "382790\n",
      "778744\n",
      "101773\n",
      "754076\n",
      "666947\n",
      "102402\n",
      "778740\n",
      "740075\n",
      "778748\n",
      "95908\n",
      "775495\n",
      "102640\n",
      "778748\n",
      "778723\n",
      "778748\n",
      "778744\n",
      "778748\n",
      "778785\n",
      "97774\n",
      "778748\n",
      "778748\n",
      "96143\n",
      "740824\n",
      "726613\n",
      "778777\n",
      "776126\n",
      "199836\n",
      "776126\n",
      "778785\n",
      "777969\n",
      "778744\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "776946\n",
      "96653\n",
      "753073\n",
      "101886\n",
      "778785\n",
      "102640\n",
      "778748\n",
      "778776\n",
      "720708\n",
      "778726\n",
      "449064\n",
      "100531\n",
      "778663\n",
      "778761\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "97774\n",
      "778748\n",
      "778748\n",
      "776952\n",
      "778712\n",
      "778785\n",
      "98150\n",
      "98181\n",
      "102017\n",
      "778785\n",
      "102240\n",
      "778748\n",
      "778652\n",
      "778723\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "763074\n",
      "98751\n",
      "778748\n",
      "778806\n",
      "778761\n",
      "384944\n",
      "778776\n",
      "778785\n",
      "778751\n",
      "769609\n",
      "778785\n",
      "778806\n",
      "102017\n",
      "99391\n",
      "778744\n",
      "102240\n",
      "102640\n",
      "778734\n",
      "101886\n",
      "751537\n",
      "778785\n",
      "778785\n",
      "747720\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "759664\n",
      "778748\n",
      "103146\n",
      "778110\n",
      "778770\n",
      "778723\n",
      "100319\n",
      "778744\n",
      "778637\n",
      "778726\n",
      "720708\n",
      "100531\n",
      "719084\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "763102\n",
      "778785\n",
      "696530\n",
      "103228\n",
      "102640\n",
      "778751\n",
      "762963\n",
      "759664\n",
      "776126\n",
      "778734\n",
      "720708\n",
      "745064\n",
      "101559\n",
      "747914\n",
      "778785\n",
      "778734\n",
      "757226\n",
      "763218\n",
      "778785\n",
      "778785\n",
      "102114\n",
      "102150\n",
      "102264\n",
      "778734\n",
      "778785\n",
      "778770\n",
      "778744\n",
      "103504\n",
      "778748\n",
      "778774\n",
      "102765\n",
      "759664\n",
      "103504\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "760639\n",
      "103230\n",
      "778785\n",
      "776118\n",
      "778785\n",
      "778785\n",
      "761525\n",
      "671940\n",
      "111703\n",
      "778785\n",
      "778785\n",
      "103826\n",
      "778785\n",
      "103948\n",
      "778734\n",
      "778716\n",
      "778785\n",
      "778813\n",
      "776946\n",
      "778592\n",
      "106328\n",
      "105917\n",
      "778785\n",
      "778766\n",
      "642764\n",
      "778761\n",
      "778798\n",
      "104860\n",
      "114489\n",
      "778785\n",
      "104989\n",
      "105269\n",
      "105269\n",
      "675935\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778809\n",
      "778681\n",
      "778766\n",
      "105643\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778809\n",
      "105917\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "642764\n",
      "642764\n",
      "114489\n",
      "778766\n",
      "778763\n",
      "778785\n",
      "106502\n",
      "762952\n",
      "778766\n",
      "778785\n",
      "778247\n",
      "778761\n",
      "106813\n",
      "778726\n",
      "670771\n",
      "778774\n",
      "778798\n",
      "778748\n",
      "778748\n",
      "107217\n",
      "778806\n",
      "669977\n",
      "778734\n",
      "778813\n",
      "778766\n",
      "778785\n",
      "778772\n",
      "640420\n",
      "778765\n",
      "778766\n",
      "114488\n",
      "778773\n",
      "778785\n",
      "778680\n",
      "778726\n",
      "778785\n",
      "778771\n",
      "778766\n",
      "677367\n",
      "778809\n",
      "778766\n",
      "108742\n",
      "778806\n",
      "778785\n",
      "778751\n",
      "655799\n",
      "778785\n",
      "778785\n",
      "778703\n",
      "778726\n",
      "765281\n",
      "778773\n",
      "778761\n",
      "778663\n",
      "778766\n",
      "109625\n",
      "778785\n",
      "778726\n",
      "667616\n",
      "765265\n",
      "109895\n",
      "109973\n",
      "778809\n",
      "734129\n",
      "778753\n",
      "778748\n",
      "776795\n",
      "114516\n",
      "765265\n",
      "765265\n",
      "763173\n",
      "777886\n",
      "778766\n",
      "778766\n",
      "676552\n",
      "778726\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "778412\n",
      "775495\n",
      "778748\n",
      "778785\n",
      "111439\n",
      "777968\n",
      "773275\n",
      "773112\n",
      "778785\n",
      "675935\n",
      "778726\n",
      "778785\n",
      "778748\n",
      "778811\n",
      "112052\n",
      "778761\n",
      "112126\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "778748\n",
      "112550\n",
      "112587\n",
      "778637\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "114090\n",
      "112918\n",
      "675935\n",
      "677205\n",
      "778785\n",
      "778813\n",
      "778766\n",
      "113252\n",
      "765265\n",
      "778722\n",
      "763375\n",
      "113479\n",
      "778723\n",
      "778766\n",
      "777620\n",
      "479868\n",
      "770418\n",
      "778393\n",
      "778726\n",
      "778785\n",
      "479868\n",
      "114261\n",
      "778766\n",
      "778412\n",
      "778744\n",
      "565421\n",
      "765265\n",
      "114488\n",
      "778773\n",
      "676439\n",
      "778785\n",
      "778744\n",
      "778774\n",
      "778809\n",
      "778734\n",
      "778748\n",
      "778785\n",
      "778247\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "778652\n",
      "778785\n",
      "124811\n",
      "778806\n",
      "770736\n",
      "308230\n",
      "122575\n",
      "778751\n",
      "778785\n",
      "778785\n",
      "359420\n",
      "778611\n",
      "778637\n",
      "778785\n",
      "778766\n",
      "778801\n",
      "776838\n",
      "207871\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "775382\n",
      "754528\n",
      "656209\n",
      "778748\n",
      "778748\n",
      "778748\n",
      "778703\n",
      "778766\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "777903\n",
      "778785\n",
      "735618\n",
      "778774\n",
      "778785\n",
      "778748\n",
      "778726\n",
      "778761\n",
      "778766\n",
      "778681\n",
      "778273\n",
      "778726\n",
      "741603\n",
      "778766\n",
      "753864\n",
      "778785\n",
      "778748\n",
      "663187\n",
      "244980\n",
      "778761\n",
      "777886\n",
      "118686\n",
      "778766\n",
      "778748\n",
      "778805\n",
      "778785\n",
      "778273\n",
      "766538\n",
      "778744\n",
      "778785\n",
      "778776\n",
      "741583\n",
      "737659\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "120086\n",
      "342051\n",
      "774134\n",
      "778426\n",
      "362562\n",
      "778805\n",
      "727726\n",
      "778504\n",
      "778761\n",
      "778766\n",
      "778785\n",
      "776157\n",
      "778807\n",
      "778805\n",
      "762781\n",
      "494981\n",
      "751668\n",
      "778734\n",
      "778766\n",
      "778785\n",
      "778620\n",
      "765243\n",
      "778785\n",
      "773429\n",
      "778734\n",
      "778785\n",
      "776152\n",
      "778785\n",
      "778785\n",
      "778806\n",
      "778748\n",
      "155526\n",
      "778785\n",
      "778766\n",
      "778748\n",
      "778785\n",
      "663923\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778722\n",
      "122478\n",
      "753265\n",
      "778178\n",
      "778260\n",
      "778766\n",
      "778726\n",
      "778726\n",
      "778748\n",
      "778785\n",
      "586983\n",
      "773429\n",
      "271016\n",
      "778785\n",
      "778785\n",
      "778722\n",
      "724468\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778273\n",
      "712579\n",
      "124973\n",
      "778785\n",
      "778785\n",
      "778771\n",
      "778761\n",
      "778748\n",
      "778785\n",
      "358769\n",
      "770073\n",
      "778748\n",
      "778766\n",
      "751363\n",
      "778785\n",
      "776943\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778668\n",
      "778785\n",
      "659312\n",
      "749053\n",
      "778806\n",
      "776346\n",
      "663923\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "745962\n",
      "778748\n",
      "760328\n",
      "526264\n",
      "778100\n",
      "752582\n",
      "762214\n",
      "778776\n",
      "778785\n",
      "778761\n",
      "778758\n",
      "778785\n",
      "778785\n",
      "778660\n",
      "778766\n",
      "778775\n",
      "778734\n",
      "778637\n",
      "728808\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778761\n",
      "778722\n",
      "778766\n",
      "127215\n",
      "778805\n",
      "778785\n",
      "778785\n",
      "778744\n",
      "778766\n",
      "778798\n",
      "712823\n",
      "778350\n",
      "127066\n",
      "778785\n",
      "778734\n",
      "127254\n",
      "778726\n",
      "778785\n",
      "778652\n",
      "778785\n",
      "778785\n",
      "778774\n",
      "778766\n",
      "778785\n",
      "778810\n",
      "778785\n",
      "778810\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778744\n",
      "778810\n",
      "778744\n",
      "778660\n",
      "778722\n",
      "778734\n",
      "778744\n",
      "778734\n",
      "778785\n",
      "778785\n",
      "778750\n",
      "775137\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778734\n",
      "778734\n",
      "778748\n",
      "778761\n",
      "778740\n",
      "778766\n",
      "778766\n",
      "778766\n",
      "778752\n",
      "778748\n",
      "778785\n",
      "778761\n",
      "778761\n",
      "778766\n",
      "778766\n",
      "778766\n",
      "778130\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778734\n",
      "778774\n",
      "778783\n",
      "763412\n",
      "778785\n",
      "778785\n",
      "772628\n",
      "778766\n",
      "778785\n",
      "778748\n",
      "778766\n",
      "778811\n",
      "773307\n",
      "778785\n",
      "778734\n",
      "778766\n",
      "778785\n",
      "778734\n",
      "778806\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778798\n",
      "778785\n",
      "778748\n",
      "778774\n",
      "778805\n",
      "778785\n",
      "778785\n",
      "130870\n",
      "778785\n",
      "778761\n",
      "778703\n",
      "778785\n",
      "778744\n",
      "778809\n",
      "778766\n",
      "778761\n",
      "778744\n",
      "778785\n",
      "778734\n",
      "131258\n",
      "778703\n",
      "778785\n",
      "778766\n",
      "778774\n",
      "778703\n",
      "778740\n",
      "763240\n",
      "778766\n",
      "305940\n",
      "778785\n",
      "778748\n",
      "778761\n",
      "778748\n",
      "247895\n",
      "778785\n",
      "778785\n",
      "717860\n",
      "778766\n",
      "778773\n",
      "778798\n",
      "751322\n",
      "131925\n",
      "778703\n",
      "778722\n",
      "778766\n",
      "778748\n",
      "763420\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "494165\n",
      "778785\n",
      "773130\n",
      "773130\n",
      "778734\n",
      "778748\n",
      "778766\n",
      "778785\n",
      "778734\n",
      "778273\n",
      "778805\n",
      "778740\n",
      "778785\n",
      "713394\n",
      "778766\n",
      "778734\n",
      "742411\n",
      "778703\n",
      "778748\n",
      "778734\n",
      "778785\n",
      "762598\n",
      "778785\n",
      "778771\n",
      "778785\n",
      "133123\n",
      "778785\n",
      "778744\n",
      "762306\n",
      "133255\n",
      "778748\n",
      "133397\n",
      "778740\n",
      "767743\n",
      "778771\n",
      "778726\n",
      "778766\n",
      "707559\n",
      "778761\n",
      "778774\n",
      "778761\n",
      "778703\n",
      "778748\n",
      "778783\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "763509\n",
      "778748\n",
      "778773\n",
      "778810\n",
      "778776\n",
      "778726\n",
      "778785\n",
      "778774\n",
      "778810\n",
      "763509\n",
      "778785\n",
      "778783\n",
      "778806\n",
      "777847\n",
      "778761\n",
      "778785\n",
      "778637\n",
      "778702\n",
      "141482\n",
      "747576\n",
      "778810\n",
      "778748\n",
      "778766\n",
      "763509\n",
      "778785\n",
      "778748\n",
      "778761\n",
      "778702\n",
      "723527\n",
      "723527\n",
      "716375\n",
      "716375\n",
      "778785\n",
      "762806\n",
      "778726\n",
      "778785\n",
      "778785\n",
      "762806\n",
      "778740\n",
      "778785\n",
      "778761\n",
      "778660\n",
      "778806\n",
      "778681\n",
      "778805\n",
      "778680\n",
      "778785\n",
      "778761\n",
      "778776\n",
      "778761\n",
      "747823\n",
      "734031\n",
      "778785\n",
      "778776\n",
      "778805\n",
      "778761\n",
      "778774\n",
      "778761\n",
      "707559\n",
      "707559\n",
      "707559\n",
      "744141\n",
      "137006\n",
      "137044\n",
      "777048\n",
      "778740\n",
      "778805\n",
      "762411\n",
      "778748\n",
      "776656\n",
      "778810\n",
      "778766\n",
      "778785\n",
      "642404\n",
      "778768\n",
      "778723\n",
      "778785\n",
      "778356\n",
      "778785\n",
      "778740\n",
      "753532\n",
      "778748\n",
      "778735\n",
      "778785\n",
      "707559\n",
      "778785\n",
      "778712\n",
      "763509\n",
      "778703\n",
      "778748\n",
      "778740\n",
      "778785\n",
      "778726\n",
      "778761\n",
      "531724\n",
      "778785\n",
      "778601\n",
      "762806\n",
      "778703\n",
      "778785\n",
      "778785\n",
      "777978\n",
      "707559\n",
      "778785\n",
      "778748\n",
      "741583\n",
      "778785\n",
      "531724\n",
      "741583\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778663\n",
      "777220\n",
      "778663\n",
      "778806\n",
      "778703\n",
      "762952\n",
      "776946\n",
      "763153\n",
      "778726\n",
      "778785\n",
      "141482\n",
      "707559\n",
      "778766\n",
      "778785\n",
      "778748\n",
      "778761\n",
      "778761\n",
      "778807\n",
      "729801\n",
      "778785\n",
      "778726\n",
      "778806\n",
      "778766\n",
      "762952\n",
      "778806\n",
      "778761\n",
      "778761\n",
      "778766\n",
      "642404\n",
      "778766\n",
      "778748\n",
      "707559\n",
      "778766\n",
      "778748\n",
      "778726\n",
      "762806\n",
      "778748\n",
      "778726\n",
      "763509\n",
      "763509\n",
      "778766\n",
      "758241\n",
      "707559\n",
      "778356\n",
      "141482\n",
      "188673\n",
      "778810\n",
      "348926\n",
      "762806\n",
      "762806\n",
      "778761\n",
      "778761\n",
      "778734\n",
      "778810\n",
      "778785\n",
      "778744\n",
      "777800\n",
      "778761\n",
      "762513\n",
      "778748\n",
      "762806\n",
      "778734\n",
      "777791\n",
      "665340\n",
      "772006\n",
      "147981\n",
      "778785\n",
      "762806\n",
      "778774\n",
      "762806\n",
      "778773\n",
      "762806\n",
      "778766\n",
      "778748\n",
      "778783\n",
      "754348\n",
      "762806\n",
      "735426\n",
      "778734\n",
      "772006\n",
      "778716\n",
      "759909\n",
      "715850\n",
      "746258\n",
      "778026\n",
      "778774\n",
      "642723\n",
      "642723\n",
      "777968\n",
      "778748\n",
      "778761\n",
      "778744\n",
      "778210\n",
      "778744\n",
      "777351\n",
      "762806\n",
      "762513\n",
      "778785\n",
      "778761\n",
      "775583\n",
      "762513\n",
      "778785\n",
      "715850\n",
      "762806\n",
      "665340\n",
      "778774\n",
      "762806\n",
      "762806\n",
      "719084\n",
      "778785\n",
      "778785\n",
      "144261\n",
      "762513\n",
      "778761\n",
      "762513\n",
      "778748\n",
      "778785\n",
      "778761\n",
      "778748\n",
      "778807\n",
      "778716\n",
      "778660\n",
      "769679\n",
      "778748\n",
      "778810\n",
      "778726\n",
      "778726\n",
      "778785\n",
      "778806\n",
      "762806\n",
      "778703\n",
      "760340\n",
      "778628\n",
      "665340\n",
      "736850\n",
      "778761\n",
      "778677\n",
      "778785\n",
      "778652\n",
      "778761\n",
      "778744\n",
      "778703\n",
      "778761\n",
      "777374\n",
      "778785\n",
      "778773\n",
      "778761\n",
      "778785\n",
      "762513\n",
      "775272\n",
      "763509\n",
      "778774\n",
      "778761\n",
      "778771\n",
      "778761\n",
      "762806\n",
      "778785\n",
      "778757\n",
      "776946\n",
      "778766\n",
      "778785\n",
      "777699\n",
      "778748\n",
      "424637\n",
      "778210\n",
      "778712\n",
      "778785\n",
      "762806\n",
      "778806\n",
      "778748\n",
      "778761\n",
      "778703\n",
      "778726\n",
      "778805\n",
      "778761\n",
      "778785\n",
      "763509\n",
      "776946\n",
      "778785\n",
      "778766\n",
      "762224\n",
      "778811\n",
      "778748\n",
      "778726\n",
      "762513\n",
      "763509\n",
      "778712\n",
      "778722\n",
      "778761\n",
      "777886\n",
      "778761\n",
      "778785\n",
      "778744\n",
      "763509\n",
      "146877\n",
      "778785\n",
      "778722\n",
      "778766\n",
      "778744\n",
      "776946\n",
      "776946\n",
      "777948\n",
      "778734\n",
      "762806\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778744\n",
      "762411\n",
      "762513\n",
      "651453\n",
      "762513\n",
      "778748\n",
      "778744\n",
      "778774\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "147899\n",
      "762411\n",
      "147981\n",
      "762806\n",
      "754070\n",
      "760737\n",
      "762411\n",
      "662193\n",
      "775095\n",
      "778806\n",
      "778806\n",
      "778774\n",
      "778766\n",
      "778781\n",
      "149353\n",
      "778766\n",
      "762963\n",
      "696522\n",
      "778243\n",
      "778761\n",
      "778748\n",
      "778637\n",
      "778748\n",
      "778734\n",
      "775523\n",
      "778744\n",
      "778734\n",
      "778748\n",
      "778766\n",
      "778766\n",
      "776946\n",
      "778734\n",
      "714446\n",
      "748075\n",
      "778785\n",
      "372930\n",
      "778652\n",
      "678102\n",
      "778761\n",
      "778740\n",
      "778785\n",
      "645956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778734\n",
      "778766\n",
      "778726\n",
      "778753\n",
      "778785\n",
      "778785\n",
      "767540\n",
      "742483\n",
      "778601\n",
      "778806\n",
      "778751\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "777956\n",
      "778761\n",
      "778222\n",
      "778748\n",
      "353102\n",
      "778806\n",
      "778798\n",
      "778761\n",
      "772628\n",
      "353102\n",
      "778785\n",
      "778785\n",
      "721476\n",
      "778766\n",
      "778785\n",
      "776840\n",
      "778222\n",
      "778766\n",
      "778748\n",
      "778766\n",
      "778752\n",
      "778785\n",
      "776124\n",
      "155326\n",
      "353102\n",
      "778748\n",
      "697677\n",
      "778734\n",
      "155638\n",
      "778766\n",
      "778766\n",
      "155751\n",
      "762734\n",
      "778766\n",
      "778785\n",
      "778002\n",
      "778504\n",
      "778785\n",
      "778748\n",
      "778766\n",
      "778766\n",
      "778734\n",
      "778785\n",
      "778766\n",
      "157430\n",
      "778761\n",
      "778726\n",
      "778785\n",
      "778744\n",
      "778766\n",
      "778785\n",
      "778002\n",
      "769355\n",
      "778716\n",
      "778748\n",
      "778734\n",
      "778174\n",
      "778652\n",
      "778785\n",
      "777873\n",
      "778785\n",
      "645956\n",
      "778785\n",
      "778748\n",
      "162288\n",
      "714373\n",
      "778785\n",
      "778785\n",
      "776356\n",
      "772628\n",
      "778210\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778638\n",
      "778702\n",
      "778785\n",
      "778798\n",
      "353102\n",
      "387719\n",
      "778761\n",
      "778785\n",
      "778002\n",
      "778748\n",
      "177220\n",
      "778785\n",
      "778785\n",
      "778783\n",
      "778748\n",
      "778785\n",
      "778026\n",
      "778751\n",
      "778806\n",
      "778222\n",
      "778734\n",
      "778734\n",
      "602855\n",
      "773307\n",
      "778734\n",
      "748075\n",
      "778766\n",
      "778808\n",
      "778766\n",
      "778243\n",
      "163238\n",
      "778785\n",
      "609765\n",
      "778222\n",
      "165420\n",
      "778776\n",
      "778744\n",
      "774461\n",
      "778785\n",
      "772835\n",
      "164274\n",
      "165197\n",
      "737659\n",
      "775495\n",
      "778774\n",
      "763298\n",
      "778734\n",
      "164805\n",
      "748075\n",
      "778785\n",
      "778783\n",
      "165593\n",
      "760685\n",
      "165420\n",
      "772628\n",
      "778774\n",
      "778806\n",
      "778785\n",
      "763297\n",
      "778751\n",
      "762411\n",
      "763102\n",
      "722655\n",
      "775495\n",
      "778761\n",
      "762806\n",
      "762806\n",
      "472153\n",
      "778806\n",
      "778744\n",
      "173244\n",
      "778761\n",
      "762806\n",
      "778785\n",
      "778504\n",
      "657297\n",
      "778740\n",
      "776946\n",
      "778774\n",
      "210750\n",
      "172615\n",
      "778748\n",
      "778766\n",
      "172615\n",
      "776946\n",
      "210750\n",
      "210750\n",
      "172235\n",
      "763509\n",
      "778773\n",
      "762806\n",
      "763509\n",
      "778761\n",
      "172235\n",
      "171868\n",
      "778766\n",
      "778748\n",
      "172235\n",
      "647391\n",
      "720609\n",
      "754875\n",
      "778761\n",
      "168932\n",
      "778703\n",
      "778785\n",
      "778734\n",
      "171868\n",
      "472153\n",
      "778412\n",
      "778785\n",
      "737903\n",
      "778243\n",
      "747901\n",
      "742411\n",
      "778766\n",
      "169912\n",
      "778703\n",
      "778748\n",
      "778637\n",
      "778785\n",
      "778774\n",
      "767127\n",
      "778748\n",
      "778773\n",
      "171868\n",
      "172321\n",
      "778748\n",
      "172321\n",
      "704692\n",
      "776946\n",
      "172615\n",
      "777020\n",
      "778342\n",
      "168932\n",
      "172235\n",
      "778785\n",
      "778785\n",
      "172615\n",
      "778761\n",
      "778748\n",
      "778806\n",
      "169912\n",
      "778776\n",
      "778734\n",
      "395705\n",
      "570759\n",
      "172369\n",
      "740393\n",
      "778761\n",
      "778748\n",
      "719946\n",
      "778761\n",
      "778810\n",
      "210750\n",
      "172719\n",
      "694669\n",
      "760301\n",
      "778785\n",
      "778716\n",
      "778785\n",
      "778748\n",
      "170022\n",
      "173172\n",
      "778748\n",
      "778806\n",
      "170262\n",
      "171868\n",
      "778723\n",
      "170346\n",
      "778785\n",
      "172719\n",
      "172326\n",
      "778774\n",
      "171868\n",
      "778777\n",
      "763509\n",
      "778761\n",
      "210750\n",
      "778806\n",
      "778774\n",
      "778172\n",
      "778766\n",
      "778721\n",
      "756211\n",
      "761525\n",
      "773130\n",
      "770435\n",
      "778748\n",
      "778703\n",
      "778783\n",
      "760340\n",
      "776946\n",
      "756094\n",
      "778774\n",
      "734420\n",
      "778774\n",
      "714877\n",
      "776591\n",
      "778740\n",
      "488650\n",
      "778709\n",
      "708634\n",
      "171833\n",
      "172369\n",
      "778766\n",
      "697781\n",
      "349846\n",
      "696761\n",
      "172369\n",
      "762806\n",
      "472153\n",
      "778748\n",
      "778810\n",
      "762806\n",
      "718414\n",
      "761348\n",
      "776946\n",
      "657297\n",
      "778748\n",
      "778243\n",
      "762092\n",
      "778734\n",
      "778734\n",
      "172719\n",
      "762806\n",
      "776805\n",
      "778748\n",
      "763509\n",
      "778734\n",
      "778726\n",
      "778748\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "778761\n",
      "762224\n",
      "778766\n",
      "770655\n",
      "778748\n",
      "739557\n",
      "738009\n",
      "778766\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778776\n",
      "773130\n",
      "778785\n",
      "778761\n",
      "761525\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778273\n",
      "635809\n",
      "181192\n",
      "778774\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "760328\n",
      "778785\n",
      "773549\n",
      "778785\n",
      "714357\n",
      "778785\n",
      "778785\n",
      "778716\n",
      "778734\n",
      "778637\n",
      "761363\n",
      "759291\n",
      "778663\n",
      "778785\n",
      "778774\n",
      "685991\n",
      "778766\n",
      "777374\n",
      "761548\n",
      "778785\n",
      "778273\n",
      "762747\n",
      "175886\n",
      "778748\n",
      "739557\n",
      "778761\n",
      "777374\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "761363\n",
      "177524\n",
      "778785\n",
      "778785\n",
      "763405\n",
      "586563\n",
      "778774\n",
      "176674\n",
      "778776\n",
      "754348\n",
      "778637\n",
      "778785\n",
      "778766\n",
      "778761\n",
      "778761\n",
      "778785\n",
      "778783\n",
      "778809\n",
      "778723\n",
      "778748\n",
      "778766\n",
      "778748\n",
      "778748\n",
      "778761\n",
      "778766\n",
      "736633\n",
      "778785\n",
      "778785\n",
      "778709\n",
      "778785\n",
      "744690\n",
      "630566\n",
      "177817\n",
      "778785\n",
      "778785\n",
      "778637\n",
      "778748\n",
      "778748\n",
      "778744\n",
      "778785\n",
      "778785\n",
      "778726\n",
      "778785\n",
      "778766\n",
      "778766\n",
      "642542\n",
      "778785\n",
      "778785\n",
      "778691\n",
      "778766\n",
      "778810\n",
      "778748\n",
      "778726\n",
      "763405\n",
      "778744\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "746158\n",
      "778785\n",
      "777807\n",
      "778785\n",
      "778761\n",
      "777374\n",
      "778726\n",
      "763405\n",
      "778758\n",
      "778766\n",
      "778766\n",
      "778761\n",
      "755647\n",
      "760328\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778734\n",
      "778734\n",
      "756462\n",
      "778785\n",
      "778734\n",
      "777873\n",
      "776513\n",
      "675589\n",
      "752439\n",
      "778761\n",
      "778726\n",
      "550843\n",
      "778637\n",
      "778652\n",
      "778801\n",
      "778761\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "778766\n",
      "778726\n",
      "776145\n",
      "754348\n",
      "778748\n",
      "778716\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778806\n",
      "771259\n",
      "778774\n",
      "778774\n",
      "778805\n",
      "778785\n",
      "778748\n",
      "742529\n",
      "778801\n",
      "778766\n",
      "778785\n",
      "775583\n",
      "778748\n",
      "778785\n",
      "778663\n",
      "760777\n",
      "778776\n",
      "778766\n",
      "778785\n",
      "182542\n",
      "762411\n",
      "778785\n",
      "778734\n",
      "778748\n",
      "778748\n",
      "778771\n",
      "187486\n",
      "776126\n",
      "778771\n",
      "778785\n",
      "778761\n",
      "778748\n",
      "778703\n",
      "778740\n",
      "778785\n",
      "778063\n",
      "778785\n",
      "778806\n",
      "778806\n",
      "778734\n",
      "778660\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "189019\n",
      "778785\n",
      "778637\n",
      "778785\n",
      "189248\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "778748\n",
      "778806\n",
      "778806\n",
      "778785\n",
      "778785\n",
      "778726\n",
      "778785\n",
      "778748\n",
      "706712\n",
      "778703\n",
      "778726\n",
      "778761\n",
      "778734\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "777968\n",
      "778785\n",
      "778785\n",
      "735790\n",
      "778702\n",
      "778734\n",
      "542937\n",
      "778734\n",
      "778370\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "685998\n",
      "778761\n",
      "778734\n",
      "186437\n",
      "189248\n",
      "776126\n",
      "774046\n",
      "740257\n",
      "778785\n",
      "778740\n",
      "187486\n",
      "187220\n",
      "778785\n",
      "778785\n",
      "752649\n",
      "778243\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778734\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778774\n",
      "396273\n",
      "771537\n",
      "778681\n",
      "778748\n",
      "189248\n",
      "778748\n",
      "186437\n",
      "778785\n",
      "778748\n",
      "778766\n",
      "778785\n",
      "778637\n",
      "778748\n",
      "778785\n",
      "751858\n",
      "740257\n",
      "778774\n",
      "778751\n",
      "778748\n",
      "778726\n",
      "778785\n",
      "778748\n",
      "778723\n",
      "778751\n",
      "778776\n",
      "778774\n",
      "778785\n",
      "778768\n",
      "763446\n",
      "778740\n",
      "778663\n",
      "778766\n",
      "778785\n",
      "768775\n",
      "777048\n",
      "778412\n",
      "778748\n",
      "778748\n",
      "778761\n",
      "775583\n",
      "778785\n",
      "775495\n",
      "778580\n",
      "751858\n",
      "778723\n",
      "187221\n",
      "778785\n",
      "776513\n",
      "778748\n",
      "778702\n",
      "778726\n",
      "778806\n",
      "751858\n",
      "778748\n",
      "778273\n",
      "778716\n",
      "778748\n",
      "778785\n",
      "759291\n",
      "778785\n",
      "778761\n",
      "778774\n",
      "778774\n",
      "778702\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778740\n",
      "778785\n",
      "778680\n",
      "726461\n",
      "778785\n",
      "778785\n",
      "188931\n",
      "778748\n",
      "778761\n",
      "778785\n",
      "716744\n",
      "778740\n",
      "778740\n",
      "188616\n",
      "188673\n",
      "778771\n",
      "778703\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "188931\n",
      "716744\n",
      "778766\n",
      "778785\n",
      "778703\n",
      "778703\n",
      "778771\n",
      "746289\n",
      "778785\n",
      "778748\n",
      "778740\n",
      "778748\n",
      "778761\n",
      "195621\n",
      "778412\n",
      "718164\n",
      "778761\n",
      "778412\n",
      "778761\n",
      "778740\n",
      "760685\n",
      "778703\n",
      "778748\n",
      "778703\n",
      "778412\n",
      "778774\n",
      "778785\n",
      "778785\n",
      "778774\n",
      "778761\n",
      "778761\n",
      "778703\n",
      "778774\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "778761\n",
      "778734\n",
      "778807\n",
      "656799\n",
      "778785\n",
      "778412\n",
      "718164\n",
      "778744\n",
      "718164\n",
      "778595\n",
      "778774\n",
      "778726\n",
      "778726\n",
      "778748\n",
      "190993\n",
      "778807\n",
      "778702\n",
      "778785\n",
      "778810\n",
      "778785\n",
      "778412\n",
      "778761\n",
      "191295\n",
      "194169\n",
      "778748\n",
      "778412\n",
      "778753\n",
      "195621\n",
      "778810\n",
      "778785\n",
      "777807\n",
      "778618\n",
      "778761\n",
      "778761\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778761\n",
      "778726\n",
      "195621\n",
      "192285\n",
      "778761\n",
      "778273\n",
      "778731\n",
      "717677\n",
      "778716\n",
      "778652\n",
      "778785\n",
      "778726\n",
      "778748\n",
      "192596\n",
      "760685\n",
      "775253\n",
      "778748\n",
      "778761\n",
      "763173\n",
      "778412\n",
      "778702\n",
      "778761\n",
      "778723\n",
      "778761\n",
      "778262\n",
      "778785\n",
      "778761\n",
      "778810\n",
      "778748\n",
      "762164\n",
      "778810\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "747720\n",
      "778785\n",
      "778652\n",
      "778766\n",
      "778810\n",
      "778761\n",
      "778809\n",
      "778680\n",
      "778703\n",
      "778785\n",
      "778785\n",
      "778726\n",
      "778761\n",
      "778785\n",
      "778703\n",
      "778774\n",
      "720157\n",
      "778734\n",
      "778761\n",
      "778748\n",
      "719582\n",
      "778748\n",
      "778766\n",
      "778758\n",
      "778726\n",
      "776946\n",
      "776152\n",
      "778412\n",
      "778785\n",
      "778766\n",
      "778748\n",
      "778637\n",
      "777565\n",
      "194839\n",
      "778785\n",
      "778776\n",
      "778726\n",
      "195621\n",
      "773130\n",
      "778412\n",
      "778766\n",
      "778761\n",
      "778774\n",
      "778803\n",
      "778740\n",
      "777847\n",
      "778785\n",
      "195622\n",
      "778761\n",
      "778785\n",
      "777847\n",
      "195815\n",
      "778761\n",
      "776946\n",
      "719582\n",
      "778726\n",
      "778748\n",
      "716744\n",
      "778703\n",
      "778748\n",
      "778637\n",
      "778766\n",
      "776946\n",
      "778798\n",
      "730426\n",
      "764739\n",
      "778807\n",
      "778716\n",
      "778776\n",
      "726005\n",
      "776946\n",
      "778766\n",
      "778766\n",
      "778216\n",
      "760685\n",
      "778785\n",
      "763118\n",
      "778771\n",
      "619956\n",
      "778734\n",
      "778785\n",
      "778785\n",
      "442758\n",
      "778637\n",
      "778806\n",
      "778806\n",
      "778774\n",
      "778785\n",
      "778652\n",
      "778748\n",
      "778798\n",
      "778785\n",
      "355245\n",
      "778750\n",
      "778785\n",
      "778798\n",
      "778785\n",
      "778734\n",
      "778734\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778740\n",
      "778766\n",
      "778744\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778776\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778798\n",
      "778758\n",
      "778785\n",
      "778785\n",
      "778774\n",
      "778785\n",
      "778761\n",
      "778703\n",
      "778734\n",
      "778798\n",
      "778809\n",
      "778766\n",
      "778774\n",
      "778785\n",
      "778761\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "720708\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778744\n",
      "778785\n",
      "778748\n",
      "778766\n",
      "778773\n",
      "778785\n",
      "778766\n",
      "778774\n",
      "778785\n",
      "778748\n",
      "778809\n",
      "778785\n",
      "778809\n",
      "778703\n",
      "778091\n",
      "778744\n",
      "778785\n",
      "763297\n",
      "205217\n",
      "778785\n",
      "778766\n",
      "778726\n",
      "778785\n",
      "778726\n",
      "778785\n",
      "778740\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "778806\n",
      "778766\n",
      "778748\n",
      "778785\n",
      "778761\n",
      "778753\n",
      "778660\n",
      "778785\n",
      "778766\n",
      "778801\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "777886\n",
      "778785\n",
      "778785\n",
      "778798\n",
      "778809\n",
      "778774\n",
      "778801\n",
      "778783\n",
      "778761\n",
      "778766\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778776\n",
      "778785\n",
      "778785\n",
      "778798\n",
      "778785\n",
      "778798\n",
      "778785\n",
      "778766\n",
      "778766\n",
      "778748\n",
      "778798\n",
      "778734\n",
      "778726\n",
      "778785\n",
      "778412\n",
      "778663\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778726\n",
      "778748\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778703\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778660\n",
      "778761\n",
      "778798\n",
      "778761\n",
      "778798\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778798\n",
      "213024\n",
      "213077\n",
      "778637\n",
      "778744\n",
      "778812\n",
      "778785\n",
      "778774\n",
      "778734\n",
      "778785\n",
      "778766\n",
      "778748\n",
      "778618\n",
      "778761\n",
      "778806\n",
      "778605\n",
      "778761\n",
      "778771\n",
      "778748\n",
      "778806\n",
      "778702\n",
      "778748\n",
      "778618\n",
      "778774\n",
      "778777\n",
      "778801\n",
      "315667\n",
      "778785\n",
      "778761\n",
      "778026\n",
      "778761\n",
      "778202\n",
      "759664\n",
      "778703\n",
      "778748\n",
      "765182\n",
      "778771\n",
      "778752\n",
      "508029\n",
      "778702\n",
      "428346\n",
      "778785\n",
      "778703\n",
      "778785\n",
      "778801\n",
      "778740\n",
      "777374\n",
      "778785\n",
      "778761\n",
      "778806\n",
      "778466\n",
      "778766\n",
      "759664\n",
      "778771\n",
      "778703\n",
      "759664\n",
      "778217\n",
      "763509\n",
      "778726\n",
      "765182\n",
      "778801\n",
      "778785\n",
      "776946\n",
      "777800\n",
      "778785\n",
      "778775\n",
      "759664\n",
      "778748\n",
      "778785\n",
      "778292\n",
      "778618\n",
      "778748\n",
      "759664\n",
      "778785\n",
      "778210\n",
      "775523\n",
      "778466\n",
      "563706\n",
      "778761\n",
      "778740\n",
      "778785\n",
      "778652\n",
      "778761\n",
      "215724\n",
      "778771\n",
      "778748\n",
      "778262\n",
      "759664\n",
      "778716\n",
      "778807\n",
      "778243\n",
      "778785\n",
      "778761\n",
      "778273\n",
      "778726\n",
      "778761\n",
      "775556\n",
      "770810\n",
      "687794\n",
      "775638\n",
      "778748\n",
      "763509\n",
      "778785\n",
      "778785\n",
      "778210\n",
      "778785\n",
      "778748\n",
      "778774\n",
      "763299\n",
      "724783\n",
      "776270\n",
      "778766\n",
      "725993\n",
      "778702\n",
      "778752\n",
      "778676\n",
      "778703\n",
      "778785\n",
      "778202\n",
      "778785\n",
      "778785\n",
      "657878\n",
      "778806\n",
      "778618\n",
      "778711\n",
      "778806\n",
      "778202\n",
      "778637\n",
      "771399\n",
      "778766\n",
      "778785\n",
      "778652\n",
      "778785\n",
      "778637\n",
      "237649\n",
      "778748\n",
      "778783\n",
      "778748\n",
      "778785\n",
      "778147\n",
      "237649\n",
      "778748\n",
      "766999\n",
      "759664\n",
      "776854\n",
      "760205\n",
      "778761\n",
      "726475\n",
      "778785\n",
      "778785\n",
      "778744\n",
      "778785\n",
      "778805\n",
      "778734\n",
      "728546\n",
      "778761\n",
      "763509\n",
      "778761\n",
      "775850\n",
      "778504\n",
      "778785\n",
      "778748\n",
      "778703\n",
      "778752\n",
      "778761\n",
      "726475\n",
      "778801\n",
      "763509\n",
      "778801\n",
      "778748\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "767365\n",
      "778147\n",
      "778748\n",
      "778766\n",
      "759664\n",
      "778734\n",
      "778761\n",
      "777968\n",
      "778785\n",
      "778740\n",
      "778774\n",
      "778763\n",
      "218167\n",
      "778637\n",
      "778766\n",
      "778744\n",
      "778637\n",
      "778726\n",
      "778785\n",
      "778761\n",
      "630572\n",
      "778761\n",
      "763446\n",
      "778761\n",
      "778773\n",
      "778761\n",
      "763346\n",
      "777433\n",
      "778681\n",
      "761001\n",
      "763471\n",
      "778752\n",
      "775495\n",
      "773177\n",
      "739584\n",
      "466948\n",
      "561076\n",
      "778785\n",
      "775926\n",
      "778748\n",
      "758252\n",
      "778216\n",
      "763509\n",
      "219634\n",
      "778777\n",
      "778744\n",
      "778722\n",
      "762606\n",
      "778726\n",
      "778726\n",
      "762537\n",
      "759664\n",
      "761282\n",
      "778785\n",
      "778766\n",
      "778806\n",
      "778761\n",
      "778806\n",
      "778723\n",
      "778761\n",
      "762164\n",
      "778785\n",
      "726142\n",
      "778766\n",
      "700837\n",
      "778761\n",
      "773275\n",
      "777940\n",
      "778766\n",
      "742364\n",
      "757742\n",
      "778785\n",
      "760340\n",
      "754348\n",
      "717245\n",
      "776946\n",
      "778740\n",
      "762411\n",
      "778751\n",
      "758681\n",
      "778766\n",
      "778723\n",
      "655446\n",
      "760340\n",
      "746082\n",
      "778565\n",
      "739762\n",
      "778785\n",
      "754348\n",
      "778785\n",
      "664492\n",
      "756094\n",
      "778703\n",
      "778757\n",
      "777791\n",
      "778740\n",
      "778806\n",
      "761080\n",
      "622875\n",
      "762455\n",
      "778503\n",
      "778776\n",
      "760463\n",
      "717671\n",
      "566508\n",
      "778761\n",
      "222654\n",
      "778726\n",
      "226096\n",
      "769696\n",
      "778766\n",
      "656799\n",
      "761369\n",
      "778204\n",
      "763509\n",
      "778785\n",
      "776152\n",
      "778730\n",
      "762164\n",
      "776949\n",
      "778726\n",
      "778726\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "754348\n",
      "763509\n",
      "778774\n",
      "778761\n",
      "778748\n",
      "335153\n",
      "223704\n",
      "763509\n",
      "762306\n",
      "721881\n",
      "749373\n",
      "778761\n",
      "778473\n",
      "223945\n",
      "778294\n",
      "226520\n",
      "778748\n",
      "224283\n",
      "224207\n",
      "224284\n",
      "778773\n",
      "778726\n",
      "778748\n",
      "224460\n",
      "661539\n",
      "778807\n",
      "778766\n",
      "778785\n",
      "707508\n",
      "778766\n",
      "778785\n",
      "700610\n",
      "778663\n",
      "778785\n",
      "778785\n",
      "778801\n",
      "762306\n",
      "763153\n",
      "769089\n",
      "248259\n",
      "765886\n",
      "761363\n",
      "656944\n",
      "778748\n",
      "760340\n",
      "778761\n",
      "776509\n",
      "778761\n",
      "759519\n",
      "762806\n",
      "778734\n",
      "778761\n",
      "777968\n",
      "662206\n",
      "757285\n",
      "225900\n",
      "738009\n",
      "778766\n",
      "778809\n",
      "778774\n",
      "778609\n",
      "760589\n",
      "778663\n",
      "777774\n",
      "755481\n",
      "778344\n",
      "226520\n",
      "739762\n",
      "778761\n",
      "756111\n",
      "778766\n",
      "778744\n",
      "760777\n",
      "763102\n",
      "778766\n",
      "778748\n",
      "778734\n",
      "763218\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "778637\n",
      "778766\n",
      "778785\n",
      "778806\n",
      "763103\n",
      "778785\n",
      "778748\n",
      "778716\n",
      "778210\n",
      "778766\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "230163\n",
      "778766\n",
      "778726\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778716\n",
      "778703\n",
      "778750\n",
      "778748\n",
      "754348\n",
      "778757\n",
      "775382\n",
      "778748\n",
      "778785\n",
      "741009\n",
      "778785\n",
      "229305\n",
      "754348\n",
      "778609\n",
      "778785\n",
      "778748\n",
      "778766\n",
      "778785\n",
      "778761\n",
      "778748\n",
      "778726\n",
      "230163\n",
      "233989\n",
      "778273\n",
      "761363\n",
      "778726\n",
      "778751\n",
      "762644\n",
      "754070\n",
      "778783\n",
      "778774\n",
      "778785\n",
      "778753\n",
      "772487\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778761\n",
      "778766\n",
      "773099\n",
      "754070\n",
      "778774\n",
      "778273\n",
      "656209\n",
      "778785\n",
      "778774\n",
      "778776\n",
      "762411\n",
      "778726\n",
      "777374\n",
      "237339\n",
      "778748\n",
      "778774\n",
      "761297\n",
      "777888\n",
      "406094\n",
      "685878\n",
      "763218\n",
      "778785\n",
      "763218\n",
      "778748\n",
      "237339\n",
      "778785\n",
      "778260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763218\n",
      "778734\n",
      "737959\n",
      "233989\n",
      "778325\n",
      "778785\n",
      "778785\n",
      "778773\n",
      "778761\n",
      "778766\n",
      "644110\n",
      "778766\n",
      "778806\n",
      "778726\n",
      "778637\n",
      "336573\n",
      "778776\n",
      "763523\n",
      "778748\n",
      "763218\n",
      "778785\n",
      "778734\n",
      "778785\n",
      "778785\n",
      "778726\n",
      "708987\n",
      "769005\n",
      "233634\n",
      "778766\n",
      "778766\n",
      "778807\n",
      "763509\n",
      "776946\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778774\n",
      "778785\n",
      "778766\n",
      "778766\n",
      "778783\n",
      "778785\n",
      "776946\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "778748\n",
      "778380\n",
      "762747\n",
      "518991\n",
      "760737\n",
      "778766\n",
      "778642\n",
      "396968\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "777968\n",
      "763375\n",
      "778726\n",
      "778761\n",
      "778761\n",
      "235808\n",
      "235880\n",
      "778748\n",
      "763126\n",
      "776793\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778712\n",
      "757750\n",
      "778785\n",
      "778785\n",
      "771140\n",
      "760737\n",
      "778785\n",
      "763088\n",
      "778785\n",
      "763103\n",
      "236819\n",
      "763335\n",
      "763509\n",
      "762806\n",
      "775495\n",
      "756111\n",
      "778785\n",
      "778748\n",
      "778652\n",
      "237944\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778751\n",
      "755647\n",
      "774345\n",
      "711951\n",
      "237531\n",
      "778785\n",
      "778748\n",
      "591719\n",
      "778702\n",
      "778776\n",
      "778748\n",
      "777575\n",
      "627835\n",
      "778734\n",
      "778774\n",
      "778748\n",
      "778722\n",
      "778748\n",
      "778748\n",
      "778726\n",
      "778748\n",
      "778716\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "778703\n",
      "778798\n",
      "778702\n",
      "778751\n",
      "238564\n",
      "778766\n",
      "761282\n",
      "543579\n",
      "778761\n",
      "778748\n",
      "778785\n",
      "778766\n",
      "778637\n",
      "667219\n",
      "778776\n",
      "778766\n",
      "778785\n",
      "778748\n",
      "775601\n",
      "778744\n",
      "778785\n",
      "758681\n",
      "778726\n",
      "441299\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "627835\n",
      "778785\n",
      "239734\n",
      "447232\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778808\n",
      "778766\n",
      "778637\n",
      "759853\n",
      "773103\n",
      "778785\n",
      "712766\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "540251\n",
      "778785\n",
      "778774\n",
      "240743\n",
      "778763\n",
      "778761\n",
      "667219\n",
      "778785\n",
      "778703\n",
      "778748\n",
      "778702\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778766\n",
      "778344\n",
      "778809\n",
      "778766\n",
      "778761\n",
      "778702\n",
      "241527\n",
      "778744\n",
      "777742\n",
      "761047\n",
      "778771\n",
      "777807\n",
      "778731\n",
      "778703\n",
      "777302\n",
      "778342\n",
      "778785\n",
      "778766\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778147\n",
      "769719\n",
      "778798\n",
      "778807\n",
      "778766\n",
      "778748\n",
      "778565\n",
      "771259\n",
      "777965\n",
      "778766\n",
      "778243\n",
      "778785\n",
      "667219\n",
      "314261\n",
      "755647\n",
      "778726\n",
      "778805\n",
      "778744\n",
      "778785\n",
      "255753\n",
      "778748\n",
      "712988\n",
      "778748\n",
      "778652\n",
      "758681\n",
      "778785\n",
      "777302\n",
      "778785\n",
      "707361\n",
      "709583\n",
      "778783\n",
      "667219\n",
      "314261\n",
      "314261\n",
      "314261\n",
      "627835\n",
      "627835\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "770039\n",
      "649832\n",
      "778637\n",
      "778785\n",
      "778785\n",
      "778805\n",
      "778773\n",
      "314261\n",
      "778748\n",
      "778785\n",
      "761337\n",
      "778774\n",
      "314261\n",
      "778766\n",
      "255753\n",
      "778785\n",
      "778785\n",
      "778751\n",
      "778766\n",
      "627835\n",
      "778748\n",
      "534326\n",
      "627835\n",
      "245115\n",
      "740075\n",
      "775095\n",
      "534302\n",
      "778771\n",
      "751537\n",
      "777374\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "777374\n",
      "778803\n",
      "245629\n",
      "774198\n",
      "778592\n",
      "778774\n",
      "762747\n",
      "776361\n",
      "778716\n",
      "778740\n",
      "778785\n",
      "778703\n",
      "778766\n",
      "765281\n",
      "778748\n",
      "777565\n",
      "246267\n",
      "778703\n",
      "762781\n",
      "763299\n",
      "778744\n",
      "778748\n",
      "772462\n",
      "778734\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778504\n",
      "765886\n",
      "778734\n",
      "778776\n",
      "778618\n",
      "778753\n",
      "769383\n",
      "246881\n",
      "778273\n",
      "778592\n",
      "756393\n",
      "778761\n",
      "778734\n",
      "778734\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "773282\n",
      "778785\n",
      "778273\n",
      "778740\n",
      "778204\n",
      "778785\n",
      "619956\n",
      "752072\n",
      "771765\n",
      "778273\n",
      "753458\n",
      "778761\n",
      "778726\n",
      "778726\n",
      "247895\n",
      "247921\n",
      "247995\n",
      "761363\n",
      "751437\n",
      "778785\n",
      "778785\n",
      "250726\n",
      "778734\n",
      "778412\n",
      "778785\n",
      "677096\n",
      "677358\n",
      "778758\n",
      "248988\n",
      "761548\n",
      "761548\n",
      "627653\n",
      "248592\n",
      "761363\n",
      "778716\n",
      "778656\n",
      "778785\n",
      "778703\n",
      "778785\n",
      "760687\n",
      "778726\n",
      "756573\n",
      "778771\n",
      "778592\n",
      "778748\n",
      "250805\n",
      "722655\n",
      "778748\n",
      "778734\n",
      "536341\n",
      "250805\n",
      "420685\n",
      "250673\n",
      "250805\n",
      "778663\n",
      "482225\n",
      "778774\n",
      "778788\n",
      "761363\n",
      "778656\n",
      "744250\n",
      "778748\n",
      "249731\n",
      "250760\n",
      "778734\n",
      "778807\n",
      "250805\n",
      "759118\n",
      "249950\n",
      "778745\n",
      "778771\n",
      "711217\n",
      "420685\n",
      "250112\n",
      "778766\n",
      "250673\n",
      "716807\n",
      "746430\n",
      "740805\n",
      "777374\n",
      "747189\n",
      "751180\n",
      "778785\n",
      "778751\n",
      "778703\n",
      "778703\n",
      "778761\n",
      "250673\n",
      "250726\n",
      "250760\n",
      "778776\n",
      "250805\n",
      "778748\n",
      "650155\n",
      "778785\n",
      "250959\n",
      "672954\n",
      "778716\n",
      "778592\n",
      "747158\n",
      "738933\n",
      "650155\n",
      "436408\n",
      "709982\n",
      "682765\n",
      "540251\n",
      "778703\n",
      "778748\n",
      "778785\n",
      "710171\n",
      "778734\n",
      "753456\n",
      "778748\n",
      "778748\n",
      "746534\n",
      "778806\n",
      "778663\n",
      "778748\n",
      "747823\n",
      "777374\n",
      "778748\n",
      "778748\n",
      "743898\n",
      "778703\n",
      "252047\n",
      "552672\n",
      "252108\n",
      "778748\n",
      "778726\n",
      "668875\n",
      "778776\n",
      "778774\n",
      "778751\n",
      "778734\n",
      "778751\n",
      "778748\n",
      "763102\n",
      "778785\n",
      "778748\n",
      "717030\n",
      "252884\n",
      "778761\n",
      "778748\n",
      "252998\n",
      "526164\n",
      "723986\n",
      "778637\n",
      "741650\n",
      "778748\n",
      "778672\n",
      "778637\n",
      "778785\n",
      "742868\n",
      "479967\n",
      "778748\n",
      "778753\n",
      "778748\n",
      "760685\n",
      "778766\n",
      "778748\n",
      "778703\n",
      "778703\n",
      "778785\n",
      "760685\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "750602\n",
      "778748\n",
      "778748\n",
      "778703\n",
      "763020\n",
      "777049\n",
      "778734\n",
      "777303\n",
      "768343\n",
      "778785\n",
      "777968\n",
      "778785\n",
      "778748\n",
      "778766\n",
      "751035\n",
      "254716\n",
      "778761\n",
      "254787\n",
      "778723\n",
      "778748\n",
      "254883\n",
      "778734\n",
      "778785\n",
      "681356\n",
      "778785\n",
      "778785\n",
      "778807\n",
      "777846\n",
      "778726\n",
      "778702\n",
      "778748\n",
      "776751\n",
      "778702\n",
      "770039\n",
      "778785\n",
      "714357\n",
      "775495\n",
      "778785\n",
      "778350\n",
      "778785\n",
      "778785\n",
      "762411\n",
      "751138\n",
      "778273\n",
      "778703\n",
      "255999\n",
      "778748\n",
      "778734\n",
      "778580\n",
      "778785\n",
      "778806\n",
      "778243\n",
      "256415\n",
      "778758\n",
      "778766\n",
      "778031\n",
      "778734\n",
      "778785\n",
      "778751\n",
      "778726\n",
      "714013\n",
      "778748\n",
      "778801\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "761363\n",
      "257091\n",
      "777374\n",
      "778748\n",
      "728136\n",
      "778748\n",
      "778785\n",
      "777969\n",
      "257400\n",
      "759291\n",
      "778726\n",
      "778748\n",
      "778744\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778716\n",
      "777672\n",
      "778773\n",
      "778273\n",
      "778748\n",
      "775495\n",
      "258560\n",
      "741110\n",
      "778726\n",
      "778748\n",
      "778210\n",
      "769696\n",
      "778748\n",
      "778748\n",
      "778766\n",
      "778748\n",
      "778703\n",
      "777774\n",
      "778748\n",
      "778785\n",
      "258533\n",
      "259814\n",
      "778785\n",
      "778273\n",
      "259699\n",
      "778748\n",
      "765182\n",
      "778773\n",
      "710558\n",
      "778785\n",
      "778504\n",
      "778716\n",
      "758232\n",
      "778785\n",
      "778785\n",
      "762411\n",
      "778751\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "757281\n",
      "778785\n",
      "259699\n",
      "769696\n",
      "259529\n",
      "756111\n",
      "259644\n",
      "778723\n",
      "778637\n",
      "774461\n",
      "259805\n",
      "338385\n",
      "634475\n",
      "768671\n",
      "523109\n",
      "777886\n",
      "761363\n",
      "260065\n",
      "770235\n",
      "778785\n",
      "778748\n",
      "760328\n",
      "778785\n",
      "778734\n",
      "778734\n",
      "740353\n",
      "760328\n",
      "778785\n",
      "760687\n",
      "778785\n",
      "653356\n",
      "778703\n",
      "747914\n",
      "778726\n",
      "778637\n",
      "778638\n",
      "770039\n",
      "778785\n",
      "760978\n",
      "778753\n",
      "778748\n",
      "778766\n",
      "499283\n",
      "778757\n",
      "710021\n",
      "778785\n",
      "262391\n",
      "745605\n",
      "759121\n",
      "778748\n",
      "641628\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "768381\n",
      "778716\n",
      "778798\n",
      "778748\n",
      "261981\n",
      "778734\n",
      "770591\n",
      "778748\n",
      "778761\n",
      "673142\n",
      "778785\n",
      "778748\n",
      "777492\n",
      "778788\n",
      "778785\n",
      "778702\n",
      "711524\n",
      "774855\n",
      "776570\n",
      "769226\n",
      "512925\n",
      "778776\n",
      "778734\n",
      "778808\n",
      "778734\n",
      "778805\n",
      "778713\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "776751\n",
      "612880\n",
      "778637\n",
      "263197\n",
      "778785\n",
      "778753\n",
      "778752\n",
      "266091\n",
      "266091\n",
      "778748\n",
      "778748\n",
      "778805\n",
      "778785\n",
      "778748\n",
      "778592\n",
      "778785\n",
      "777886\n",
      "778761\n",
      "778703\n",
      "778806\n",
      "263766\n",
      "761047\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "727020\n",
      "778774\n",
      "778785\n",
      "778805\n",
      "778785\n",
      "778761\n",
      "763509\n",
      "778776\n",
      "778748\n",
      "768381\n",
      "778748\n",
      "778776\n",
      "778734\n",
      "758203\n",
      "778703\n",
      "778806\n",
      "778785\n",
      "778766\n",
      "752286\n",
      "778771\n",
      "778726\n",
      "777124\n",
      "264874\n",
      "777159\n",
      "344534\n",
      "266166\n",
      "264995\n",
      "778726\n",
      "747594\n",
      "778785\n",
      "778726\n",
      "778748\n",
      "741692\n",
      "770104\n",
      "778703\n",
      "778734\n",
      "778785\n",
      "778734\n",
      "728893\n",
      "778748\n",
      "533885\n",
      "778766\n",
      "778748\n",
      "778748\n",
      "323567\n",
      "778806\n",
      "778785\n",
      "778785\n",
      "754546\n",
      "778761\n",
      "778243\n",
      "778785\n",
      "778672\n",
      "743386\n",
      "777774\n",
      "266166\n",
      "778703\n",
      "778748\n",
      "768671\n",
      "778748\n",
      "775756\n",
      "707898\n",
      "733610\n",
      "267221\n",
      "757983\n",
      "778748\n",
      "778740\n",
      "778748\n",
      "778710\n",
      "778785\n",
      "778592\n",
      "778785\n",
      "763525\n",
      "778803\n",
      "566217\n",
      "680814\n",
      "778748\n",
      "778630\n",
      "771203\n",
      "778734\n",
      "778748\n",
      "778776\n",
      "778785\n",
      "778712\n",
      "344534\n",
      "778748\n",
      "267346\n",
      "776946\n",
      "762644\n",
      "759664\n",
      "778324\n",
      "778710\n",
      "778748\n",
      "778243\n",
      "619956\n",
      "778748\n",
      "778761\n",
      "778748\n",
      "777886\n",
      "778766\n",
      "778761\n",
      "778761\n",
      "767013\n",
      "778761\n",
      "761369\n",
      "742332\n",
      "776564\n",
      "773649\n",
      "778748\n",
      "778766\n",
      "763509\n",
      "778686\n",
      "778785\n",
      "651903\n",
      "775556\n",
      "778748\n",
      "759664\n",
      "777978\n",
      "759853\n",
      "759853\n",
      "762806\n",
      "778637\n",
      "778761\n",
      "754070\n",
      "754070\n",
      "754070\n",
      "546737\n",
      "651903\n",
      "760340\n",
      "778785\n",
      "762806\n",
      "763509\n",
      "759664\n",
      "763509\n",
      "763509\n",
      "778758\n",
      "778758\n",
      "762806\n",
      "759853\n",
      "763509\n",
      "778740\n",
      "759853\n",
      "762806\n",
      "662193\n",
      "778785\n",
      "778761\n",
      "778761\n",
      "742411\n",
      "759853\n",
      "760340\n",
      "754070\n",
      "778766\n",
      "651903\n",
      "754070\n",
      "778748\n",
      "778702\n",
      "778785\n",
      "778766\n",
      "759664\n",
      "778766\n",
      "754070\n",
      "754070\n",
      "651903\n",
      "662193\n",
      "761525\n",
      "762806\n",
      "773182\n",
      "759664\n",
      "759664\n",
      "274582\n",
      "759664\n",
      "778785\n",
      "651903\n",
      "778678\n",
      "762952\n",
      "763509\n",
      "651903\n",
      "778324\n",
      "754070\n",
      "272524\n",
      "744236\n",
      "277764\n",
      "274582\n",
      "754070\n",
      "762479\n",
      "754070\n",
      "762479\n",
      "754070\n",
      "759853\n",
      "778761\n",
      "759853\n",
      "773182\n",
      "759853\n",
      "619956\n",
      "761345\n",
      "778785\n",
      "771879\n",
      "778766\n",
      "773182\n",
      "763405\n",
      "778751\n",
      "778766\n",
      "778758\n",
      "274821\n",
      "778680\n",
      "778273\n",
      "619956\n",
      "778766\n",
      "744236\n",
      "776946\n",
      "760340\n",
      "763509\n",
      "759664\n",
      "662193\n",
      "762713\n",
      "760340\n",
      "761369\n",
      "778785\n",
      "744690\n",
      "778766\n",
      "778761\n",
      "274953\n",
      "773605\n",
      "778703\n",
      "762224\n",
      "778785\n",
      "778806\n",
      "775583\n",
      "778703\n",
      "773182\n",
      "704223\n",
      "778243\n",
      "778748\n",
      "401885\n",
      "778785\n",
      "275877\n",
      "778668\n",
      "275877\n",
      "776546\n",
      "778273\n",
      "759853\n",
      "761525\n",
      "762806\n",
      "662193\n",
      "778748\n",
      "778723\n",
      "778637\n",
      "562971\n",
      "762806\n",
      "762806\n",
      "744690\n",
      "761369\n",
      "763375\n",
      "759853\n",
      "759664\n",
      "762806\n",
      "776564\n",
      "778744\n",
      "778703\n",
      "778703\n",
      "662193\n",
      "759664\n",
      "760340\n",
      "662193\n",
      "777888\n",
      "656935\n",
      "761525\n",
      "285733\n",
      "778726\n",
      "655445\n",
      "778748\n",
      "778702\n",
      "778703\n",
      "778748\n",
      "778766\n",
      "778751\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "285938\n",
      "778785\n",
      "285733\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "746432\n",
      "778785\n",
      "279336\n",
      "777774\n",
      "778243\n",
      "778810\n",
      "778734\n",
      "778785\n",
      "644110\n",
      "773597\n",
      "776656\n",
      "778785\n",
      "778702\n",
      "655445\n",
      "778785\n",
      "773181\n",
      "778404\n",
      "646517\n",
      "280194\n",
      "778466\n",
      "778002\n",
      "700610\n",
      "759037\n",
      "778748\n",
      "778806\n",
      "470833\n",
      "778726\n",
      "778740\n",
      "778722\n",
      "778785\n",
      "778734\n",
      "778785\n",
      "742368\n",
      "778785\n",
      "778785\n",
      "281193\n",
      "778758\n",
      "769696\n",
      "778702\n",
      "434245\n",
      "642020\n",
      "778785\n",
      "778025\n",
      "778748\n",
      "285733\n",
      "778748\n",
      "778807\n",
      "778785\n",
      "776946\n",
      "281880\n",
      "778080\n",
      "778744\n",
      "778785\n",
      "763074\n",
      "778734\n",
      "700661\n",
      "778761\n",
      "778776\n",
      "778356\n",
      "778785\n",
      "778785\n",
      "750497\n",
      "778761\n",
      "282403\n",
      "282462\n",
      "285281\n",
      "778766\n",
      "776166\n",
      "285733\n",
      "778748\n",
      "285281\n",
      "285733\n",
      "778785\n",
      "776317\n",
      "773597\n",
      "759909\n",
      "778766\n",
      "283314\n",
      "285733\n",
      "778748\n",
      "778744\n",
      "778324\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "766732\n",
      "283928\n",
      "778785\n",
      "759151\n",
      "778785\n",
      "778785\n",
      "776838\n",
      "655445\n",
      "778785\n",
      "655445\n",
      "778716\n",
      "775095\n",
      "778785\n",
      "735427\n",
      "777020\n",
      "628334\n",
      "778785\n",
      "285281\n",
      "778785\n",
      "721276\n",
      "778740\n",
      "778766\n",
      "285042\n",
      "530898\n",
      "285170\n",
      "778785\n",
      "285281\n",
      "778785\n",
      "742576\n",
      "778785\n",
      "747779\n",
      "759118\n",
      "775495\n",
      "778776\n",
      "699728\n",
      "778785\n",
      "285853\n",
      "778785\n",
      "778766\n",
      "752138\n",
      "712579\n",
      "744781\n",
      "778748\n",
      "744677\n",
      "286282\n",
      "778324\n",
      "778785\n",
      "286429\n",
      "778273\n",
      "742368\n",
      "741048\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "286808\n",
      "760737\n",
      "778609\n",
      "760737\n",
      "759058\n",
      "778637\n",
      "778748\n",
      "676933\n",
      "778418\n",
      "778785\n",
      "505731\n",
      "778766\n",
      "778785\n",
      "778726\n",
      "751728\n",
      "778785\n",
      "729333\n",
      "778785\n",
      "778744\n",
      "729091\n",
      "288033\n",
      "778785\n",
      "778785\n",
      "762479\n",
      "778703\n",
      "775854\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "778806\n",
      "778805\n",
      "778774\n",
      "778774\n",
      "778712\n",
      "778761\n",
      "777674\n",
      "778785\n",
      "778726\n",
      "778785\n",
      "777620\n",
      "778806\n",
      "778806\n",
      "778761\n",
      "778806\n",
      "778761\n",
      "778660\n",
      "778807\n",
      "778785\n",
      "778210\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "762479\n",
      "778748\n",
      "778748\n",
      "778807\n",
      "762479\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "534980\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778805\n",
      "778806\n",
      "778748\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "775756\n",
      "290658\n",
      "778750\n",
      "778785\n",
      "778785\n",
      "778504\n",
      "778761\n",
      "778744\n",
      "778761\n",
      "778785\n",
      "778758\n",
      "778766\n",
      "778809\n",
      "761363\n",
      "778740\n",
      "778761\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778660\n",
      "778785\n",
      "778807\n",
      "778761\n",
      "778766\n",
      "778805\n",
      "778761\n",
      "778785\n",
      "778210\n",
      "778785\n",
      "778785\n",
      "763509\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778751\n",
      "778785\n",
      "778504\n",
      "778785\n",
      "741981\n",
      "778785\n",
      "768422\n",
      "544595\n",
      "778204\n",
      "772859\n",
      "776946\n",
      "778763\n",
      "778243\n",
      "753591\n",
      "778785\n",
      "777594\n",
      "771467\n",
      "728734\n",
      "762479\n",
      "778785\n",
      "778785\n",
      "776946\n",
      "778785\n",
      "778766\n",
      "761363\n",
      "778785\n",
      "777789\n",
      "778726\n",
      "741981\n",
      "773099\n",
      "771537\n",
      "778785\n",
      "763509\n",
      "763509\n",
      "716933\n",
      "778785\n",
      "776946\n",
      "755573\n",
      "778210\n",
      "778785\n",
      "778766\n",
      "778748\n",
      "778785\n",
      "778751\n",
      "775882\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778744\n",
      "778785\n",
      "778147\n",
      "778210\n",
      "778084\n",
      "778210\n",
      "778758\n",
      "776838\n",
      "761363\n",
      "778504\n",
      "778748\n",
      "778776\n",
      "762479\n",
      "534689\n",
      "778726\n",
      "778703\n",
      "763509\n",
      "778761\n",
      "778504\n",
      "761363\n",
      "534689\n",
      "778785\n",
      "763509\n",
      "534689\n",
      "776123\n",
      "761363\n",
      "534980\n",
      "729414\n",
      "772990\n",
      "763509\n",
      "534980\n",
      "778766\n",
      "542081\n",
      "297428\n",
      "778774\n",
      "778734\n",
      "775583\n",
      "297710\n",
      "778766\n",
      "760737\n",
      "760685\n",
      "778785\n",
      "778785\n",
      "737132\n",
      "742411\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778703\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "778748\n",
      "301537\n",
      "778785\n",
      "727355\n",
      "778748\n",
      "778748\n",
      "736379\n",
      "778748\n",
      "778785\n",
      "685891\n",
      "298936\n",
      "778785\n",
      "778734\n",
      "491482\n",
      "778748\n",
      "778637\n",
      "778761\n",
      "728063\n",
      "777049\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "778722\n",
      "778748\n",
      "307474\n",
      "778785\n",
      "753298\n",
      "763215\n",
      "778766\n",
      "491482\n",
      "300354\n",
      "778810\n",
      "768184\n",
      "771487\n",
      "778748\n",
      "778785\n",
      "491482\n",
      "300922\n",
      "778082\n",
      "300993\n",
      "778785\n",
      "592753\n",
      "778734\n",
      "301185\n",
      "677811\n",
      "778785\n",
      "778761\n",
      "778703\n",
      "778785\n",
      "777760\n",
      "778805\n",
      "778637\n",
      "417170\n",
      "301676\n",
      "464020\n",
      "778810\n",
      "778748\n",
      "778766\n",
      "678863\n",
      "309605\n",
      "778785\n",
      "778748\n",
      "309436\n",
      "778766\n",
      "778785\n",
      "778744\n",
      "778628\n",
      "778734\n",
      "491482\n",
      "742862\n",
      "309605\n",
      "778748\n",
      "778785\n",
      "778734\n",
      "778785\n",
      "778766\n",
      "778776\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "755165\n",
      "778776\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "746440\n",
      "778273\n",
      "309605\n",
      "778785\n",
      "778726\n",
      "305699\n",
      "778748\n",
      "778785\n",
      "778329\n",
      "778748\n",
      "778199\n",
      "764760\n",
      "778748\n",
      "778748\n",
      "778740\n",
      "778734\n",
      "778761\n",
      "778702\n",
      "305680\n",
      "778785\n",
      "778663\n",
      "776946\n",
      "778761\n",
      "491482\n",
      "778734\n",
      "671281\n",
      "751035\n",
      "312979\n",
      "778766\n",
      "778766\n",
      "491482\n",
      "559931\n",
      "778740\n",
      "778748\n",
      "778761\n",
      "778766\n",
      "778785\n",
      "491482\n",
      "308594\n",
      "778766\n",
      "684704\n",
      "743080\n",
      "778748\n",
      "491482\n",
      "776930\n",
      "309605\n",
      "730490\n",
      "778785\n",
      "335011\n",
      "731194\n",
      "736379\n",
      "778766\n",
      "778785\n",
      "776805\n",
      "778761\n",
      "774393\n",
      "778734\n",
      "778310\n",
      "778734\n",
      "778785\n",
      "642020\n",
      "778748\n",
      "778637\n",
      "769696\n",
      "778748\n",
      "778748\n",
      "503271\n",
      "763375\n",
      "778712\n",
      "778734\n",
      "778785\n",
      "778716\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "743876\n",
      "747720\n",
      "778785\n",
      "763481\n",
      "309622\n",
      "778785\n",
      "778805\n",
      "778766\n",
      "493319\n",
      "534921\n",
      "776946\n",
      "778776\n",
      "778785\n",
      "778637\n",
      "311422\n",
      "778748\n",
      "310345\n",
      "778766\n",
      "762224\n",
      "778722\n",
      "534921\n",
      "778082\n",
      "762806\n",
      "723997\n",
      "778785\n",
      "778637\n",
      "311422\n",
      "738009\n",
      "534921\n",
      "311187\n",
      "709816\n",
      "778748\n",
      "755342\n",
      "735426\n",
      "315685\n",
      "738009\n",
      "753591\n",
      "311422\n",
      "311422\n",
      "311422\n",
      "763505\n",
      "778785\n",
      "311668\n",
      "778273\n",
      "778692\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "739479\n",
      "739479\n",
      "778748\n",
      "778807\n",
      "764739\n",
      "313439\n",
      "778761\n",
      "778761\n",
      "747620\n",
      "776946\n",
      "778766\n",
      "762479\n",
      "312422\n",
      "778761\n",
      "778766\n",
      "751363\n",
      "778785\n",
      "778766\n",
      "421455\n",
      "778761\n",
      "778210\n",
      "763509\n",
      "768834\n",
      "762479\n",
      "312837\n",
      "778748\n",
      "759941\n",
      "778748\n",
      "778766\n",
      "778726\n",
      "778771\n",
      "778748\n",
      "778774\n",
      "778785\n",
      "778761\n",
      "778807\n",
      "778761\n",
      "777049\n",
      "778810\n",
      "778785\n",
      "778785\n",
      "778716\n",
      "778785\n",
      "778766\n",
      "778806\n",
      "778785\n",
      "778785\n",
      "775583\n",
      "777699\n",
      "763173\n",
      "778748\n",
      "778740\n",
      "778652\n",
      "709982\n",
      "667219\n",
      "778798\n",
      "778798\n",
      "778761\n",
      "762806\n",
      "762806\n",
      "778082\n",
      "778766\n",
      "778806\n",
      "759058\n",
      "759058\n",
      "778748\n",
      "778798\n",
      "778785\n",
      "778785\n",
      "778776\n",
      "778785\n",
      "778751\n",
      "778766\n",
      "772575\n",
      "778774\n",
      "763509\n",
      "315685\n",
      "777159\n",
      "778734\n",
      "778723\n",
      "778766\n",
      "778774\n",
      "778785\n",
      "778745\n",
      "778785\n",
      "778766\n",
      "778663\n",
      "778785\n",
      "778806\n",
      "763509\n",
      "756188\n",
      "761047\n",
      "778766\n",
      "778798\n",
      "778798\n",
      "778798\n",
      "763509\n",
      "778748\n",
      "778785\n",
      "778726\n",
      "778785\n",
      "778761\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778806\n",
      "778785\n",
      "759058\n",
      "778809\n",
      "763297\n",
      "778785\n",
      "773605\n",
      "778785\n",
      "778642\n",
      "778726\n",
      "778808\n",
      "778785\n",
      "778776\n",
      "778748\n",
      "667219\n",
      "778748\n",
      "709982\n",
      "778766\n",
      "763509\n",
      "778771\n",
      "778806\n",
      "763509\n",
      "763509\n",
      "762245\n",
      "762806\n",
      "778774\n",
      "778722\n",
      "778785\n",
      "778785\n",
      "396181\n",
      "778806\n",
      "778766\n",
      "778805\n",
      "778761\n",
      "321029\n",
      "777446\n",
      "361948\n",
      "325587\n",
      "742331\n",
      "778776\n",
      "778702\n",
      "778726\n",
      "618265\n",
      "778744\n",
      "778273\n",
      "771837\n",
      "778809\n",
      "778740\n",
      "760962\n",
      "778748\n",
      "778748\n",
      "491790\n",
      "778785\n",
      "771869\n",
      "778740\n",
      "778744\n",
      "778785\n",
      "778748\n",
      "442565\n",
      "324068\n",
      "778761\n",
      "454966\n",
      "778748\n",
      "778702\n",
      "761363\n",
      "778785\n",
      "778744\n",
      "778726\n",
      "321617\n",
      "773529\n",
      "321476\n",
      "778745\n",
      "778744\n",
      "778656\n",
      "778785\n",
      "592710\n",
      "778785\n",
      "778744\n",
      "778785\n",
      "778766\n",
      "778712\n",
      "778810\n",
      "778712\n",
      "542712\n",
      "613750\n",
      "778726\n",
      "778740\n",
      "778656\n",
      "531851\n",
      "775601\n",
      "778726\n",
      "778676\n",
      "777800\n",
      "778807\n",
      "778810\n",
      "778766\n",
      "322527\n",
      "778764\n",
      "766600\n",
      "322625\n",
      "777048\n",
      "778752\n",
      "778273\n",
      "778764\n",
      "322920\n",
      "758130\n",
      "776346\n",
      "778785\n",
      "323142\n",
      "778685\n",
      "768564\n",
      "778785\n",
      "495428\n",
      "778418\n",
      "777791\n",
      "323552\n",
      "323569\n",
      "323614\n",
      "323636\n",
      "778801\n",
      "778740\n",
      "684704\n",
      "778761\n",
      "323948\n",
      "778785\n",
      "778744\n",
      "773708\n",
      "778702\n",
      "778656\n",
      "324257\n",
      "744931\n",
      "724545\n",
      "778785\n",
      "776795\n",
      "778740\n",
      "324734\n",
      "778773\n",
      "778247\n",
      "778247\n",
      "778803\n",
      "621408\n",
      "778785\n",
      "777552\n",
      "778761\n",
      "778744\n",
      "778726\n",
      "583464\n",
      "777347\n",
      "395771\n",
      "326020\n",
      "777643\n",
      "328630\n",
      "778774\n",
      "325654\n",
      "775601\n",
      "778806\n",
      "770039\n",
      "778761\n",
      "325974\n",
      "326236\n",
      "776508\n",
      "755102\n",
      "778761\n",
      "778774\n",
      "778785\n",
      "772437\n",
      "778712\n",
      "777645\n",
      "778785\n",
      "754921\n",
      "762683\n",
      "778220\n",
      "739997\n",
      "326816\n",
      "755150\n",
      "326971\n",
      "778294\n",
      "778273\n",
      "778785\n",
      "729146\n",
      "778220\n",
      "778273\n",
      "778776\n",
      "707049\n",
      "729247\n",
      "327517\n",
      "771869\n",
      "327631\n",
      "778504\n",
      "776508\n",
      "327801\n",
      "778273\n",
      "777552\n",
      "773993\n",
      "778744\n",
      "328106\n",
      "694165\n",
      "754873\n",
      "746559\n",
      "328361\n",
      "400538\n",
      "778744\n",
      "778806\n",
      "776508\n",
      "776590\n",
      "777303\n",
      "600380\n",
      "778809\n",
      "771425\n",
      "776691\n",
      "328967\n",
      "329013\n",
      "778785\n",
      "778766\n",
      "778774\n",
      "336707\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778809\n",
      "778761\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "694671\n",
      "778758\n",
      "778785\n",
      "778785\n",
      "335931\n",
      "778774\n",
      "778748\n",
      "774128\n",
      "778774\n",
      "753591\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778621\n",
      "778806\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778806\n",
      "778785\n",
      "778642\n",
      "778785\n",
      "778748\n",
      "776832\n",
      "778766\n",
      "777511\n",
      "778774\n",
      "778785\n",
      "696611\n",
      "767127\n",
      "778809\n",
      "670278\n",
      "778774\n",
      "778676\n",
      "778785\n",
      "778766\n",
      "778766\n",
      "778776\n",
      "773682\n",
      "778785\n",
      "778748\n",
      "778315\n",
      "336434\n",
      "778785\n",
      "778809\n",
      "744313\n",
      "778785\n",
      "778766\n",
      "777847\n",
      "778761\n",
      "769538\n",
      "778807\n",
      "532069\n",
      "778680\n",
      "761506\n",
      "778595\n",
      "772527\n",
      "778761\n",
      "778785\n",
      "778774\n",
      "778748\n",
      "778761\n",
      "778785\n",
      "778774\n",
      "778716\n",
      "778761\n",
      "778785\n",
      "778801\n",
      "778702\n",
      "778812\n",
      "778785\n",
      "778734\n",
      "778663\n",
      "778785\n",
      "778785\n",
      "769089\n",
      "422432\n",
      "778761\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778726\n",
      "335931\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778726\n",
      "778744\n",
      "778204\n",
      "778748\n",
      "778703\n",
      "778785\n",
      "342893\n",
      "778785\n",
      "422432\n",
      "778748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778702\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778721\n",
      "778785\n",
      "549763\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778734\n",
      "778748\n",
      "778753\n",
      "763673\n",
      "778785\n",
      "778806\n",
      "778785\n",
      "778609\n",
      "778785\n",
      "778783\n",
      "778810\n",
      "778740\n",
      "778806\n",
      "751668\n",
      "778766\n",
      "778785\n",
      "778744\n",
      "776946\n",
      "778766\n",
      "778766\n",
      "778808\n",
      "778712\n",
      "774625\n",
      "778785\n",
      "778761\n",
      "778783\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "335433\n",
      "778808\n",
      "778260\n",
      "778785\n",
      "777283\n",
      "778726\n",
      "778761\n",
      "778761\n",
      "778761\n",
      "778766\n",
      "501733\n",
      "778811\n",
      "778761\n",
      "778806\n",
      "778785\n",
      "778785\n",
      "778744\n",
      "778766\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "573798\n",
      "778785\n",
      "778808\n",
      "778785\n",
      "778734\n",
      "755461\n",
      "778785\n",
      "778744\n",
      "778294\n",
      "775979\n",
      "778744\n",
      "764738\n",
      "778660\n",
      "778785\n",
      "778601\n",
      "778785\n",
      "778785\n",
      "710340\n",
      "734129\n",
      "778748\n",
      "607878\n",
      "778785\n",
      "778702\n",
      "778748\n",
      "778761\n",
      "778807\n",
      "726142\n",
      "778247\n",
      "778766\n",
      "726010\n",
      "778761\n",
      "778805\n",
      "778734\n",
      "739788\n",
      "778771\n",
      "778761\n",
      "778785\n",
      "483829\n",
      "778766\n",
      "776513\n",
      "778761\n",
      "778740\n",
      "778766\n",
      "778766\n",
      "777433\n",
      "778565\n",
      "720157\n",
      "741504\n",
      "778774\n",
      "730426\n",
      "778774\n",
      "771137\n",
      "776946\n",
      "778766\n",
      "730426\n",
      "338940\n",
      "778785\n",
      "778766\n",
      "778766\n",
      "778243\n",
      "778785\n",
      "778776\n",
      "778785\n",
      "751466\n",
      "629230\n",
      "339375\n",
      "752214\n",
      "778783\n",
      "752214\n",
      "778785\n",
      "778785\n",
      "778798\n",
      "778761\n",
      "778204\n",
      "778785\n",
      "339759\n",
      "751180\n",
      "776751\n",
      "755461\n",
      "778785\n",
      "778761\n",
      "778273\n",
      "745612\n",
      "778329\n",
      "538928\n",
      "778748\n",
      "763509\n",
      "778776\n",
      "433925\n",
      "778798\n",
      "610906\n",
      "778766\n",
      "778660\n",
      "763044\n",
      "777888\n",
      "778748\n",
      "613190\n",
      "778418\n",
      "670771\n",
      "777049\n",
      "777586\n",
      "778785\n",
      "778748\n",
      "695983\n",
      "778748\n",
      "741504\n",
      "778785\n",
      "607878\n",
      "773549\n",
      "771341\n",
      "341375\n",
      "778774\n",
      "778774\n",
      "778785\n",
      "778761\n",
      "777632\n",
      "778785\n",
      "341676\n",
      "778766\n",
      "768531\n",
      "759546\n",
      "778785\n",
      "778761\n",
      "778618\n",
      "777452\n",
      "778761\n",
      "342051\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "760978\n",
      "748595\n",
      "778748\n",
      "778744\n",
      "715258\n",
      "778785\n",
      "778785\n",
      "777433\n",
      "776946\n",
      "778766\n",
      "778785\n",
      "778757\n",
      "778785\n",
      "778630\n",
      "776946\n",
      "770619\n",
      "778785\n",
      "778810\n",
      "778766\n",
      "343097\n",
      "343131\n",
      "699004\n",
      "732167\n",
      "732089\n",
      "734031\n",
      "777791\n",
      "778591\n",
      "772006\n",
      "778773\n",
      "778783\n",
      "739875\n",
      "778785\n",
      "778761\n",
      "776946\n",
      "778774\n",
      "778785\n",
      "778771\n",
      "777978\n",
      "776946\n",
      "343933\n",
      "778744\n",
      "778783\n",
      "778753\n",
      "776946\n",
      "753566\n",
      "765281\n",
      "642319\n",
      "776946\n",
      "778766\n",
      "755342\n",
      "631553\n",
      "725992\n",
      "344534\n",
      "778592\n",
      "344618\n",
      "725993\n",
      "344669\n",
      "778748\n",
      "778722\n",
      "778748\n",
      "778798\n",
      "763346\n",
      "778734\n",
      "778753\n",
      "778806\n",
      "778785\n",
      "711938\n",
      "778774\n",
      "344904\n",
      "778761\n",
      "778766\n",
      "778757\n",
      "778680\n",
      "778774\n",
      "777886\n",
      "778748\n",
      "773130\n",
      "778805\n",
      "763509\n",
      "778785\n",
      "778663\n",
      "763509\n",
      "778785\n",
      "778761\n",
      "778273\n",
      "778785\n",
      "778703\n",
      "778773\n",
      "778785\n",
      "778785\n",
      "739341\n",
      "778761\n",
      "763299\n",
      "778637\n",
      "778774\n",
      "778342\n",
      "778712\n",
      "778761\n",
      "778806\n",
      "772684\n",
      "778734\n",
      "778774\n",
      "778785\n",
      "778801\n",
      "778766\n",
      "778740\n",
      "778748\n",
      "778766\n",
      "778773\n",
      "778748\n",
      "778798\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "778809\n",
      "778785\n",
      "778766\n",
      "778766\n",
      "739826\n",
      "778748\n",
      "778771\n",
      "778785\n",
      "678793\n",
      "631301\n",
      "778761\n",
      "778785\n",
      "778811\n",
      "778766\n",
      "763509\n",
      "778761\n",
      "778810\n",
      "778776\n",
      "777968\n",
      "778748\n",
      "346414\n",
      "778766\n",
      "778766\n",
      "778740\n",
      "778748\n",
      "778785\n",
      "778801\n",
      "763452\n",
      "778748\n",
      "778773\n",
      "778806\n",
      "778716\n",
      "778785\n",
      "346661\n",
      "778761\n",
      "776946\n",
      "347345\n",
      "778766\n",
      "778766\n",
      "778748\n",
      "778748\n",
      "778766\n",
      "609827\n",
      "778748\n",
      "778748\n",
      "777105\n",
      "778740\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "347148\n",
      "778766\n",
      "778583\n",
      "778748\n",
      "778785\n",
      "776946\n",
      "777942\n",
      "754070\n",
      "778726\n",
      "778785\n",
      "778412\n",
      "778785\n",
      "778726\n",
      "778766\n",
      "647228\n",
      "778748\n",
      "778766\n",
      "778748\n",
      "762411\n",
      "778748\n",
      "778785\n",
      "778752\n",
      "778785\n",
      "778776\n",
      "778752\n",
      "778785\n",
      "778798\n",
      "711938\n",
      "778752\n",
      "778785\n",
      "778785\n",
      "348450\n",
      "778703\n",
      "778744\n",
      "778785\n",
      "778798\n",
      "348812\n",
      "778734\n",
      "778637\n",
      "759909\n",
      "778748\n",
      "778748\n",
      "778798\n",
      "778785\n",
      "778798\n",
      "778744\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778734\n",
      "778216\n",
      "778757\n",
      "719582\n",
      "778806\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "778726\n",
      "778785\n",
      "778783\n",
      "778783\n",
      "778726\n",
      "762773\n",
      "778785\n",
      "778774\n",
      "778740\n",
      "778806\n",
      "778748\n",
      "778785\n",
      "778262\n",
      "350104\n",
      "777807\n",
      "640989\n",
      "778806\n",
      "778785\n",
      "734607\n",
      "778785\n",
      "643772\n",
      "778785\n",
      "677596\n",
      "778807\n",
      "778785\n",
      "778806\n",
      "778785\n",
      "777374\n",
      "778761\n",
      "776946\n",
      "778761\n",
      "778774\n",
      "778766\n",
      "778783\n",
      "775556\n",
      "778766\n",
      "778785\n",
      "778726\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "778783\n",
      "705932\n",
      "778703\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "778761\n",
      "778621\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778712\n",
      "778785\n",
      "778748\n",
      "778806\n",
      "778726\n",
      "778637\n",
      "643772\n",
      "778810\n",
      "778766\n",
      "778785\n",
      "778766\n",
      "778766\n",
      "778734\n",
      "778761\n",
      "778273\n",
      "778618\n",
      "778766\n",
      "778805\n",
      "778766\n",
      "778618\n",
      "778748\n",
      "778806\n",
      "778785\n",
      "778324\n",
      "778761\n",
      "778812\n",
      "778761\n",
      "778776\n",
      "778774\n",
      "778785\n",
      "778811\n",
      "777807\n",
      "778785\n",
      "778785\n",
      "778712\n",
      "778740\n",
      "778761\n",
      "778771\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778758\n",
      "778785\n",
      "778621\n",
      "778774\n",
      "777374\n",
      "775479\n",
      "778774\n",
      "778785\n",
      "778806\n",
      "778785\n",
      "772767\n",
      "778740\n",
      "778761\n",
      "778810\n",
      "778766\n",
      "778806\n",
      "734481\n",
      "778774\n",
      "778798\n",
      "778798\n",
      "778806\n",
      "354478\n",
      "777113\n",
      "778748\n",
      "778726\n",
      "778785\n",
      "778761\n",
      "778805\n",
      "395705\n",
      "778748\n",
      "637341\n",
      "778771\n",
      "778748\n",
      "778761\n",
      "741009\n",
      "778785\n",
      "697316\n",
      "759664\n",
      "778785\n",
      "778761\n",
      "778806\n",
      "778785\n",
      "354478\n",
      "778677\n",
      "778766\n",
      "778785\n",
      "778716\n",
      "754341\n",
      "778302\n",
      "778785\n",
      "772247\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778766\n",
      "355235\n",
      "778663\n",
      "778764\n",
      "553541\n",
      "355235\n",
      "778785\n",
      "778785\n",
      "778751\n",
      "778723\n",
      "355339\n",
      "778785\n",
      "720399\n",
      "778785\n",
      "778504\n",
      "778766\n",
      "772487\n",
      "718218\n",
      "778740\n",
      "777197\n",
      "507125\n",
      "778748\n",
      "365011\n",
      "734519\n",
      "778637\n",
      "704744\n",
      "777433\n",
      "617006\n",
      "373675\n",
      "742868\n",
      "760645\n",
      "760685\n",
      "776793\n",
      "778745\n",
      "695968\n",
      "606976\n",
      "778630\n",
      "778766\n",
      "778748\n",
      "713841\n",
      "732766\n",
      "416787\n",
      "778637\n",
      "778703\n",
      "778748\n",
      "778748\n",
      "778771\n",
      "630570\n",
      "778702\n",
      "778748\n",
      "778776\n",
      "778785\n",
      "357436\n",
      "778748\n",
      "672898\n",
      "357655\n",
      "778748\n",
      "778810\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "540436\n",
      "778785\n",
      "548291\n",
      "778785\n",
      "777020\n",
      "778785\n",
      "778748\n",
      "772644\n",
      "594155\n",
      "778761\n",
      "771065\n",
      "358391\n",
      "778703\n",
      "778785\n",
      "778734\n",
      "778766\n",
      "778748\n",
      "778806\n",
      "778766\n",
      "778684\n",
      "419485\n",
      "761127\n",
      "732145\n",
      "778766\n",
      "769697\n",
      "778785\n",
      "373675\n",
      "747914\n",
      "760978\n",
      "776946\n",
      "632637\n",
      "778785\n",
      "673196\n",
      "778748\n",
      "711440\n",
      "778748\n",
      "359793\n",
      "778637\n",
      "359793\n",
      "778785\n",
      "359877\n",
      "778766\n",
      "778748\n",
      "778751\n",
      "778785\n",
      "778774\n",
      "778342\n",
      "778785\n",
      "778748\n",
      "775871\n",
      "762773\n",
      "778723\n",
      "778785\n",
      "773186\n",
      "407203\n",
      "705104\n",
      "364977\n",
      "360927\n",
      "424505\n",
      "778578\n",
      "774695\n",
      "747373\n",
      "778748\n",
      "361236\n",
      "763074\n",
      "778748\n",
      "778748\n",
      "778735\n",
      "778785\n",
      "778748\n",
      "361562\n",
      "553129\n",
      "767013\n",
      "361774\n",
      "739997\n",
      "778785\n",
      "775495\n",
      "760687\n",
      "778785\n",
      "769626\n",
      "778748\n",
      "362158\n",
      "778785\n",
      "778751\n",
      "778785\n",
      "622682\n",
      "778409\n",
      "778785\n",
      "764733\n",
      "778748\n",
      "362733\n",
      "763535\n",
      "771259\n",
      "778748\n",
      "778785\n",
      "756094\n",
      "778785\n",
      "741992\n",
      "778785\n",
      "757138\n",
      "365011\n",
      "778785\n",
      "778766\n",
      "552113\n",
      "778774\n",
      "778785\n",
      "548705\n",
      "764676\n",
      "730830\n",
      "507125\n",
      "364209\n",
      "778748\n",
      "677202\n",
      "773117\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "761363\n",
      "755573\n",
      "778580\n",
      "364734\n",
      "778801\n",
      "364847\n",
      "778748\n",
      "643506\n",
      "778748\n",
      "364977\n",
      "365011\n",
      "365089\n",
      "365125\n",
      "778748\n",
      "778766\n",
      "584368\n",
      "778766\n",
      "734519\n",
      "632637\n",
      "778785\n",
      "778766\n",
      "382790\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "765324\n",
      "778734\n",
      "778785\n",
      "778748\n",
      "778210\n",
      "778785\n",
      "778774\n",
      "778761\n",
      "778761\n",
      "778766\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "778204\n",
      "778766\n",
      "746962\n",
      "762806\n",
      "778785\n",
      "778751\n",
      "778703\n",
      "778766\n",
      "778761\n",
      "778785\n",
      "778776\n",
      "778734\n",
      "778761\n",
      "778660\n",
      "777807\n",
      "778785\n",
      "778785\n",
      "778809\n",
      "778785\n",
      "778785\n",
      "709920\n",
      "778785\n",
      "367624\n",
      "752214\n",
      "773163\n",
      "778785\n",
      "773163\n",
      "709920\n",
      "778656\n",
      "778748\n",
      "778771\n",
      "778785\n",
      "778771\n",
      "778774\n",
      "778774\n",
      "778766\n",
      "778744\n",
      "778726\n",
      "778774\n",
      "690723\n",
      "491566\n",
      "719186\n",
      "539695\n",
      "778723\n",
      "778748\n",
      "778734\n",
      "778766\n",
      "778273\n",
      "778748\n",
      "778809\n",
      "778774\n",
      "778734\n",
      "778761\n",
      "778766\n",
      "778748\n",
      "759291\n",
      "690723\n",
      "778761\n",
      "778803\n",
      "369562\n",
      "778412\n",
      "778774\n",
      "778726\n",
      "690723\n",
      "778807\n",
      "759291\n",
      "778726\n",
      "778748\n",
      "690723\n",
      "778761\n",
      "778809\n",
      "778748\n",
      "778734\n",
      "778785\n",
      "778761\n",
      "397368\n",
      "370241\n",
      "778785\n",
      "370295\n",
      "777939\n",
      "710556\n",
      "778740\n",
      "778809\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "778734\n",
      "778063\n",
      "552541\n",
      "778785\n",
      "778785\n",
      "778773\n",
      "778766\n",
      "778774\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "764770\n",
      "778412\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778751\n",
      "778774\n",
      "778785\n",
      "778726\n",
      "778761\n",
      "778774\n",
      "778637\n",
      "372248\n",
      "450060\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "775095\n",
      "778748\n",
      "778723\n",
      "778807\n",
      "778785\n",
      "778712\n",
      "778637\n",
      "775095\n",
      "778785\n",
      "778766\n",
      "778776\n",
      "773417\n",
      "778703\n",
      "778785\n",
      "767361\n",
      "778324\n",
      "372890\n",
      "778703\n",
      "700175\n",
      "778785\n",
      "778748\n",
      "778776\n",
      "778785\n",
      "778703\n",
      "778726\n",
      "778785\n",
      "450060\n",
      "389783\n",
      "773163\n",
      "629262\n",
      "778748\n",
      "778677\n",
      "778761\n",
      "778809\n",
      "373722\n",
      "728655\n",
      "778726\n",
      "678102\n",
      "778703\n",
      "778752\n",
      "778740\n",
      "719582\n",
      "778785\n",
      "717239\n",
      "643929\n",
      "778748\n",
      "778761\n",
      "713969\n",
      "374897\n",
      "375550\n",
      "778723\n",
      "778798\n",
      "778751\n",
      "726012\n",
      "778723\n",
      "778748\n",
      "778734\n",
      "778766\n",
      "778766\n",
      "778726\n",
      "778748\n",
      "778637\n",
      "778147\n",
      "778766\n",
      "778726\n",
      "778761\n",
      "778785\n",
      "778723\n",
      "375550\n",
      "726461\n",
      "542884\n",
      "450060\n",
      "388634\n",
      "778592\n",
      "778776\n",
      "778774\n",
      "778785\n",
      "450060\n",
      "778761\n",
      "778592\n",
      "778766\n",
      "778748\n",
      "778785\n",
      "778710\n",
      "778785\n",
      "778785\n",
      "752656\n",
      "778751\n",
      "761268\n",
      "778785\n",
      "778609\n",
      "745127\n",
      "778776\n",
      "778785\n",
      "376529\n",
      "778766\n",
      "778785\n",
      "778783\n",
      "778722\n",
      "654398\n",
      "759118\n",
      "778748\n",
      "376869\n",
      "778785\n",
      "377074\n",
      "376999\n",
      "742868\n",
      "756524\n",
      "778744\n",
      "778761\n",
      "778753\n",
      "778740\n",
      "778785\n",
      "778637\n",
      "778785\n",
      "778766\n",
      "778807\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778703\n",
      "730329\n",
      "714018\n",
      "778761\n",
      "778785\n",
      "548911\n",
      "778712\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778723\n",
      "778784\n",
      "778785\n",
      "778761\n",
      "778761\n",
      "382587\n",
      "778748\n",
      "778734\n",
      "778748\n",
      "778723\n",
      "778785\n",
      "778761\n",
      "763525\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "762747\n",
      "778785\n",
      "778785\n",
      "382587\n",
      "778785\n",
      "778785\n",
      "705368\n",
      "778703\n",
      "778785\n",
      "778785\n",
      "778751\n",
      "778748\n",
      "778766\n",
      "762747\n",
      "778766\n",
      "382587\n",
      "379970\n",
      "753591\n",
      "778734\n",
      "778726\n",
      "778785\n",
      "778726\n",
      "379970\n",
      "778766\n",
      "778726\n",
      "778785\n",
      "775523\n",
      "777374\n",
      "709920\n",
      "778798\n",
      "778783\n",
      "778761\n",
      "778761\n",
      "778785\n",
      "709920\n",
      "762806\n",
      "778785\n",
      "778766\n",
      "778771\n",
      "778807\n",
      "778771\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778771\n",
      "625103\n",
      "778726\n",
      "778766\n",
      "778748\n",
      "778785\n",
      "778761\n",
      "778761\n",
      "778798\n",
      "778766\n",
      "778761\n",
      "710025\n",
      "778771\n",
      "778785\n",
      "778785\n",
      "778637\n",
      "722655\n",
      "778726\n",
      "778785\n",
      "753458\n",
      "742172\n",
      "778785\n",
      "710025\n",
      "778204\n",
      "778748\n",
      "778740\n",
      "778785\n",
      "778771\n",
      "776946\n",
      "709920\n",
      "778637\n",
      "778748\n",
      "778766\n",
      "382503\n",
      "777552\n",
      "382587\n",
      "778785\n",
      "778748\n",
      "778766\n",
      "778771\n",
      "709920\n",
      "778785\n",
      "778744\n",
      "778637\n",
      "778766\n",
      "778703\n",
      "383439\n",
      "383848\n",
      "775495\n",
      "778806\n",
      "778748\n",
      "383439\n",
      "383848\n",
      "778776\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "383848\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "771446\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "762806\n",
      "778785\n",
      "778785\n",
      "763452\n",
      "442758\n",
      "442758\n",
      "778703\n",
      "778776\n",
      "778798\n",
      "778785\n",
      "778712\n",
      "778785\n",
      "778734\n",
      "778771\n",
      "778750\n",
      "778771\n",
      "778273\n",
      "778785\n",
      "775742\n",
      "778748\n",
      "778785\n",
      "651763\n",
      "777374\n",
      "778412\n",
      "778412\n",
      "778412\n",
      "778273\n",
      "778785\n",
      "778806\n",
      "778785\n",
      "778766\n",
      "778637\n",
      "390120\n",
      "778726\n",
      "778774\n",
      "778774\n",
      "778761\n",
      "778785\n",
      "778726\n",
      "778726\n",
      "777699\n",
      "777968\n",
      "778785\n",
      "778744\n",
      "778766\n",
      "778273\n",
      "778774\n",
      "778774\n",
      "778785\n",
      "778652\n",
      "778172\n",
      "778774\n",
      "778663\n",
      "778637\n",
      "778703\n",
      "778785\n",
      "778785\n",
      "778663\n",
      "591952\n",
      "778681\n",
      "778726\n",
      "778412\n",
      "778663\n",
      "778637\n",
      "778703\n",
      "778703\n",
      "778748\n",
      "778652\n",
      "386852\n",
      "778785\n",
      "778785\n",
      "709125\n",
      "390162\n",
      "778785\n",
      "536985\n",
      "575009\n",
      "778766\n",
      "778785\n",
      "387625\n",
      "778723\n",
      "387504\n",
      "778703\n",
      "778703\n",
      "390162\n",
      "778785\n",
      "778748\n",
      "387840\n",
      "778808\n",
      "388634\n",
      "778785\n",
      "388112\n",
      "388145\n",
      "777137\n",
      "778723\n",
      "388410\n",
      "687794\n",
      "388634\n",
      "542884\n",
      "778637\n",
      "388634\n",
      "778785\n",
      "778663\n",
      "778785\n",
      "769463\n",
      "778785\n",
      "778324\n",
      "778774\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "763103\n",
      "778723\n",
      "723789\n",
      "778785\n",
      "776793\n",
      "511937\n",
      "778785\n",
      "778785\n",
      "778783\n",
      "778748\n",
      "778723\n",
      "778748\n",
      "778726\n",
      "778726\n",
      "778748\n",
      "390162\n",
      "778734\n",
      "778785\n",
      "776946\n",
      "389783\n",
      "778785\n",
      "763299\n",
      "778785\n",
      "390241\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "390120\n",
      "778785\n",
      "390241\n",
      "778785\n",
      "778773\n",
      "778785\n",
      "763509\n",
      "763509\n",
      "778758\n",
      "778775\n",
      "775850\n",
      "778761\n",
      "778785\n",
      "778766\n",
      "778766\n",
      "778806\n",
      "778748\n",
      "778766\n",
      "778771\n",
      "396226\n",
      "774313\n",
      "778748\n",
      "760673\n",
      "760699\n",
      "392107\n",
      "778748\n",
      "778748\n",
      "541223\n",
      "778734\n",
      "778785\n",
      "775850\n",
      "715231\n",
      "778766\n",
      "750222\n",
      "778785\n",
      "778761\n",
      "778734\n",
      "778766\n",
      "778809\n",
      "711747\n",
      "778748\n",
      "776946\n",
      "778761\n",
      "778785\n",
      "778766\n",
      "395272\n",
      "778785\n",
      "778712\n",
      "778748\n",
      "778748\n",
      "775583\n",
      "778771\n",
      "778740\n",
      "778710\n",
      "778776\n",
      "778748\n",
      "778748\n",
      "776495\n",
      "778785\n",
      "778766\n",
      "778766\n",
      "778785\n",
      "778768\n",
      "771188\n",
      "778774\n",
      "778663\n",
      "778810\n",
      "778766\n",
      "778734\n",
      "778752\n",
      "600380\n",
      "778785\n",
      "778766\n",
      "775850\n",
      "396226\n",
      "778785\n",
      "407392\n",
      "751858\n",
      "720793\n",
      "778785\n",
      "770971\n",
      "777966\n",
      "778785\n",
      "778748\n",
      "762952\n",
      "775850\n",
      "778761\n",
      "778776\n",
      "778785\n",
      "778806\n",
      "778809\n",
      "778663\n",
      "760737\n",
      "778785\n",
      "778785\n",
      "711747\n",
      "778761\n",
      "778809\n",
      "778785\n",
      "778748\n",
      "778761\n",
      "776946\n",
      "778766\n",
      "778716\n",
      "778660\n",
      "778785\n",
      "778726\n",
      "778761\n",
      "778726\n",
      "778785\n",
      "778751\n",
      "778775\n",
      "778748\n",
      "775583\n",
      "778748\n",
      "778766\n",
      "751858\n",
      "778766\n",
      "396261\n",
      "778730\n",
      "778660\n",
      "776946\n",
      "775850\n",
      "778761\n",
      "778767\n",
      "778785\n",
      "778766\n",
      "778806\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "394870\n",
      "778785\n",
      "778771\n",
      "778761\n",
      "778702\n",
      "769626\n",
      "778785\n",
      "778730\n",
      "778748\n",
      "720793\n",
      "778785\n",
      "637931\n",
      "778798\n",
      "778766\n",
      "778761\n",
      "778785\n",
      "778753\n",
      "778744\n",
      "778766\n",
      "712659\n",
      "778726\n",
      "778785\n",
      "778774\n",
      "778766\n",
      "778776\n",
      "778726\n",
      "778761\n",
      "778751\n",
      "778744\n",
      "778712\n",
      "776946\n",
      "658462\n",
      "778748\n",
      "778766\n",
      "778785\n",
      "777853\n",
      "778292\n",
      "778748\n",
      "778663\n",
      "778785\n",
      "778766\n",
      "778734\n",
      "778026\n",
      "778703\n",
      "778785\n",
      "778766\n",
      "778763\n",
      "778785\n",
      "778785\n",
      "778703\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "400606\n",
      "763102\n",
      "778785\n",
      "778774\n",
      "778785\n",
      "778785\n",
      "396820\n",
      "778785\n",
      "777114\n",
      "778785\n",
      "778504\n",
      "778761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397026\n",
      "778766\n",
      "778785\n",
      "778771\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778784\n",
      "778734\n",
      "534993\n",
      "778771\n",
      "778771\n",
      "778726\n",
      "778766\n",
      "778761\n",
      "778748\n",
      "778753\n",
      "778785\n",
      "778750\n",
      "778785\n",
      "399320\n",
      "778785\n",
      "400480\n",
      "399320\n",
      "778771\n",
      "778785\n",
      "762479\n",
      "778726\n",
      "778763\n",
      "778785\n",
      "778785\n",
      "401615\n",
      "778734\n",
      "535944\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778744\n",
      "778761\n",
      "778785\n",
      "778774\n",
      "777374\n",
      "778776\n",
      "778785\n",
      "778805\n",
      "777886\n",
      "778748\n",
      "777940\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778806\n",
      "778766\n",
      "778798\n",
      "778785\n",
      "699893\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778703\n",
      "778748\n",
      "778785\n",
      "778806\n",
      "778774\n",
      "778766\n",
      "778776\n",
      "778766\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "399894\n",
      "778776\n",
      "778785\n",
      "778785\n",
      "762479\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "761525\n",
      "778761\n",
      "778808\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778766\n",
      "778748\n",
      "778703\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "740633\n",
      "778761\n",
      "778703\n",
      "778761\n",
      "778766\n",
      "775379\n",
      "778766\n",
      "535944\n",
      "778761\n",
      "778774\n",
      "778785\n",
      "778748\n",
      "778774\n",
      "761525\n",
      "778740\n",
      "778748\n",
      "778811\n",
      "778740\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778761\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "701904\n",
      "778748\n",
      "778785\n",
      "778774\n",
      "778771\n",
      "778766\n",
      "736084\n",
      "778785\n",
      "754348\n",
      "778785\n",
      "778752\n",
      "778740\n",
      "778703\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778766\n",
      "778785\n",
      "778773\n",
      "778785\n",
      "778785\n",
      "778734\n",
      "402998\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "770550\n",
      "778785\n",
      "778712\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "746082\n",
      "778637\n",
      "778748\n",
      "762411\n",
      "760737\n",
      "459094\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "656072\n",
      "405343\n",
      "778785\n",
      "778702\n",
      "410336\n",
      "778748\n",
      "778748\n",
      "753689\n",
      "778748\n",
      "778748\n",
      "778748\n",
      "755427\n",
      "489652\n",
      "778734\n",
      "404428\n",
      "747158\n",
      "778247\n",
      "775583\n",
      "404637\n",
      "778785\n",
      "711592\n",
      "410557\n",
      "407093\n",
      "778785\n",
      "410557\n",
      "778785\n",
      "778748\n",
      "778766\n",
      "778766\n",
      "776946\n",
      "656072\n",
      "472153\n",
      "778592\n",
      "778703\n",
      "778785\n",
      "778726\n",
      "723527\n",
      "776946\n",
      "778785\n",
      "755342\n",
      "409905\n",
      "651905\n",
      "405493\n",
      "410557\n",
      "778761\n",
      "778809\n",
      "411074\n",
      "410557\n",
      "778773\n",
      "778761\n",
      "742411\n",
      "762806\n",
      "778748\n",
      "778734\n",
      "735426\n",
      "778776\n",
      "763375\n",
      "753307\n",
      "651905\n",
      "778785\n",
      "762806\n",
      "778748\n",
      "776591\n",
      "659312\n",
      "778785\n",
      "751437\n",
      "778785\n",
      "778766\n",
      "778748\n",
      "726012\n",
      "778766\n",
      "778711\n",
      "778418\n",
      "540251\n",
      "778785\n",
      "778748\n",
      "778680\n",
      "778748\n",
      "427960\n",
      "448333\n",
      "651905\n",
      "763102\n",
      "762806\n",
      "735426\n",
      "407375\n",
      "777302\n",
      "778770\n",
      "778356\n",
      "778811\n",
      "778748\n",
      "651905\n",
      "778776\n",
      "773597\n",
      "778740\n",
      "778810\n",
      "704312\n",
      "763509\n",
      "778740\n",
      "778810\n",
      "776317\n",
      "778734\n",
      "776946\n",
      "778766\n",
      "778273\n",
      "778774\n",
      "778785\n",
      "775245\n",
      "552166\n",
      "647228\n",
      "778752\n",
      "778703\n",
      "778380\n",
      "778758\n",
      "778785\n",
      "762092\n",
      "775583\n",
      "778726\n",
      "778785\n",
      "778785\n",
      "778771\n",
      "778726\n",
      "773307\n",
      "651905\n",
      "760925\n",
      "778766\n",
      "778637\n",
      "742411\n",
      "778748\n",
      "651905\n",
      "778734\n",
      "778726\n",
      "410557\n",
      "776263\n",
      "777374\n",
      "778785\n",
      "777473\n",
      "411074\n",
      "778726\n",
      "776946\n",
      "778785\n",
      "778766\n",
      "409708\n",
      "472153\n",
      "778761\n",
      "778618\n",
      "778776\n",
      "777807\n",
      "775093\n",
      "778740\n",
      "552166\n",
      "778796\n",
      "778748\n",
      "778766\n",
      "778656\n",
      "778806\n",
      "552166\n",
      "778663\n",
      "778785\n",
      "778785\n",
      "778621\n",
      "778761\n",
      "778761\n",
      "778774\n",
      "778761\n",
      "778723\n",
      "778774\n",
      "552166\n",
      "778740\n",
      "746258\n",
      "778732\n",
      "735426\n",
      "411011\n",
      "778774\n",
      "778766\n",
      "776946\n",
      "778726\n",
      "416547\n",
      "594005\n",
      "743179\n",
      "759058\n",
      "751537\n",
      "778785\n",
      "778785\n",
      "777774\n",
      "778785\n",
      "778748\n",
      "778751\n",
      "417390\n",
      "778785\n",
      "411878\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "777844\n",
      "778761\n",
      "778757\n",
      "778748\n",
      "778785\n",
      "778702\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "700285\n",
      "778785\n",
      "778703\n",
      "778785\n",
      "778761\n",
      "773197\n",
      "778723\n",
      "629883\n",
      "746440\n",
      "778761\n",
      "778637\n",
      "778748\n",
      "778808\n",
      "777049\n",
      "778748\n",
      "778785\n",
      "778808\n",
      "778785\n",
      "778748\n",
      "709188\n",
      "778734\n",
      "778748\n",
      "778748\n",
      "778771\n",
      "778618\n",
      "778748\n",
      "777515\n",
      "778785\n",
      "778785\n",
      "778063\n",
      "778744\n",
      "778766\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "417170\n",
      "778785\n",
      "778785\n",
      "690264\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "413818\n",
      "778748\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "778744\n",
      "778748\n",
      "778798\n",
      "769696\n",
      "778734\n",
      "778785\n",
      "769628\n",
      "769696\n",
      "778785\n",
      "778751\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "776946\n",
      "639152\n",
      "778785\n",
      "775495\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "777374\n",
      "778807\n",
      "778761\n",
      "778806\n",
      "777968\n",
      "741583\n",
      "745342\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "775495\n",
      "778761\n",
      "778726\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "778740\n",
      "778740\n",
      "417431\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "775495\n",
      "417170\n",
      "417390\n",
      "417431\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "776691\n",
      "778663\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "776827\n",
      "778777\n",
      "417170\n",
      "767738\n",
      "778806\n",
      "484235\n",
      "778785\n",
      "416573\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "732888\n",
      "778785\n",
      "778204\n",
      "778785\n",
      "778785\n",
      "742868\n",
      "778785\n",
      "778807\n",
      "778807\n",
      "778807\n",
      "778785\n",
      "778785\n",
      "743179\n",
      "776832\n",
      "776832\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778740\n",
      "778785\n",
      "751363\n",
      "707898\n",
      "778785\n",
      "718107\n",
      "778785\n",
      "778785\n",
      "417831\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778758\n",
      "778766\n",
      "778810\n",
      "778734\n",
      "778773\n",
      "778785\n",
      "778785\n",
      "750613\n",
      "778703\n",
      "778806\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "778744\n",
      "778726\n",
      "778748\n",
      "778726\n",
      "778785\n",
      "778773\n",
      "778758\n",
      "778748\n",
      "778748\n",
      "778774\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778766\n",
      "778744\n",
      "419892\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778734\n",
      "778798\n",
      "778785\n",
      "726254\n",
      "739223\n",
      "778751\n",
      "778776\n",
      "778783\n",
      "778748\n",
      "778785\n",
      "610092\n",
      "778761\n",
      "778740\n",
      "778785\n",
      "778761\n",
      "778809\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "553541\n",
      "778785\n",
      "778785\n",
      "778774\n",
      "778766\n",
      "610092\n",
      "421383\n",
      "776805\n",
      "778776\n",
      "759118\n",
      "422176\n",
      "778776\n",
      "778785\n",
      "778740\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "695983\n",
      "747938\n",
      "778806\n",
      "422175\n",
      "422176\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "737677\n",
      "778776\n",
      "778771\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778740\n",
      "778753\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "778761\n",
      "778773\n",
      "778703\n",
      "738009\n",
      "763173\n",
      "778785\n",
      "738009\n",
      "778785\n",
      "778726\n",
      "778761\n",
      "778785\n",
      "700661\n",
      "778761\n",
      "778703\n",
      "778776\n",
      "778761\n",
      "778637\n",
      "775495\n",
      "778785\n",
      "778203\n",
      "635911\n",
      "778734\n",
      "778734\n",
      "534326\n",
      "778776\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "778748\n",
      "778801\n",
      "763102\n",
      "778776\n",
      "778748\n",
      "778761\n",
      "778663\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "738009\n",
      "755481\n",
      "778766\n",
      "778703\n",
      "778766\n",
      "778785\n",
      "778748\n",
      "747594\n",
      "778805\n",
      "778726\n",
      "778785\n",
      "735790\n",
      "778776\n",
      "778801\n",
      "778774\n",
      "778774\n",
      "762421\n",
      "777969\n",
      "778766\n",
      "778785\n",
      "778703\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778776\n",
      "776946\n",
      "778734\n",
      "778637\n",
      "769401\n",
      "481149\n",
      "778748\n",
      "778748\n",
      "777777\n",
      "778761\n",
      "778668\n",
      "778251\n",
      "428346\n",
      "778744\n",
      "778734\n",
      "778748\n",
      "778251\n",
      "778084\n",
      "778785\n",
      "778748\n",
      "776118\n",
      "778748\n",
      "776946\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "715036\n",
      "429184\n",
      "778748\n",
      "778785\n",
      "776946\n",
      "771537\n",
      "778785\n",
      "778766\n",
      "778734\n",
      "760340\n",
      "778761\n",
      "778761\n",
      "721321\n",
      "778761\n",
      "775583\n",
      "778785\n",
      "778504\n",
      "778702\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "430435\n",
      "778210\n",
      "778798\n",
      "778685\n",
      "778775\n",
      "777620\n",
      "775495\n",
      "778761\n",
      "778785\n",
      "776946\n",
      "778748\n",
      "698159\n",
      "778761\n",
      "438701\n",
      "778785\n",
      "778628\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "778380\n",
      "778785\n",
      "778740\n",
      "778748\n",
      "739491\n",
      "684714\n",
      "777800\n",
      "656935\n",
      "778748\n",
      "778785\n",
      "778734\n",
      "778785\n",
      "778734\n",
      "778748\n",
      "778273\n",
      "778734\n",
      "761282\n",
      "778726\n",
      "778785\n",
      "778763\n",
      "777374\n",
      "651170\n",
      "538404\n",
      "775495\n",
      "778785\n",
      "778785\n",
      "432180\n",
      "778726\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778676\n",
      "729801\n",
      "760328\n",
      "742868\n",
      "775495\n",
      "778273\n",
      "778785\n",
      "778785\n",
      "778798\n",
      "432923\n",
      "778785\n",
      "432975\n",
      "778740\n",
      "776946\n",
      "778766\n",
      "778722\n",
      "776946\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "775583\n",
      "778273\n",
      "778726\n",
      "778726\n",
      "778774\n",
      "433733\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778748\n",
      "746962\n",
      "778785\n",
      "434035\n",
      "778273\n",
      "778773\n",
      "778702\n",
      "778785\n",
      "753073\n",
      "434522\n",
      "778703\n",
      "672229\n",
      "684714\n",
      "778748\n",
      "753456\n",
      "778785\n",
      "778785\n",
      "739341\n",
      "778734\n",
      "778785\n",
      "435634\n",
      "778243\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "656935\n",
      "728655\n",
      "775583\n",
      "684892\n",
      "778785\n",
      "615536\n",
      "753645\n",
      "778726\n",
      "778785\n",
      "778777\n",
      "778777\n",
      "778748\n",
      "778726\n",
      "778785\n",
      "778785\n",
      "778204\n",
      "778204\n",
      "776946\n",
      "710478\n",
      "778344\n",
      "776946\n",
      "778761\n",
      "656935\n",
      "778292\n",
      "778806\n",
      "778766\n",
      "778798\n",
      "778748\n",
      "778734\n",
      "778748\n",
      "778773\n",
      "778785\n",
      "778806\n",
      "443027\n",
      "443027\n",
      "778776\n",
      "778774\n",
      "741981\n",
      "778761\n",
      "751138\n",
      "778734\n",
      "443666\n",
      "778807\n",
      "778740\n",
      "778621\n",
      "778726\n",
      "778748\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "443198\n",
      "778734\n",
      "778785\n",
      "778806\n",
      "763020\n",
      "778785\n",
      "778758\n",
      "778752\n",
      "778761\n",
      "778748\n",
      "778748\n",
      "778776\n",
      "778805\n",
      "778763\n",
      "778809\n",
      "778773\n",
      "778773\n",
      "778785\n",
      "438923\n",
      "778785\n",
      "778785\n",
      "778273\n",
      "741650\n",
      "778663\n",
      "778748\n",
      "778798\n",
      "778785\n",
      "754070\n",
      "778806\n",
      "440390\n",
      "778734\n",
      "778785\n",
      "778785\n",
      "778806\n",
      "778806\n",
      "778785\n",
      "778748\n",
      "741662\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "778761\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778744\n",
      "778273\n",
      "778774\n",
      "776946\n",
      "778785\n",
      "778273\n",
      "778726\n",
      "778774\n",
      "778785\n",
      "778785\n",
      "778806\n",
      "778798\n",
      "778773\n",
      "778785\n",
      "778761\n",
      "778761\n",
      "778691\n",
      "778748\n",
      "778748\n",
      "776946\n",
      "778726\n",
      "685891\n",
      "778798\n",
      "778748\n",
      "539361\n",
      "778785\n",
      "778783\n",
      "777847\n",
      "778785\n",
      "778805\n",
      "775583\n",
      "778748\n",
      "443666\n",
      "778748\n",
      "778761\n",
      "778748\n",
      "748902\n",
      "778785\n",
      "778766\n",
      "778740\n",
      "778809\n",
      "773130\n",
      "778785\n",
      "778243\n",
      "778785\n",
      "539219\n",
      "744074\n",
      "778785\n",
      "778785\n",
      "777705\n",
      "778785\n",
      "778761\n",
      "778726\n",
      "778785\n",
      "778806\n",
      "778785\n",
      "778785\n",
      "778776\n",
      "778766\n",
      "778726\n",
      "778652\n",
      "778761\n",
      "778761\n",
      "443198\n",
      "778761\n",
      "778734\n",
      "778652\n",
      "778702\n",
      "778702\n",
      "778806\n",
      "778243\n",
      "776946\n",
      "778785\n",
      "778785\n",
      "778761\n",
      "778761\n",
      "778806\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778726\n",
      "778785\n",
      "778748\n",
      "778785\n",
      "650948\n",
      "778273\n",
      "778776\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778774\n",
      "778740\n",
      "778785\n",
      "778752\n",
      "778766\n",
      "778806\n",
      "778766\n",
      "778740\n",
      "778726\n",
      "778785\n",
      "778806\n",
      "778663\n",
      "778806\n",
      "756393\n",
      "778712\n",
      "778785\n",
      "778761\n",
      "778734\n",
      "741662\n",
      "778748\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "776815\n",
      "778766\n",
      "778748\n",
      "778785\n",
      "778734\n",
      "452372\n",
      "448381\n",
      "778785\n",
      "451918\n",
      "778409\n",
      "547829\n",
      "778766\n",
      "445300\n",
      "778744\n",
      "547829\n",
      "629317\n",
      "592543\n",
      "778785\n",
      "747482\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "452348\n",
      "740416\n",
      "594155\n",
      "445205\n",
      "778785\n",
      "778807\n",
      "743819\n",
      "778810\n",
      "748974\n",
      "778753\n",
      "778785\n",
      "761272\n",
      "640261\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778766\n",
      "778637\n",
      "778740\n",
      "778726\n",
      "778785\n",
      "778776\n",
      "778748\n",
      "778748\n",
      "762806\n",
      "653356\n",
      "778748\n",
      "777639\n",
      "778785\n",
      "762806\n",
      "763074\n",
      "778801\n",
      "594155\n",
      "755461\n",
      "655800\n",
      "542941\n",
      "778761\n",
      "735790\n",
      "451918\n",
      "778785\n",
      "778663\n",
      "778748\n",
      "778785\n",
      "451918\n",
      "778766\n",
      "778734\n",
      "778806\n",
      "704769\n",
      "778698\n",
      "778637\n",
      "755063\n",
      "778748\n",
      "778766\n",
      "778748\n",
      "778748\n",
      "778738\n",
      "778785\n",
      "496028\n",
      "770829\n",
      "778748\n",
      "778748\n",
      "778503\n",
      "758681\n",
      "778785\n",
      "778810\n",
      "742868\n",
      "447769\n",
      "778734\n",
      "778761\n",
      "778660\n",
      "778740\n",
      "778748\n",
      "630958\n",
      "778766\n",
      "778774\n",
      "778785\n",
      "778785\n",
      "758232\n",
      "778785\n",
      "778748\n",
      "678728\n",
      "778582\n",
      "448413\n",
      "778785\n",
      "778771\n",
      "776805\n",
      "778637\n",
      "749968\n",
      "778748\n",
      "735790\n",
      "730917\n",
      "778748\n",
      "778766\n",
      "449039\n",
      "778748\n",
      "778785\n",
      "778734\n",
      "778734\n",
      "752926\n",
      "778785\n",
      "778785\n",
      "778637\n",
      "778748\n",
      "778329\n",
      "687551\n",
      "449865\n",
      "776691\n",
      "778748\n",
      "690723\n",
      "747933\n",
      "450060\n",
      "450182\n",
      "450277\n",
      "778748\n",
      "766890\n",
      "778748\n",
      "778748\n",
      "744690\n",
      "759901\n",
      "778748\n",
      "778766\n",
      "778026\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "450777\n",
      "778785\n",
      "751955\n",
      "734420\n",
      "778766\n",
      "778766\n",
      "778748\n",
      "778748\n",
      "778734\n",
      "761438\n",
      "778637\n",
      "778785\n",
      "625634\n",
      "778748\n",
      "594155\n",
      "778766\n",
      "547829\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "761438\n",
      "778310\n",
      "778748\n",
      "778744\n",
      "778734\n",
      "778656\n",
      "778748\n",
      "778771\n",
      "778766\n",
      "778766\n",
      "639154\n",
      "778744\n",
      "778766\n",
      "778766\n",
      "452790\n",
      "776946\n",
      "778748\n",
      "778257\n",
      "752214\n",
      "778777\n",
      "778748\n",
      "778766\n",
      "778774\n",
      "778761\n",
      "778761\n",
      "778776\n",
      "778766\n",
      "778761\n",
      "778761\n",
      "778748\n",
      "778785\n",
      "778766\n",
      "778748\n",
      "778748\n",
      "778761\n",
      "778761\n",
      "778748\n",
      "778766\n",
      "778663\n",
      "778776\n",
      "778748\n",
      "778748\n",
      "453857\n",
      "778745\n",
      "776946\n",
      "778785\n",
      "778785\n",
      "454044\n",
      "778785\n",
      "778748\n",
      "778637\n",
      "778748\n",
      "778766\n",
      "778776\n",
      "778785\n",
      "756112\n",
      "778761\n",
      "778748\n",
      "778748\n",
      "778766\n",
      "778785\n",
      "634469\n",
      "775808\n",
      "778748\n",
      "778766\n",
      "778766\n",
      "778766\n",
      "778761\n",
      "778766\n",
      "778740\n",
      "778785\n",
      "778766\n",
      "455215\n",
      "778796\n",
      "456789\n",
      "778801\n",
      "778785\n",
      "778801\n",
      "778766\n",
      "778692\n",
      "778766\n",
      "778810\n",
      "778766\n",
      "778744\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778748\n",
      "778771\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "776946\n",
      "456043\n",
      "778744\n",
      "778785\n",
      "778419\n",
      "778785\n",
      "770408\n",
      "714290\n",
      "778785\n",
      "778766\n",
      "763509\n",
      "776815\n",
      "684032\n",
      "778748\n",
      "730426\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778785\n",
      "456675\n",
      "778774\n",
      "778785\n",
      "778771\n",
      "778723\n",
      "778748\n",
      "717030\n",
      "456953\n",
      "778766\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778783\n",
      "778748\n",
      "457227\n",
      "778774\n",
      "746984\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "777925\n",
      "778785\n",
      "764739\n",
      "778748\n",
      "778774\n",
      "778785\n",
      "778748\n",
      "778723\n",
      "778771\n",
      "778766\n",
      "778748\n",
      "777807\n",
      "778785\n",
      "778785\n",
      "778808\n",
      "750036\n",
      "778766\n",
      "778748\n",
      "762479\n",
      "778766\n",
      "778771\n",
      "778776\n",
      "778785\n",
      "712976\n",
      "778748\n",
      "778766\n",
      "778748\n",
      "778748\n",
      "778785\n",
      "459072\n",
      "577452\n",
      "459064\n",
      "778766\n",
      "712976\n",
      "778771\n",
      "714290\n",
      "778785\n",
      "759853\n",
      "778785\n",
      "778761\n",
      "778663\n",
      "760328\n",
      "763509\n",
      "778748\n",
      "778740\n",
      "648080\n",
      "778771\n",
      "778752\n",
      "778798\n",
      "778766\n",
      "778752\n",
      "778761\n",
      "459979\n",
      "733940\n",
      "776946\n",
      "778785\n",
      "765281\n",
      "778785\n",
      "778766\n",
      "778761\n",
      "778766\n",
      "776946\n",
      "777925\n",
      "778744\n",
      "756211\n",
      "778145\n",
      "699728\n",
      "648540\n",
      "586712\n",
      "763509\n",
      "778766\n",
      "658818\n",
      "777978\n",
      "778609\n",
      "778766\n",
      "778766\n",
      "778785\n",
      "712988\n",
      "776946\n",
      "778807\n",
      "778273\n",
      "778761\n",
      "778785\n",
      "778761\n",
      "778785\n",
      "778766\n",
      "778785\n",
      "778766\n",
      "778809\n",
      "778748\n",
      "778771\n",
      "778621\n",
      "778703\n",
      "778748\n",
      "710171\n",
      "778702\n",
      "461908\n",
      "655644\n",
      "778776\n",
      "778748\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "735319\n",
      "778744\n",
      "778748\n",
      "778766\n",
      "778726\n",
      "778761\n",
      "778766\n",
      "778748\n",
      "778748\n",
      "778758\n",
      "778702\n",
      "713387\n",
      "778748\n",
      "778744\n",
      "731270\n",
      "778785\n",
      "778766\n",
      "763375\n",
      "778702\n",
      "778748\n",
      "778774\n",
      "775583\n",
      "778785\n",
      "778766\n",
      "778744\n",
      "608664\n",
      "778703\n",
      "778702\n",
      "778761\n",
      "778157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753566\n",
      "778785\n",
      "778688\n",
      "778785\n",
      "778726\n",
      "778761\n",
      "778748\n",
      "778703\n",
      "778771\n",
      "778748\n",
      "778806\n",
      "778785\n",
      "778785\n",
      "753934\n",
      "778806\n",
      "778726\n",
      "778726\n",
      "778748\n",
      "777374\n",
      "464041\n",
      "748902\n",
      "778766\n",
      "778663\n",
      "778785\n",
      "778748\n",
      "778748\n",
      "778748\n",
      "754070\n",
      "778785\n",
      "778748\n",
      "778642\n",
      "678979\n",
      "778373\n",
      "778785\n",
      "778785\n",
      "760340\n",
      "778766\n",
      "778776\n",
      "778734\n",
      "778805\n",
      "778766\n",
      "778761\n",
      "778766\n",
      "778761\n",
      "778726\n",
      "778761\n",
      "600380\n",
      "760340\n",
      "778785\n",
      "778785\n",
      "778237\n",
      "762796\n",
      "778748\n",
      "778751\n",
      "778748\n",
      "778734\n",
      "778641\n",
      "778247\n",
      "778766\n",
      "778785\n",
      "778734\n",
      "731270\n",
      "778785\n",
      "778703\n",
      "778785\n",
      "778785\n",
      "778785\n",
      "778734\n",
      "778748\n",
      "646195\n",
      "600380\n",
      "466865\n",
      "778251\n",
      "466853\n",
      "777303\n",
      "778785\n",
      "778734\n",
      "778761\n",
      "778785\n",
      "778734\n",
      "778766\n",
      "466865\n",
      "778783\n",
      "778785\n",
      "762155\n",
      "778766\n",
      "778761\n",
      "778734\n",
      "778726\n",
      "778785\n",
      "778785\n",
      "763509\n",
      "778734\n",
      "778761\n",
      "778172\n",
      "778758\n",
      "778734\n",
      "467242\n",
      "778100\n",
      "778785\n"
     ]
    }
   ],
   "source": [
    "m3_train_labels = []\n",
    "for item in answers_train:\n",
    "    m3_train_labels.append(vocab[item-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9870"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m3_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two',\n",
       " 'of',\n",
       " 'passepartout',\n",
       " 'and',\n",
       " 'on',\n",
       " 'to',\n",
       " 'here',\n",
       " 'four',\n",
       " 'barings',\n",
       " '840',\n",
       " 'five',\n",
       " 'of',\n",
       " 'fogg',\n",
       " 'of',\n",
       " 'her',\n",
       " '700',\n",
       " 'in',\n",
       " 'the',\n",
       " 'to',\n",
       " 'by',\n",
       " 'with',\n",
       " 'the',\n",
       " 'passepartout',\n",
       " 'the',\n",
       " 'of',\n",
       " 'queenstown',\n",
       " 'thousand',\n",
       " '770',\n",
       " 'the',\n",
       " 'hours',\n",
       " 'the',\n",
       " 'will',\n",
       " 'speedy',\n",
       " 'two',\n",
       " 'the',\n",
       " 'a',\n",
       " 'the',\n",
       " 'the',\n",
       " 'chicago',\n",
       " 'omaha',\n",
       " 'a',\n",
       " 'fort',\n",
       " 'three',\n",
       " 'passepartout',\n",
       " 'three',\n",
       " 'and',\n",
       " 'passepartout',\n",
       " 'colonel',\n",
       " 'passepartout',\n",
       " 'the',\n",
       " 'by',\n",
       " 'the',\n",
       " 'a',\n",
       " 'and',\n",
       " 'to',\n",
       " 'to',\n",
       " 'passepartout',\n",
       " 'a',\n",
       " 'colonel',\n",
       " 'fogg',\n",
       " 'how',\n",
       " 'the',\n",
       " 'city',\n",
       " 'ogden',\n",
       " 'the',\n",
       " 'passepartout',\n",
       " 'seven',\n",
       " 'of',\n",
       " 'for',\n",
       " 'fix',\n",
       " 'mountains',\n",
       " 'of',\n",
       " 'fogg',\n",
       " 'fix',\n",
       " 'street',\n",
       " 'for',\n",
       " 'the',\n",
       " 'francisco',\n",
       " '180th',\n",
       " 'day',\n",
       " 'friends',\n",
       " 'fix',\n",
       " 'who',\n",
       " 'the',\n",
       " 'yen',\n",
       " 'batulcar',\n",
       " 'the',\n",
       " 'an',\n",
       " 'and',\n",
       " 'singing',\n",
       " 'of',\n",
       " 'of',\n",
       " 'november',\n",
       " 'of',\n",
       " 'the',\n",
       " 'november',\n",
       " 'the',\n",
       " 'for',\n",
       " 'one',\n",
       " 'and',\n",
       " 'south',\n",
       " 'north',\n",
       " 'to',\n",
       " 'the',\n",
       " 'to',\n",
       " 'john',\n",
       " 'yokohama',\n",
       " 'it',\n",
       " 'the',\n",
       " 'opium',\n",
       " 'what',\n",
       " 'of',\n",
       " 'fix',\n",
       " 'the',\n",
       " 'holland',\n",
       " 'jejeeh',\n",
       " 'the',\n",
       " 'the',\n",
       " 'passepartout',\n",
       " 'the',\n",
       " 'fogg',\n",
       " 'take',\n",
       " 'take',\n",
       " 'both',\n",
       " 'friends',\n",
       " 'singapore',\n",
       " 'to',\n",
       " 'on',\n",
       " 'in',\n",
       " 'blue',\n",
       " 'fogg',\n",
       " 'to',\n",
       " 'the',\n",
       " 'and',\n",
       " 'two',\n",
       " 'the',\n",
       " 'judge',\n",
       " 'a',\n",
       " 'sir',\n",
       " 'the',\n",
       " 'station',\n",
       " 'passepartout',\n",
       " 'the',\n",
       " 'of',\n",
       " 'fogg',\n",
       " 'suicide',\n",
       " 'kiouni',\n",
       " 'miles',\n",
       " 'and',\n",
       " 'sir',\n",
       " 'of',\n",
       " 'of',\n",
       " 'rabbit',\n",
       " 'hours',\n",
       " 'fix',\n",
       " 'of',\n",
       " 'an',\n",
       " 'to',\n",
       " 'fix',\n",
       " 'jean',\n",
       " 'the',\n",
       " 'the',\n",
       " 'of',\n",
       " 'suez',\n",
       " 'the',\n",
       " 'cross',\n",
       " 'pounds',\n",
       " 'p.m',\n",
       " 'the',\n",
       " 'at',\n",
       " 'pounds',\n",
       " 'whist',\n",
       " 'pounds',\n",
       " 'the',\n",
       " '7',\n",
       " 'madame',\n",
       " 'french',\n",
       " 'to',\n",
       " 'the',\n",
       " '1872',\n",
       " 'get',\n",
       " 'a',\n",
       " 'joseph',\n",
       " 'that',\n",
       " 'sharp',\n",
       " 'he',\n",
       " 'the',\n",
       " 'by',\n",
       " 'bohemians',\n",
       " 'he',\n",
       " 'and',\n",
       " 'they',\n",
       " 'becky',\n",
       " 'becky',\n",
       " 'take',\n",
       " 'a',\n",
       " 'germany',\n",
       " 'have',\n",
       " 'richmond',\n",
       " 'mrs.',\n",
       " 'john',\n",
       " 'william',\n",
       " 'john',\n",
       " 'and',\n",
       " 'like',\n",
       " 'a',\n",
       " 'that',\n",
       " 'and',\n",
       " 'the',\n",
       " 'the',\n",
       " 'is',\n",
       " 'joseph',\n",
       " 'grandfather',\n",
       " 'william',\n",
       " 'at',\n",
       " 'and',\n",
       " 'island',\n",
       " 'pitt',\n",
       " 'sharp',\n",
       " 'the',\n",
       " 'friend',\n",
       " 'to',\n",
       " 'to',\n",
       " 'crawley',\n",
       " 'the',\n",
       " 'crawley',\n",
       " 'lady',\n",
       " 'a',\n",
       " 'her',\n",
       " 'charades',\n",
       " 'lady',\n",
       " 'the',\n",
       " 'they',\n",
       " 'the',\n",
       " 'lady',\n",
       " 'at',\n",
       " 'singing',\n",
       " 'sexual',\n",
       " 'lord',\n",
       " 'the',\n",
       " 'and',\n",
       " 'and',\n",
       " 'lady',\n",
       " 'is',\n",
       " 'jane',\n",
       " 'william',\n",
       " 'crawley',\n",
       " 'lady',\n",
       " 'lady',\n",
       " 'the',\n",
       " 'sharp',\n",
       " 'is',\n",
       " 'a',\n",
       " 'william',\n",
       " 'and',\n",
       " 'jane',\n",
       " 'a',\n",
       " 'john',\n",
       " 'in',\n",
       " 'and',\n",
       " 'lady',\n",
       " 'the',\n",
       " 'miss',\n",
       " 'sharp',\n",
       " 'miss',\n",
       " 'sir',\n",
       " 'sir',\n",
       " 'a',\n",
       " 'william',\n",
       " 'william',\n",
       " 'georgy',\n",
       " 'of',\n",
       " 'pitt',\n",
       " 'john',\n",
       " 'friend',\n",
       " 'former',\n",
       " 'does',\n",
       " 'and',\n",
       " 'william',\n",
       " 'and',\n",
       " 'lady',\n",
       " 'sharp',\n",
       " 'lady',\n",
       " 'pitt',\n",
       " 'pitt',\n",
       " 'crawley',\n",
       " 'horses',\n",
       " 'george',\n",
       " 'a',\n",
       " 'joseph',\n",
       " 'joseph',\n",
       " 'at',\n",
       " 'in',\n",
       " 'joseph',\n",
       " 'the',\n",
       " 'all',\n",
       " 'the',\n",
       " 'is',\n",
       " 'high',\n",
       " 'joseph',\n",
       " 'mrs.odowd',\n",
       " 'their',\n",
       " 'and',\n",
       " 'mrs.',\n",
       " 'her',\n",
       " 'the',\n",
       " 'the',\n",
       " 'he',\n",
       " 'does',\n",
       " 'friend',\n",
       " 'the',\n",
       " 'because',\n",
       " 'a',\n",
       " 'one',\n",
       " 'john',\n",
       " 'sister',\n",
       " 'for',\n",
       " 'is',\n",
       " 'and',\n",
       " 'and',\n",
       " 'george',\n",
       " 'a',\n",
       " 'to',\n",
       " 'and',\n",
       " 'is',\n",
       " 'he',\n",
       " 'the',\n",
       " 'amelia',\n",
       " 'a',\n",
       " 'crawley',\n",
       " 'sharp',\n",
       " 'housekeeper',\n",
       " 'and',\n",
       " 'he',\n",
       " 'the',\n",
       " 'osborne',\n",
       " 'not',\n",
       " 'and',\n",
       " 'a',\n",
       " 'brother',\n",
       " 'pitt',\n",
       " 'lady',\n",
       " 'pitt',\n",
       " 'and',\n",
       " 'its',\n",
       " 'amelia',\n",
       " 'and',\n",
       " 'to',\n",
       " 'sharp',\n",
       " 'be',\n",
       " 'figs',\n",
       " 'william',\n",
       " 'of',\n",
       " 'the',\n",
       " 'is',\n",
       " 'brother',\n",
       " 'friends',\n",
       " 'an',\n",
       " 'the',\n",
       " '1850s',\n",
       " 'ferrenby',\n",
       " 'he',\n",
       " 'selfish',\n",
       " 'princeton',\n",
       " 'team',\n",
       " 'of',\n",
       " 'impractical',\n",
       " 'equality',\n",
       " 'struggle',\n",
       " 'idealists',\n",
       " 'out',\n",
       " 'a',\n",
       " 'security',\n",
       " 'darcy',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'the',\n",
       " 'his',\n",
       " 'darcys',\n",
       " 'the',\n",
       " 'rosalinds',\n",
       " 'the',\n",
       " 'stella',\n",
       " 'was',\n",
       " 'a',\n",
       " 'the',\n",
       " 'jill',\n",
       " 'the',\n",
       " 'whispering',\n",
       " 'rosalind',\n",
       " 'his',\n",
       " 'hotel',\n",
       " 'bourbon',\n",
       " 'uncomfortable',\n",
       " 'alec',\n",
       " 'city',\n",
       " 'the',\n",
       " 'years',\n",
       " 'poems',\n",
       " 'they',\n",
       " 'memories',\n",
       " 'his',\n",
       " 'away',\n",
       " 'a',\n",
       " 'go',\n",
       " 'literature',\n",
       " 'into',\n",
       " 'intellectual',\n",
       " 'ramilly',\n",
       " 'france',\n",
       " 'french',\n",
       " 'poetry',\n",
       " 'the',\n",
       " 'walking',\n",
       " 'an',\n",
       " 'mother',\n",
       " 'the',\n",
       " 'bored',\n",
       " 'like',\n",
       " 'a',\n",
       " 'friends',\n",
       " 'a',\n",
       " 'a',\n",
       " 'reading',\n",
       " 'prohibition',\n",
       " 'the',\n",
       " 'tom',\n",
       " 'a',\n",
       " 'fighting',\n",
       " 'place',\n",
       " 'his',\n",
       " 'a',\n",
       " 'for',\n",
       " 'does',\n",
       " 'her',\n",
       " 'weeks',\n",
       " 'he',\n",
       " 'a',\n",
       " 'a',\n",
       " 'eccentric',\n",
       " 'he',\n",
       " 'amory',\n",
       " 'her',\n",
       " 'a',\n",
       " 'connage',\n",
       " 'kiss',\n",
       " 'men',\n",
       " '19',\n",
       " '16',\n",
       " '2',\n",
       " 'holiday',\n",
       " 'poetry',\n",
       " 'new',\n",
       " 'his',\n",
       " 'isabelle',\n",
       " '2',\n",
       " 'to',\n",
       " 'johnson',\n",
       " 'baudelaire',\n",
       " 'tom',\n",
       " 'a',\n",
       " 'a',\n",
       " 'for',\n",
       " 'a',\n",
       " 'will',\n",
       " 'for',\n",
       " 'long',\n",
       " 'darcy',\n",
       " 'when',\n",
       " 'to',\n",
       " 'the',\n",
       " 'a',\n",
       " 'sunday',\n",
       " 'in',\n",
       " 'philadelphia',\n",
       " 'they',\n",
       " 'her',\n",
       " 'darcy',\n",
       " 'writes',\n",
       " 'religious',\n",
       " 'nobody',\n",
       " 'amory',\n",
       " 'from',\n",
       " 'feels',\n",
       " 'burne',\n",
       " 'junior',\n",
       " 'fall',\n",
       " 'to',\n",
       " 'no',\n",
       " 'their',\n",
       " 'his',\n",
       " 'tom',\n",
       " 'he',\n",
       " 'tom',\n",
       " 'a',\n",
       " 'fred',\n",
       " 'new',\n",
       " 'is',\n",
       " 'france',\n",
       " 'a',\n",
       " 'christmas',\n",
       " 'a',\n",
       " 'the',\n",
       " 'house',\n",
       " 'car',\n",
       " 'the',\n",
       " 'they',\n",
       " 'boys',\n",
       " 'isabelle',\n",
       " 'parties',\n",
       " 'on',\n",
       " 'the',\n",
       " 'sophomore',\n",
       " 'tom',\n",
       " 'play',\n",
       " 'writing',\n",
       " 'he',\n",
       " 'football',\n",
       " 'is',\n",
       " 'kerry',\n",
       " 'his',\n",
       " 'princeton',\n",
       " 'new',\n",
       " 'is',\n",
       " 'plays',\n",
       " 'darcy',\n",
       " 'school',\n",
       " 'is',\n",
       " 'lake',\n",
       " 'read',\n",
       " 'froggy',\n",
       " 'myra',\n",
       " 'his',\n",
       " 'minneapolis',\n",
       " 'europe',\n",
       " 'on',\n",
       " 'appendicitis',\n",
       " 'him',\n",
       " 'beatrice',\n",
       " 'blain',\n",
       " 'peaceful',\n",
       " 'he',\n",
       " 'to',\n",
       " 'the',\n",
       " 'farish',\n",
       " 'to',\n",
       " 'the',\n",
       " 'for',\n",
       " 'ill',\n",
       " 'to',\n",
       " 'is',\n",
       " 'satisfied',\n",
       " 'the',\n",
       " 'betray',\n",
       " 'that',\n",
       " 'at',\n",
       " 'embarrassment',\n",
       " 'apologies',\n",
       " 'lawrence',\n",
       " 'will',\n",
       " 'his',\n",
       " 'to',\n",
       " 'she',\n",
       " 'was',\n",
       " 'she',\n",
       " 'to',\n",
       " 'drug',\n",
       " 'is',\n",
       " 'with',\n",
       " 'place',\n",
       " 'his',\n",
       " 'and',\n",
       " 'the',\n",
       " 'unhappy',\n",
       " 'new',\n",
       " 'luxurious',\n",
       " 'is',\n",
       " 'social',\n",
       " 'will',\n",
       " 'for',\n",
       " 'with',\n",
       " 'is',\n",
       " 'is',\n",
       " 'to',\n",
       " 'the',\n",
       " 'friendly',\n",
       " 'his',\n",
       " 'carry',\n",
       " 'the',\n",
       " 'simon',\n",
       " 'with',\n",
       " 'is',\n",
       " 'him',\n",
       " 'to',\n",
       " 'them',\n",
       " 'must',\n",
       " 'she',\n",
       " 'new',\n",
       " 'grateful',\n",
       " 'for',\n",
       " 'immoral',\n",
       " 'repay',\n",
       " 'grace',\n",
       " 'farish',\n",
       " 'shock',\n",
       " 'her',\n",
       " 'to',\n",
       " 'will',\n",
       " 'the',\n",
       " 'and',\n",
       " 'tense',\n",
       " 'to',\n",
       " 'the',\n",
       " 'to',\n",
       " 'the',\n",
       " 'mrs.',\n",
       " 'with',\n",
       " 'of',\n",
       " 'moral',\n",
       " 'in',\n",
       " 'disgusted',\n",
       " 'will',\n",
       " 'to',\n",
       " 'to',\n",
       " 'the',\n",
       " 'for',\n",
       " 'lawrence',\n",
       " 'gambling',\n",
       " 'clothing',\n",
       " 'was',\n",
       " 'and',\n",
       " 'the',\n",
       " 'when',\n",
       " 'he',\n",
       " 'the',\n",
       " 'and',\n",
       " 'to',\n",
       " 'the',\n",
       " 'alone',\n",
       " 'is',\n",
       " 'the',\n",
       " 'to',\n",
       " 'the',\n",
       " 'in',\n",
       " 'vivants',\n",
       " 'new',\n",
       " 'high',\n",
       " 'to',\n",
       " 'on',\n",
       " 'the',\n",
       " 'time',\n",
       " 'cousin',\n",
       " 'aunt',\n",
       " 'it',\n",
       " 'a',\n",
       " 'betrayed',\n",
       " 'trenor',\n",
       " 'the',\n",
       " 'no',\n",
       " 'time',\n",
       " 'them',\n",
       " 'them',\n",
       " 'the',\n",
       " 'it',\n",
       " 'day',\n",
       " 'her',\n",
       " 'time',\n",
       " 'in',\n",
       " 'excited',\n",
       " 'hopeful',\n",
       " 'delighted',\n",
       " 'it',\n",
       " 'is',\n",
       " 'he',\n",
       " 'for',\n",
       " 'what',\n",
       " 'he',\n",
       " 'mrs.',\n",
       " 'the',\n",
       " 'for',\n",
       " 'is',\n",
       " 'was',\n",
       " 'for',\n",
       " 'to',\n",
       " 'will',\n",
       " 'on',\n",
       " 'and',\n",
       " 'the',\n",
       " 'to',\n",
       " 'with',\n",
       " 'lawrence',\n",
       " 'lawrence',\n",
       " 'a',\n",
       " 'the',\n",
       " 'tasks',\n",
       " 'the',\n",
       " 'a',\n",
       " 'the',\n",
       " 'to',\n",
       " 'gambling',\n",
       " 'and',\n",
       " 'playing',\n",
       " 'mrs.',\n",
       " 'a',\n",
       " 'americana',\n",
       " 'will',\n",
       " 'for',\n",
       " 'her',\n",
       " 'the',\n",
       " 'is',\n",
       " 'to',\n",
       " 'on',\n",
       " 'and',\n",
       " 'cousin',\n",
       " 'amused',\n",
       " 'a',\n",
       " 'to',\n",
       " 'to',\n",
       " 'early',\n",
       " 'new',\n",
       " 'brother',\n",
       " 'the',\n",
       " 'not',\n",
       " 'take',\n",
       " 'aglaya',\n",
       " 'a',\n",
       " 'the',\n",
       " 'rogozhin',\n",
       " 'she',\n",
       " 'he',\n",
       " 'and',\n",
       " 'ganja',\n",
       " 'the',\n",
       " 'the',\n",
       " 'for',\n",
       " 'catholicism',\n",
       " 'the',\n",
       " 'a',\n",
       " 'happy',\n",
       " 'and',\n",
       " 'myshkin',\n",
       " 'a',\n",
       " 'and',\n",
       " 'myshkin',\n",
       " 'a',\n",
       " 'come',\n",
       " 'napoleon',\n",
       " 'and',\n",
       " 'the',\n",
       " 'his',\n",
       " 'a',\n",
       " 'aglaya',\n",
       " 'hates',\n",
       " 'the',\n",
       " 'husband',\n",
       " 'to',\n",
       " 'is',\n",
       " 'on',\n",
       " 'nastassya',\n",
       " 'the',\n",
       " 'is',\n",
       " 'roubles',\n",
       " 'ferdyshchenko',\n",
       " 'ganja',\n",
       " 'marry',\n",
       " 'to',\n",
       " 'his',\n",
       " 'he',\n",
       " 'everyone',\n",
       " 'in',\n",
       " 'at',\n",
       " 'by',\n",
       " 'the',\n",
       " 'a',\n",
       " 'to',\n",
       " 'his',\n",
       " 'a',\n",
       " 'a',\n",
       " 'seven',\n",
       " 'to',\n",
       " 'his',\n",
       " 'radomsky',\n",
       " 'birthday',\n",
       " 'keller',\n",
       " 'is',\n",
       " 'a',\n",
       " 'nastassya',\n",
       " 'a',\n",
       " 'the',\n",
       " 'madam',\n",
       " 'to',\n",
       " 'and',\n",
       " 'says',\n",
       " 'four',\n",
       " 'to',\n",
       " 'consumption',\n",
       " 'he',\n",
       " 'lawyers',\n",
       " 'he',\n",
       " '10000',\n",
       " 'money',\n",
       " 'keller',\n",
       " 'madam',\n",
       " 'on',\n",
       " 'a',\n",
       " 'poor',\n",
       " 'who',\n",
       " 'a',\n",
       " 'general',\n",
       " 'villa',\n",
       " 'he',\n",
       " 'rogozhin',\n",
       " 'koyla',\n",
       " 'one',\n",
       " 'him',\n",
       " 'and',\n",
       " 'the',\n",
       " 'a',\n",
       " 'in',\n",
       " 'with',\n",
       " 'and',\n",
       " 'to',\n",
       " 'on',\n",
       " 'house',\n",
       " 'koyla',\n",
       " 'is',\n",
       " 'months',\n",
       " 'moscow',\n",
       " 'myshkin',\n",
       " 'uses',\n",
       " 'he',\n",
       " 'ganja',\n",
       " 'the',\n",
       " 'is',\n",
       " 'with',\n",
       " 'roubles',\n",
       " 'to',\n",
       " '100000',\n",
       " 'the',\n",
       " 'the',\n",
       " 'a',\n",
       " 'they',\n",
       " 'a',\n",
       " 'money',\n",
       " 'his',\n",
       " 'ferdyshchenko',\n",
       " 'a',\n",
       " 'the',\n",
       " 'nastassya',\n",
       " 'a',\n",
       " 'himself',\n",
       " 'is',\n",
       " 'koyla',\n",
       " 'and',\n",
       " 'that',\n",
       " 'with',\n",
       " '18000',\n",
       " 'is',\n",
       " 'a',\n",
       " 'the',\n",
       " 'is',\n",
       " 'a',\n",
       " 'flippovna',\n",
       " 'to',\n",
       " 'in',\n",
       " 'for',\n",
       " 'angrily',\n",
       " 'from',\n",
       " 'a',\n",
       " 'in',\n",
       " 'of',\n",
       " 'arranging',\n",
       " 'ganja',\n",
       " 'aglaya',\n",
       " 'photograph',\n",
       " 'aglaya',\n",
       " 'him',\n",
       " 'a',\n",
       " 'was',\n",
       " 'pearl',\n",
       " 'his',\n",
       " 'years',\n",
       " 'his',\n",
       " 'to',\n",
       " 'he',\n",
       " 'ganja',\n",
       " 'the',\n",
       " 'flippovna',\n",
       " 'calligraphy',\n",
       " 'the',\n",
       " 'a',\n",
       " 'three',\n",
       " 'by',\n",
       " 'to',\n",
       " 'incredulous',\n",
       " 'to',\n",
       " 'madame',\n",
       " 'epilepsy',\n",
       " 'switzerland',\n",
       " 'st.',\n",
       " 'god',\n",
       " 'the',\n",
       " 'a',\n",
       " 'was',\n",
       " 'solomon',\n",
       " 'a',\n",
       " 'had',\n",
       " 'his',\n",
       " 'in',\n",
       " 'the',\n",
       " 'to',\n",
       " 'for',\n",
       " 'for',\n",
       " 'for',\n",
       " 'mixed',\n",
       " 'to',\n",
       " 'a',\n",
       " 'william',\n",
       " 'who',\n",
       " 'for',\n",
       " 'and',\n",
       " 'and',\n",
       " 'take',\n",
       " 'and',\n",
       " 'the',\n",
       " 'the',\n",
       " 'and',\n",
       " 'henry',\n",
       " 'new',\n",
       " 'his',\n",
       " 'new',\n",
       " 'new',\n",
       " 'the',\n",
       " 'of',\n",
       " 'of',\n",
       " 'and',\n",
       " 'the',\n",
       " 'for',\n",
       " 'samuel',\n",
       " 'the',\n",
       " 'the',\n",
       " 'not',\n",
       " 'for',\n",
       " 'william',\n",
       " 'who',\n",
       " 'the',\n",
       " 'and',\n",
       " 'to',\n",
       " 'as',\n",
       " 'secretive',\n",
       " 'a',\n",
       " 'deceptive',\n",
       " 'solomon',\n",
       " 'the',\n",
       " 'mary',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'was',\n",
       " 'on',\n",
       " 'the',\n",
       " 'in',\n",
       " 'so',\n",
       " 'and',\n",
       " 'and',\n",
       " 'yet',\n",
       " 'for',\n",
       " 'a',\n",
       " 'to',\n",
       " 'freedom',\n",
       " 'rest',\n",
       " 'the',\n",
       " 'not',\n",
       " 'and',\n",
       " 'and',\n",
       " 'the',\n",
       " 'their',\n",
       " 'the',\n",
       " 'a',\n",
       " 'children',\n",
       " 'was',\n",
       " 'land',\n",
       " 'and',\n",
       " 'cruel',\n",
       " 'by',\n",
       " 'was',\n",
       " 'the',\n",
       " 'the',\n",
       " 'patsey',\n",
       " 'kindness',\n",
       " 'and',\n",
       " 'the',\n",
       " 'the',\n",
       " 'one',\n",
       " 'for',\n",
       " 'money',\n",
       " 'vengeful',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " ...]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    you were ruined but now you are rich again 180...\n",
      "1    the reader will remember that at five minutes ...\n",
      "2    he deducted however from passepartouts share t...\n",
      "3    passepartout went on his errand enchanted 177....\n",
      "4    you have made a mistake of one day we arrived ...\n",
      "Name: 0, dtype: object\n",
      "0    is that the chance and hope you mentioned jaco...\n",
      "1    marleys ghost marley was dead to begin with 2....\n",
      "2    why did i walk through crowds of fellow beings...\n",
      "3    mind i dont mean to say that i know of my own ...\n",
      "4    you wish to be anonymous i wish to be left alo...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_m3_train_data = df_m3_train_data[0].astype(str)\n",
    "print(df_m3_train_data.head()) #.astype(str)\n",
    "\n",
    "df_m3_test_data = df_m3_test_data[0] #.astype(str)\n",
    "print(df_m3_test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the TFIDF leveraged, the vocabulary size shrinks to : 20730\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import * \n",
    "##################################################\n",
    "vectorizer = TfidfVectorizer(lowercase=False) #already converted to lower case earlier, no need to run 2x\n",
    "train_vec = vectorizer.fit_transform(df_m3_train_data)\n",
    "test_vec = vectorizer.transform(df_m3_test_data)\n",
    "\n",
    "print (\"With the TFIDF leveraged, the vocabulary size shrinks to : \" + str(train_vec.shape[1]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07346241812845043\n",
      "Model run time (H:M:S) 00:05:44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\lucky\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Model with no preprocessing \n",
    "tfidf_LR_model_m1 = LogisticRegression(penalty=\"l2\", C = 100)\n",
    "tfidf_LR_model_m1.fit(train_vec, m3_train_labels)\n",
    "\n",
    "#makes predictions on the dev dataset with the model I just built\n",
    "tfidf_dev_pred_labels_m1 = tfidf_LR_model_m1.predict(test_vec)\n",
    "tfidf_dev_pred_probs_m1 = tfidf_LR_model_m1.predict_proba(test_vec)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.f1_score(m3_test_labels, tfidf_dev_pred_labels_m1, average='weighted'))\n",
    "\n",
    "end_time = time.time()\n",
    "run_time = time.strftime(\"%H:%M:%S\",time.gmtime(end_time - start_time))\n",
    "print ('Model run time (H:M:S) ' + run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
